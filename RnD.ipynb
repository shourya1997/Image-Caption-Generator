{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Image Caption Generation\n",
    "\n",
    "## Preparing Text and Images\n",
    "\n",
    "### Preparing the Text\n",
    "The dataset contains multiple descriptions for each photograph and the text of the descriptions requires some minimal cleaning.\n",
    "\n",
    "First, we will load the file containing all of the descriptions. Function *load_doc*\n",
    "\n",
    "Each photo has a unique identifier. This is used in the photo filename and in the text file of descriptions. Next, we will step through the list of photo descriptions and save the first description for each photo. Below defines a function named *load_descriptions()* that, given the loaded document text, will return a dictionary of photo identifiers to descriptions.\n",
    "\n",
    "Next, we need to clean the description text.\n",
    "\n",
    "The descriptions are already tokenized and easy to work with. We will clean the text in the following ways in order to reduce the size of the vocabulary of words we will need to work with:\n",
    "\n",
    "1. Convert all words to lowercase.\n",
    "2. Remove all punctuation.\n",
    "3. Remove all words that are one character or less in length (e.g. ‘a’).\n",
    "\n",
    "Below defines the *clean_descriptions()* function that, given the dictionary of image identifiers to descriptions, steps through each description and cleans the text.\n",
    "\n",
    "Finally, we save the dictionary of image identifiers and descriptions to a new file named *descriptions.txt*, with one image identifier and description per line.\n",
    "\n",
    "Below defines the *save_doc()* function that given a dictionary containing the mapping of identifiers to descriptions and a filename, saves the mapping to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 8092 \n",
      "Vocabulary Size: 4484\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# load doc in memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# extract descriptions for images\n",
    "def load_descriptions(doc):\n",
    "    mapping = dict()\n",
    "    #process lines\n",
    "    for line in doc.split('\\n'):\n",
    "        #split line by white spaces\n",
    "        tokens = line.split()\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        # take the first token as the image id, the rest as the description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        #remove filename from the image id\n",
    "        image_id = image_id.split('.')[0]\n",
    "        #convert the decsription token back to the string\n",
    "        image_desc = ' '.join(image_desc)\n",
    "        #store the first description for ecah image\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = image_desc\n",
    "    return mapping\n",
    "\n",
    "def clean_descriptions(descriptions):\n",
    "    #prepare the transaltion table for removing the punctuation\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    for key, desc in descriptions.items():\n",
    "        #tokenize\n",
    "        desc = desc.split()\n",
    "        # convert to lower case\n",
    "        desc = [word.lower() for word in desc]\n",
    "        # remove punctuation for each token\n",
    "        desc = [w.translate(table) for w in desc]\n",
    "        # remove hanging a and s \n",
    "        desc = [word for word in desc if len(word)>1]\n",
    "        # store the string\n",
    "        descriptions[key] = ' '.join(desc)\n",
    "\n",
    "#save descritions to file, one per line\n",
    "def save_doc(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc in descriptions.items():\n",
    "        lines.append(key + ' ' +desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    \n",
    "filename = 'Flickr8k_text/Flickr8k.token.txt'\n",
    "# load descriptions\n",
    "doc = load_doc(filename)\n",
    "# parse descriptions\n",
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: %d ' % len(descriptions))\n",
    "# clean descriptions\n",
    "clean_descriptions(descriptions)\n",
    "# summarize vocabulary\n",
    "all_tokens = ' '.join(descriptions.values()).split()\n",
    "vocabulary = set(all_tokens)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))\n",
    "# save descriptions\n",
    "save_doc(descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Image\n",
    "\n",
    "We will use a pre-trained model to interpret the content of the photos.\n",
    "\n",
    "There are many models to choose from. In this case, we will use the Oxford Visual Geometry Group or VGG model that won the ImageNet competition in 2014. Learn more about the model here:\n",
    "\n",
    "[Very Deep Convolutional Networks for Large-Scale Visual Recognition](http://www.robots.ox.ac.uk/~vgg/research/very_deep/)\n",
    "\n",
    "Keras provides this pre-trained model directly. Note, the first time you use this model, Keras will download the model weights from the Internet, which are about 500 Megabytes. This may take a few minutes depending on your internet connection.\n",
    "\n",
    "We could use this model as part of a broader image caption model. The problem is, it is a large model and running each photo through the network every time we want to test a new language model configuration (downstream) is redundant.\n",
    "\n",
    "Instead, we can pre-compute the “photo features” using the pre-trained model and save them to file. We can then load these features later and feed them into our model as the interpretation of a given photo in the dataset. It is no different to running the photo through the full VGG model, it is just that we will have done it once in advance.\n",
    "\n",
    "This is an optimization that will make training our models faster and consume less memory.\n",
    "\n",
    "We can load the VGG model in Keras using the VGG class. We will load the model without the top; this means without the layers at the end of the network that are used to interpret the features extracted from the input and turn them into a class prediction. We are not interested in the image net classification of the photos and we will train our own interpretation of the image features.\n",
    "\n",
    "Keras also provides tools for reshaping the loaded photo into the preferred size for the model (e.g. 3 channel 224 x 224 pixel image).\n",
    "\n",
    "Below is a function named extract_features() that given a directory name will load each photo, prepare it for VGG and collect the predicted features from the VGG model. The image features are a 3-dimensional array with the shape (7, 7, 512).\n",
    "\n",
    "The function returns a dictionary of image identifier to image features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Features: 8091\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Input\n",
    "\n",
    "# extract features from each photo in the directory\n",
    "def extract_features(directory):\n",
    "    # load the model\n",
    "    in_layer = Input(shape=(224, 224, 3))\n",
    "    model = VGG16(include_top=False, input_tensor=in_layer)\n",
    "    print(model.summary())\n",
    "    # extract features from each photo\n",
    "    features = dict()\n",
    "    for name in listdir(directory):\n",
    "        # load an image from file\n",
    "        filename = directory + '/' + name\n",
    "        image = load_img(filename, target_size=(224, 224))\n",
    "        # convert the image pixels to a numpy array\n",
    "        image = img_to_array(image)\n",
    "        # reshape data for the model\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        # prepare the image for the VGG model\n",
    "        image = preprocess_input(image)\n",
    "        # get features\n",
    "        feature = model.predict(image, verbose=0)\n",
    "        # get image id\n",
    "        image_id = name.split('.')[0]\n",
    "        # store feature\n",
    "        features[image_id] = feature\n",
    "        print('>%s' % name)\n",
    "    return features\n",
    "\n",
    "# extract features from images\n",
    "# directory = 'Flicker8k_Dataset/Flicker8k_Dataset'\n",
    "# features = extract_features(directory)\n",
    "features = load(open('features-rnd.pkl', 'rb'))\n",
    "print('Extracted Features: %d' %len(features))\n",
    "# save file in pickle\n",
    "# dump(features, open('features-rnd.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Caption Generation Model\n",
    "\n",
    "In this section, we will define a baseline model for generating captions for photos and how to evaluate it so that it can be compared to variations on this baseline.\n",
    "\n",
    "This section is divided into 5 parts:\n",
    "\n",
    "1. Load Data.\n",
    "2. Fit Model.\n",
    "3. Evaluate Model.\n",
    "4. Complete Example\n",
    "5. “A” versus “A” Test\n",
    "6. Generate Photo Captions\n",
    "\n",
    "### 1. Load Data\n",
    "We are not going to fit the model on all of the caption data, or even on a large sample of the data.\n",
    "\n",
    "In this tutorial, we are interested in quickly testing a suite of different configurations of a caption model to see what works on this data. That means we need the evaluation of one model configuration to happen quickly. Toward this end, we will train the models on 100 photographs and captions, then evaluate them on both the training dataset and on a new test set of 100 photographs and captions.\n",
    "\n",
    "First, we need to load a pre-defined subset of photographs. The provided dataset has separate sets for train, test, and development, which are really just different groups of photo identifiers. We will load the development set and use the first 100 identifiers for train and the second 100 (e.g. from 100 to 200) as the test set.\n",
    "\n",
    "The function *load_set()* below will load a pre-defined set of identifiers, and we will call it with the *Flickr_8k.devImages.txt* filename as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    # process line by line\n",
    "    for line in doc.split('\\n'):\n",
    "        # skip empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # get image identifier\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to split the set into train and test sets.\n",
    "\n",
    "We will start by ordering the identifiers by sorting them to ensure we always split them consistently across machines and runs, then take the first 100 for train and the next 100 for test.\n",
    "\n",
    "The *train_test_split()* function below will create this split given the loaded set of identifiers as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a dataset into train/test elements\n",
    "def train_test_split(dataset):\n",
    "    # order keys so the split is consistent\n",
    "    ordered = sorted(dataset)\n",
    "    # return split dataset as two new sets\n",
    "    return set(ordered[:100]), set(ordered[100:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load the photo descriptions using the pre-defined set of train or test identifiers.\n",
    "\n",
    "Below is the function *load_clean_descriptions()* that loads the cleaned text descriptions from ‘descriptions.txt‘ for a given set of identifiers and returns a dictionary of identifier to text.\n",
    "\n",
    "The model we will develop will generate a caption given a photo, and the caption will be generated one word at a time. The sequence of previously generated words will be provided as input. Therefore, we will need a “first word” to kick-off the generation process and a ‘last word‘ to signal the end of the caption. We will use the strings *‘startseq‘* and *‘endseq‘* for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    # load document\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        # split id from description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # skip images not in the set\n",
    "        if image_id in dataset:\n",
    "            # store\n",
    "            descriptions[image_id] = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can load the photo features for a given dataset.\n",
    "\n",
    "Below defines a function named *load_photo_features()* that loads the entire set of photo descriptions, then returns the subset of interest for a given set of photo identifiers. This is not very efficient as the loaded dictionary of all photo features is about 700 Megabytes. Nevertheless, this will get us up and running quickly.\n",
    "\n",
    "Note, if you have a better approach, share it in the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load photo features\n",
    "def load_photo_features(filename, dataset):\n",
    "    # load all features\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    # filter features\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1000\n",
      "Train=100, Test=100\n",
      "Descriptions: train=100, test=100\n",
      "Photos: train=100, test=100\n"
     ]
    }
   ],
   "source": [
    "# load dev set\n",
    "filename = 'Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "dataset = load_set(filename)\n",
    "print('Dataset: %d' % len(dataset))\n",
    "# train-test split\n",
    "train, test = train_test_split(dataset)\n",
    "print('Train=%d, Test=%d' % (len(train), len(test)))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
    "print('Descriptions: train=%d, test=%d' % (len(train_descriptions), len(test_descriptions)))\n",
    "# photo features\n",
    "train_features = load_photo_features('features-rnd.pkl', train)\n",
    "test_features = load_photo_features('features-rnd.pkl', test)\n",
    "print('Photos: train=%d, test=%d' % (len(train_features), len(test_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description text will need to be encoded to numbers before it can be presented to the model as in input or compared to the model’s predictions.\n",
    "\n",
    "The first step in encoding the data is to create a consistent mapping from words to unique integer values. Keras provides the Tokenizer class that can learn this mapping from the loaded description data.\n",
    "\n",
    "Below defines the *create_tokenizer()* that will fit a Tokenizer given the loaded photo description text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 4485\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# fit the tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = list(descriptions.values())\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "#prepare tokenizer\n",
    "tokenizer = create_tokenizer(descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now encode the text.\n",
    "\n",
    "Each description will be split into words. The model will be provided one word and the photo and generate the next word. Then the first two words of the description will be provided to the model as input with the image to generate the next word. This is how the model will be trained.\n",
    "\n",
    "For example, the input sequence “little girl running in field” would be split into 6 input-output pairs to train the model:\n",
    "\n",
    "|X1   |\tX2 (text sequence)                          |y (word)|\n",
    "|-----|---------------------------------------------|--------|\n",
    "|photo|\tstartseq,                                   |little  |\n",
    "|photo|\tstartseq, little,                           |girl    |\n",
    "|photo|\tstartseq, little, girl,                     |running |\n",
    "|photo|\tstartseq, little, girl, running,            |in      |\n",
    "|photo|\tstartseq, little, girl, running, in,        |field   |\n",
    "|photo|\tstartseq, little, girl, running, in, field, |endseq  |\n",
    "\n",
    "Later when the model is used to generate descriptions, the generated words will be concatenated and recursively provided as input to generate a caption for an image.\n",
    "\n",
    "The function below named *create_sequences()* given the tokenizer, a single clean description, the features for a photo, and the maximum description length will prepare a set of input-output pairs for training a model. Calling this function will return X1 and X2 for the arrays of image data and input sequence data and the y value for the output word.\n",
    "\n",
    "The input sequences are integer encoded and the output word is one-hot encoded to represent the probability distribution of the expected word across the whole vocabulary of possible words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, desc, image, max_length):\n",
    "    Ximages, XSeq, y = list(), list(),list()\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    # integer encode the description\n",
    "    seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "    # split one sequence into multiple X,y pairs\n",
    "    for i in range(1, len(seq)):\n",
    "        # select\n",
    "        in_seq, out_seq = seq[:i], seq[i]\n",
    "        # pad input sequence\n",
    "        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "        # encode output sequence\n",
    "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "        # store\n",
    "        Ximages.append(image)\n",
    "        XSeq.append(in_seq)\n",
    "        y.append(out_seq)\n",
    "    # Ximages, XSeq, y = array(Ximages), array(XSeq), array(y)\n",
    "    return [Ximages, XSeq, y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fit Model\n",
    "The model is based on the example laid out in the paper [“Show and Tell: A Neural Image Caption Generator“, 2015.](https://arxiv.org/abs/1411.4555)\n",
    "\n",
    "The model involves three parts:\n",
    "\n",
    "1. Photo Feature Extractor. This is a 16-layer VGG model pre-trained on the ImageNet dataset. We have pre-processed the photos with a the VGG model (without the top) and will use the extracted features predicted by this model as input.\n",
    "2. Sequence Processor. This is a word embedding layer for handling the text input, followed by an LSTM layer. The LSTM output is interpreted by a Dense layer one output at a time.\n",
    "3. Interpreter (for lack of a better name). Both the feature extractor and sequence processor output a fixed-length vector that is the length of a maximum sequence. These are concatenated together and processed by an LSTM and Dense layer before a final prediction is made.\n",
    "A conservative number of neurons is used in the base model. Specifically, a 128 Dense layer after the feature extractor, a 50-dimensionality word embedding followed by a 256 unit LSTM and 128 neuron Dense after the sequence processor, and finally a 500 unit LSTM followed by a 500 neuron Dense at the end of the network.\n",
    "\n",
    "The model predicts a probability distribution across the vocabulary, therefore a softmax activation function is used and a categorical cross entropy loss function is minimized while fitting the network.\n",
    "\n",
    "The function define_model() defines the baseline model, given the size of the vocabulary and the maximum length of photo descriptions. The Keras functional API is used to define the model as it provides the flexibility needed to define a model that takes two input streams and combines them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "    # feature extractor (encoder)\n",
    "    inputs1 = Input(shape=(7, 7, 512))\n",
    "    fe1 = GlobalMaxPooling2D()(inputs1)\n",
    "    fe2 = Dense(128, activation='relu')(fe1)\n",
    "    fe3 = RepeatVector(max_length)(fe2)\n",
    "    # embedding\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    emb2 = Embedding(vocab_size, 50, mask_zero=True)(inputs2)\n",
    "    emb3 = LSTM(256, return_sequences=True)(emb2)\n",
    "    emb4 = TimeDistributed(Dense(128, activation='relu'))(emb3)\n",
    "    # merge inputs\n",
    "    merged = concatenate([fe3, emb4])\n",
    "    # language model (decoder)\n",
    "    lm2 = LSTM(500)(merged)\n",
    "    lm3 = Dense(500, activation='relu')(lm2)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(lm3)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    plot_model(model, show_shapes=True, to_file='baseline1-plot.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the model using a data generator. This is strictly not required given that the captions and extracted photo features can probably fit into memory as a single dataset. Nevertheless, it is good practice for when you come to train the final model on the entire dataset.\n",
    "\n",
    "A generator will yield a result when called. In Keras, it will yield a single batch of input-output samples that are used to estimate the error gradient and update the model weights.\n",
    "\n",
    "The function data_generator() defines the data generator, given a dictionary of loaded photo descriptions, photo features, the tokenizer for integer encoding sequences, and the maximum sequence length in the dataset.\n",
    "\n",
    "The generator loops forever and keeps yielding batches of input-output pairs when asked. We also have a n_step parameter that allows us to tune how many images worth of input-output pairs to generate for each batch. The average sequence has 10 words, that is 10 input-output pairs, and a good batch size might be 30 samples, which is about 2-to-3 images worth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(descriptions, features, tokenizer, max_length, n_step):\n",
    "    # loop until we finish training\n",
    "    while 1:\n",
    "        # loop over photo identifiers in the dataset\n",
    "        keys = list(descriptions.keys())\n",
    "        for i in range(0, len(keys), n_step):\n",
    "            Ximages, XSeq, y = list(), list(),list()\n",
    "            for j in range(i, min(len(keys), i+n_step)):\n",
    "                image_id = keys[j]\n",
    "                # retrieve photo feature input\n",
    "                image = features[image_id][0]\n",
    "                # retrieve text input\n",
    "                desc = descriptions[image_id]\n",
    "                # generate input-output pairs\n",
    "                in_img, in_seq, out_word = create_sequences(tokenizer, desc, image, max_length)\n",
    "                for k in range(len(in_img)):\n",
    "                    Ximages.append(in_img[k])\n",
    "                    XSeq.append(in_seq[k])\n",
    "                    y.append(out_word[k])\n",
    "            # yield this batch of samples to the model\n",
    "            yield [[array(Ximages), array(XSeq)], array(y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these experiments, we will use 2 images per batch, 50 batches (or 100 images) per epoch, and 50 training epochs. You can experiment with different configurations in your own experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate Model\n",
    "Now that we know how to prepare the data and define a model, we must define a test harness to evaluate a given model.\n",
    "\n",
    "We will evaluate a model by training it on the dataset, generating descriptions for all photos in the training dataset, evaluating those predictions with a cost function, and then repeating this evaluation process multiple times.\n",
    "\n",
    "The outcome will be a distribution of skill scores for the model that we can summarize by calculating the mean and standard deviation. This is the preferred way to evaluate deep learning models. \n",
    "\n",
    "First, we need to be able to generate a description for a photo using a trained model.\n",
    "\n",
    "This involves passing in the start description token ‘startseq‘, generating one word, then calling the model recursively with generated words as input until the end of sequence token is reached ‘endseq‘ or the maximum description length is reached.\n",
    "\n",
    "The function below named generate_desc() implements this behavior and generates a textual description given a trained model, and a given prepared photo as input. It calls the function word_for_id() in order to map an integer prediction back to a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    " \n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(max_length):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate predictions for all photos in the training dataset and in the test dataset.\n",
    "\n",
    "The function below named *evaluate_model()* will evaluate a trained model against a given dataset of photo descriptions and photo features. The actual and predicted descriptions are collected and evaluated collectively using the corpus BLEU score that summarizes how close the generated text is to the expected text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    # step over the whole set\n",
    "    for key, desc in descriptions.items():\n",
    "        # generate description\n",
    "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "        # store actual and predicted\n",
    "        actual.append([desc.split()])\n",
    "        predicted.append(yhat.split())\n",
    "    # calculate BLEU score\n",
    "    bleu = corpus_bleu(actual, predicted)\n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU scores are used in text translation for evaluating translated text against one or more reference translations. We do in fact have access to multiple reference descriptions for each image that we could compare to, but for simplicity, we will use the first description for each photo in the dataset (e.g. the cleaned version).\n",
    "\n",
    "You can learn more about the BLEU score here:\n",
    "\n",
    "[BLEU (bilingual evaluation understudy) on Wikipedia](https://en.wikipedia.org/wiki/BLEU)\n",
    "\n",
    "The NLTK Python library implements the BLEU score calculation in the corpus_bleu() function. A higher score close to 1.0 is better, a score closer to zero is worse.\n",
    "\n",
    "Finally, all we need to do is define, fit, and evaluate the model multiple times in a loop then report the final average score.\n",
    "\n",
    "Ideally, we would repeat the experiment 30 times or more, but this will take too long for our small test harness. Instead, will evaluate the model 3 times. It will be faster, but the mean score will have higher variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from pandas import DataFrame\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from pickle import load\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.pooling import GlobalMaxPooling2D, GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description Length: 25\n"
     ]
    }
   ],
   "source": [
    "# determine the maximum sequence length\n",
    "max_length = max(len(s.split()) for s in list(train_descriptions.values()))\n",
    "print('Description Length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description Length: 25\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           (None, 7, 7, 512)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_8 (GlobalM (None, 512)          0           input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 25, 50)       224250      input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 128)          65664       global_max_pooling2d_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_15 (LSTM)                  (None, 25, 256)      314368      embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_8 (RepeatVector)  (None, 25, 128)      0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, 25, 128)      32896       lstm_15[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 25, 256)      0           repeat_vector_8[0][0]            \n",
      "                                                                 time_distributed_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm_16 (LSTM)                  (None, 500)          1514000     concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 500)          250500      lstm_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 4485)         2246985     dense_31[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,648,663\n",
      "Trainable params: 4,648,663\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      " - 15s - loss: 7.0622 - acc: 0.0471\n",
      "Epoch 2/100\n",
      " - 13s - loss: 5.6024 - acc: 0.0565\n",
      "Epoch 3/100\n",
      " - 13s - loss: 5.5119 - acc: 0.0630\n",
      "Epoch 4/100\n",
      " - 13s - loss: 5.4021 - acc: 0.0630\n",
      "Epoch 5/100\n",
      " - 13s - loss: 5.3526 - acc: 0.0630\n",
      "Epoch 6/100\n",
      " - 13s - loss: 5.3332 - acc: 0.0630\n",
      "Epoch 7/100\n",
      " - 13s - loss: 5.3089 - acc: 0.0630\n",
      "Epoch 8/100\n",
      " - 13s - loss: 5.3060 - acc: 0.0730\n",
      "Epoch 9/100\n",
      " - 13s - loss: 5.2490 - acc: 0.0751\n",
      "Epoch 10/100\n",
      " - 13s - loss: 5.2052 - acc: 0.0746\n",
      "Epoch 11/100\n",
      " - 13s - loss: 5.1607 - acc: 0.0740\n",
      "Epoch 12/100\n",
      " - 13s - loss: 5.1635 - acc: 0.0756\n",
      "Epoch 13/100\n",
      " - 13s - loss: 5.1495 - acc: 0.0768\n",
      "Epoch 14/100\n",
      " - 13s - loss: 5.0874 - acc: 0.0708\n",
      "Epoch 15/100\n",
      " - 13s - loss: 5.0793 - acc: 0.0784\n",
      "Epoch 16/100\n",
      " - 13s - loss: 5.0883 - acc: 0.0760\n",
      "Epoch 17/100\n",
      " - 13s - loss: 5.0523 - acc: 0.0763\n",
      "Epoch 18/100\n",
      " - 13s - loss: 5.0775 - acc: 0.0754\n",
      "Epoch 19/100\n",
      " - 13s - loss: 5.0166 - acc: 0.0732\n",
      "Epoch 20/100\n",
      " - 13s - loss: 4.9888 - acc: 0.0774\n",
      "Epoch 21/100\n",
      " - 13s - loss: 4.9881 - acc: 0.0769\n",
      "Epoch 22/100\n",
      " - 13s - loss: 5.0373 - acc: 0.0776\n",
      "Epoch 23/100\n",
      " - 13s - loss: 4.9646 - acc: 0.0802\n",
      "Epoch 24/100\n",
      " - 13s - loss: 4.9376 - acc: 0.0791\n",
      "Epoch 25/100\n",
      " - 13s - loss: 4.9041 - acc: 0.0797\n",
      "Epoch 26/100\n",
      " - 13s - loss: 4.8651 - acc: 0.0874\n",
      "Epoch 27/100\n",
      " - 13s - loss: 4.8881 - acc: 0.0801\n",
      "Epoch 28/100\n",
      " - 13s - loss: 4.8544 - acc: 0.0805\n",
      "Epoch 29/100\n",
      " - 13s - loss: 4.8958 - acc: 0.0880\n",
      "Epoch 30/100\n",
      " - 13s - loss: 4.8698 - acc: 0.0831\n",
      "Epoch 31/100\n",
      " - 13s - loss: 4.8900 - acc: 0.0817\n",
      "Epoch 32/100\n",
      " - 13s - loss: 4.9614 - acc: 0.0820\n",
      "Epoch 33/100\n",
      " - 13s - loss: 4.9042 - acc: 0.0871\n",
      "Epoch 34/100\n",
      " - 13s - loss: 4.7867 - acc: 0.0857\n",
      "Epoch 35/100\n",
      " - 13s - loss: 4.7263 - acc: 0.0833\n",
      "Epoch 36/100\n",
      " - 13s - loss: 4.6576 - acc: 0.0840\n",
      "Epoch 37/100\n",
      " - 13s - loss: 4.6070 - acc: 0.0822\n",
      "Epoch 38/100\n",
      " - 13s - loss: 4.5840 - acc: 0.0936\n",
      "Epoch 39/100\n",
      " - 13s - loss: 4.4953 - acc: 0.0932\n",
      "Epoch 40/100\n",
      " - 13s - loss: 4.4504 - acc: 0.0934\n",
      "Epoch 41/100\n",
      " - 13s - loss: 4.4057 - acc: 0.0933\n",
      "Epoch 42/100\n",
      " - 13s - loss: 4.3702 - acc: 0.0951\n",
      "Epoch 43/100\n",
      " - 13s - loss: 4.3262 - acc: 0.0876\n",
      "Epoch 44/100\n",
      " - 13s - loss: 4.2810 - acc: 0.0910\n",
      "Epoch 45/100\n",
      " - 13s - loss: 4.2280 - acc: 0.0896\n",
      "Epoch 46/100\n",
      " - 13s - loss: 4.2362 - acc: 0.0969\n",
      "Epoch 47/100\n",
      " - 13s - loss: 4.2163 - acc: 0.0981\n",
      "Epoch 48/100\n",
      " - 13s - loss: 4.1082 - acc: 0.1038\n",
      "Epoch 49/100\n",
      " - 13s - loss: 4.0463 - acc: 0.1092\n",
      "Epoch 50/100\n",
      " - 13s - loss: 4.0331 - acc: 0.1033\n",
      "Epoch 51/100\n",
      " - 13s - loss: 4.0200 - acc: 0.1128\n",
      "Epoch 52/100\n",
      " - 13s - loss: 3.9964 - acc: 0.1143\n",
      "Epoch 53/100\n",
      " - 13s - loss: 3.9100 - acc: 0.1066\n",
      "Epoch 54/100\n",
      " - 13s - loss: 3.9534 - acc: 0.1058\n",
      "Epoch 55/100\n",
      " - 13s - loss: 3.8558 - acc: 0.1255\n",
      "Epoch 56/100\n",
      " - 13s - loss: 3.9316 - acc: 0.1085\n",
      "Epoch 57/100\n",
      " - 13s - loss: 3.7711 - acc: 0.1180\n",
      "Epoch 58/100\n",
      " - 13s - loss: 3.7362 - acc: 0.1386\n",
      "Epoch 59/100\n",
      " - 13s - loss: 3.8174 - acc: 0.1221\n",
      "Epoch 60/100\n",
      " - 13s - loss: 3.7884 - acc: 0.1176\n",
      "Epoch 61/100\n",
      " - 13s - loss: 3.6281 - acc: 0.1241\n",
      "Epoch 62/100\n",
      " - 13s - loss: 3.5718 - acc: 0.1374\n",
      "Epoch 63/100\n",
      " - 13s - loss: 3.4827 - acc: 0.1326\n",
      "Epoch 64/100\n",
      " - 13s - loss: 3.5029 - acc: 0.1376\n",
      "Epoch 65/100\n",
      " - 13s - loss: 3.3928 - acc: 0.1521\n",
      "Epoch 66/100\n",
      " - 13s - loss: 3.4322 - acc: 0.1393\n",
      "Epoch 67/100\n",
      " - 13s - loss: 3.3993 - acc: 0.1372\n",
      "Epoch 68/100\n",
      " - 13s - loss: 3.2826 - acc: 0.1620\n",
      "Epoch 69/100\n",
      " - 13s - loss: 3.2931 - acc: 0.1525\n",
      "Epoch 70/100\n",
      " - 13s - loss: 3.1936 - acc: 0.1554\n",
      "Epoch 71/100\n",
      " - 13s - loss: 3.1637 - acc: 0.1807\n",
      "Epoch 72/100\n",
      " - 13s - loss: 3.1204 - acc: 0.1742\n",
      "Epoch 73/100\n",
      " - 13s - loss: 3.1325 - acc: 0.1828\n",
      "Epoch 74/100\n",
      " - 13s - loss: 3.0322 - acc: 0.1802\n",
      "Epoch 75/100\n",
      " - 13s - loss: 3.0036 - acc: 0.1890\n",
      "Epoch 76/100\n",
      " - 13s - loss: 2.9726 - acc: 0.1899\n",
      "Epoch 77/100\n",
      " - 13s - loss: 2.8897 - acc: 0.2135\n",
      "Epoch 78/100\n",
      " - 13s - loss: 2.8513 - acc: 0.2236\n",
      "Epoch 79/100\n",
      " - 13s - loss: 2.8475 - acc: 0.2302\n",
      "Epoch 80/100\n",
      " - 13s - loss: 2.8607 - acc: 0.2054\n",
      "Epoch 81/100\n",
      " - 13s - loss: 2.7784 - acc: 0.2243\n",
      "Epoch 82/100\n",
      " - 13s - loss: 2.6724 - acc: 0.2477\n",
      "Epoch 83/100\n",
      " - 13s - loss: 2.6286 - acc: 0.2609\n",
      "Epoch 84/100\n",
      " - 13s - loss: 2.6120 - acc: 0.2575\n",
      "Epoch 85/100\n",
      " - 13s - loss: 2.6227 - acc: 0.2466\n",
      "Epoch 86/100\n",
      " - 13s - loss: 2.6073 - acc: 0.2662\n",
      "Epoch 87/100\n",
      " - 13s - loss: 2.5288 - acc: 0.2701\n",
      "Epoch 88/100\n",
      " - 13s - loss: 2.4461 - acc: 0.2734\n",
      "Epoch 89/100\n",
      " - 13s - loss: 2.3978 - acc: 0.2817\n",
      "Epoch 90/100\n",
      " - 13s - loss: 2.3636 - acc: 0.3038\n",
      "Epoch 91/100\n",
      " - 13s - loss: 2.2925 - acc: 0.3179\n",
      "Epoch 92/100\n",
      " - 13s - loss: 2.2948 - acc: 0.3215\n",
      "Epoch 93/100\n",
      " - 13s - loss: 2.3024 - acc: 0.3130\n",
      "Epoch 94/100\n",
      " - 13s - loss: 2.2294 - acc: 0.3348\n",
      "Epoch 95/100\n",
      " - 13s - loss: 2.2208 - acc: 0.3294\n",
      "Epoch 96/100\n",
      " - 13s - loss: 2.2863 - acc: 0.3177\n",
      "Epoch 97/100\n",
      " - 13s - loss: 2.2872 - acc: 0.3079\n",
      "Epoch 98/100\n",
      " - 13s - loss: 2.1541 - acc: 0.3670\n",
      "Epoch 99/100\n",
      " - 13s - loss: 2.1055 - acc: 0.3668\n",
      "Epoch 100/100\n",
      " - 13s - loss: 2.0308 - acc: 0.3573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shourya97/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/home/shourya97/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1: train=0.052499 test=0.129957\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           (None, 7, 7, 512)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_9 (GlobalM (None, 512)          0           input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 25, 50)       224250      input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 128)          65664       global_max_pooling2d_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_17 (LSTM)                  (None, 25, 256)      314368      embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_9 (RepeatVector)  (None, 25, 128)      0           dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistrib (None, 25, 128)      32896       lstm_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 25, 256)      0           repeat_vector_9[0][0]            \n",
      "                                                                 time_distributed_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm_18 (LSTM)                  (None, 500)          1514000     concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 500)          250500      lstm_18[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 4485)         2246985     dense_35[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,648,663\n",
      "Trainable params: 4,648,663\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      " - 15s - loss: 7.1042 - acc: 0.0413\n",
      "Epoch 2/100\n",
      " - 13s - loss: 5.5923 - acc: 0.0659\n",
      "Epoch 3/100\n",
      " - 13s - loss: 5.4596 - acc: 0.0630\n",
      "Epoch 4/100\n",
      " - 13s - loss: 5.3904 - acc: 0.0630\n",
      "Epoch 5/100\n",
      " - 14s - loss: 5.3576 - acc: 0.0695\n",
      "Epoch 6/100\n",
      " - 14s - loss: 5.3215 - acc: 0.0707\n",
      "Epoch 7/100\n",
      " - 16s - loss: 5.2345 - acc: 0.0720\n",
      "Epoch 8/100\n",
      " - 13s - loss: 5.1905 - acc: 0.0731\n",
      "Epoch 9/100\n",
      " - 14s - loss: 5.1219 - acc: 0.0767\n",
      "Epoch 10/100\n",
      " - 14s - loss: 5.0846 - acc: 0.0786\n",
      "Epoch 11/100\n",
      " - 13s - loss: 5.0284 - acc: 0.0807\n",
      "Epoch 12/100\n",
      " - 13s - loss: 4.9963 - acc: 0.0738\n",
      "Epoch 13/100\n",
      " - 13s - loss: 4.9729 - acc: 0.0795\n",
      "Epoch 14/100\n",
      " - 13s - loss: 4.9827 - acc: 0.0809\n",
      "Epoch 15/100\n",
      " - 13s - loss: 4.8869 - acc: 0.0792\n",
      "Epoch 16/100\n",
      " - 13s - loss: 4.8109 - acc: 0.0899\n",
      "Epoch 17/100\n",
      " - 13s - loss: 4.7727 - acc: 0.0831\n",
      "Epoch 18/100\n",
      " - 13s - loss: 4.7061 - acc: 0.0909\n",
      "Epoch 19/100\n",
      " - 13s - loss: 4.6403 - acc: 0.0900\n",
      "Epoch 20/100\n",
      " - 13s - loss: 4.6741 - acc: 0.0943\n",
      "Epoch 21/100\n",
      " - 13s - loss: 4.5904 - acc: 0.0932\n",
      "Epoch 22/100\n",
      " - 13s - loss: 4.5133 - acc: 0.0922\n",
      "Epoch 23/100\n",
      " - 13s - loss: 4.5005 - acc: 0.0846\n",
      "Epoch 24/100\n",
      " - 13s - loss: 4.4598 - acc: 0.0961\n",
      "Epoch 25/100\n",
      " - 13s - loss: 4.3928 - acc: 0.1011\n",
      "Epoch 26/100\n",
      " - 13s - loss: 4.3877 - acc: 0.0987\n",
      "Epoch 27/100\n",
      " - 13s - loss: 4.3157 - acc: 0.0953\n",
      "Epoch 28/100\n",
      " - 13s - loss: 4.2608 - acc: 0.0983\n",
      "Epoch 29/100\n",
      " - 13s - loss: 4.2452 - acc: 0.0998\n",
      "Epoch 30/100\n",
      " - 13s - loss: 4.2205 - acc: 0.1002\n",
      "Epoch 31/100\n",
      " - 13s - loss: 4.1924 - acc: 0.1049\n",
      "Epoch 32/100\n",
      " - 13s - loss: 4.1713 - acc: 0.1074\n",
      "Epoch 33/100\n",
      " - 13s - loss: 4.1769 - acc: 0.1060\n",
      "Epoch 34/100\n",
      " - 13s - loss: 4.1533 - acc: 0.1065\n",
      "Epoch 35/100\n",
      " - 13s - loss: 4.0576 - acc: 0.1009\n",
      "Epoch 36/100\n",
      " - 13s - loss: 3.9821 - acc: 0.1200\n",
      "Epoch 37/100\n",
      " - 13s - loss: 3.9065 - acc: 0.1142\n",
      "Epoch 38/100\n",
      " - 13s - loss: 3.8622 - acc: 0.1130\n",
      "Epoch 39/100\n",
      " - 13s - loss: 3.8006 - acc: 0.1203\n",
      "Epoch 40/100\n",
      " - 13s - loss: 3.7211 - acc: 0.1187\n",
      "Epoch 41/100\n",
      " - 13s - loss: 3.7101 - acc: 0.1278\n",
      "Epoch 42/100\n",
      " - 13s - loss: 3.6136 - acc: 0.1287\n",
      "Epoch 43/100\n",
      " - 13s - loss: 3.5510 - acc: 0.1453\n",
      "Epoch 44/100\n",
      " - 13s - loss: 3.4560 - acc: 0.1355\n",
      "Epoch 45/100\n",
      " - 13s - loss: 3.3509 - acc: 0.1580\n",
      "Epoch 46/100\n",
      " - 13s - loss: 3.2750 - acc: 0.1548\n",
      "Epoch 47/100\n",
      " - 13s - loss: 3.2241 - acc: 0.1693\n",
      "Epoch 48/100\n",
      " - 13s - loss: 3.1469 - acc: 0.1775\n",
      "Epoch 49/100\n",
      " - 13s - loss: 3.1121 - acc: 0.1743\n",
      "Epoch 50/100\n",
      " - 13s - loss: 3.0640 - acc: 0.1819\n",
      "Epoch 51/100\n",
      " - 13s - loss: 2.9614 - acc: 0.2048\n",
      "Epoch 52/100\n",
      " - 13s - loss: 2.9155 - acc: 0.2024\n",
      "Epoch 53/100\n",
      " - 13s - loss: 2.8070 - acc: 0.2177\n",
      "Epoch 54/100\n",
      " - 13s - loss: 2.7796 - acc: 0.2336\n",
      "Epoch 55/100\n",
      " - 13s - loss: 2.7163 - acc: 0.2259\n",
      "Epoch 56/100\n",
      " - 13s - loss: 2.6168 - acc: 0.2594\n",
      "Epoch 57/100\n",
      " - 13s - loss: 2.5304 - acc: 0.2545\n",
      "Epoch 58/100\n",
      " - 13s - loss: 2.5096 - acc: 0.2736\n",
      "Epoch 59/100\n",
      " - 13s - loss: 2.4866 - acc: 0.2555\n",
      "Epoch 60/100\n",
      " - 13s - loss: 2.3721 - acc: 0.3097\n",
      "Epoch 61/100\n",
      " - 13s - loss: 2.2929 - acc: 0.3118\n",
      "Epoch 62/100\n",
      " - 13s - loss: 2.2932 - acc: 0.3324\n",
      "Epoch 63/100\n",
      " - 13s - loss: 2.1929 - acc: 0.3311\n",
      "Epoch 64/100\n",
      " - 13s - loss: 2.1847 - acc: 0.3361\n",
      "Epoch 65/100\n",
      " - 13s - loss: 2.1152 - acc: 0.3591\n",
      "Epoch 66/100\n",
      " - 13s - loss: 2.0975 - acc: 0.3600\n",
      "Epoch 67/100\n",
      " - 13s - loss: 1.9760 - acc: 0.3836\n",
      "Epoch 68/100\n",
      " - 13s - loss: 1.9142 - acc: 0.3778\n",
      "Epoch 69/100\n",
      " - 13s - loss: 1.9276 - acc: 0.3852\n",
      "Epoch 70/100\n",
      " - 13s - loss: 1.8808 - acc: 0.4067\n",
      "Epoch 71/100\n",
      " - 13s - loss: 1.8142 - acc: 0.4288\n",
      "Epoch 72/100\n",
      " - 13s - loss: 1.7941 - acc: 0.4074\n",
      "Epoch 73/100\n",
      " - 13s - loss: 1.8094 - acc: 0.4261\n",
      "Epoch 74/100\n",
      " - 13s - loss: 1.6978 - acc: 0.4451\n",
      "Epoch 75/100\n",
      " - 13s - loss: 1.6874 - acc: 0.4403\n",
      "Epoch 76/100\n",
      " - 13s - loss: 1.7052 - acc: 0.4327\n",
      "Epoch 77/100\n",
      " - 13s - loss: 1.6314 - acc: 0.4610\n",
      "Epoch 78/100\n",
      " - 13s - loss: 1.5332 - acc: 0.4889\n",
      "Epoch 79/100\n",
      " - 13s - loss: 1.5462 - acc: 0.4861\n",
      "Epoch 80/100\n",
      " - 13s - loss: 1.5212 - acc: 0.4856\n",
      "Epoch 81/100\n",
      " - 13s - loss: 1.4889 - acc: 0.4985\n",
      "Epoch 82/100\n",
      " - 13s - loss: 1.5512 - acc: 0.4677\n",
      "Epoch 83/100\n",
      " - 13s - loss: 1.4529 - acc: 0.5075\n",
      "Epoch 84/100\n",
      " - 13s - loss: 1.4769 - acc: 0.5087\n",
      "Epoch 85/100\n",
      " - 13s - loss: 1.3663 - acc: 0.5395\n",
      "Epoch 86/100\n",
      " - 13s - loss: 1.3071 - acc: 0.5565\n",
      "Epoch 87/100\n",
      " - 13s - loss: 1.3776 - acc: 0.5282\n",
      "Epoch 88/100\n",
      " - 13s - loss: 1.3226 - acc: 0.5636\n",
      "Epoch 89/100\n",
      " - 13s - loss: 1.2489 - acc: 0.5611\n",
      "Epoch 90/100\n",
      " - 13s - loss: 1.2638 - acc: 0.5823\n",
      "Epoch 91/100\n",
      " - 13s - loss: 1.2261 - acc: 0.5763\n",
      "Epoch 92/100\n",
      " - 13s - loss: 1.2589 - acc: 0.5726\n",
      "Epoch 93/100\n",
      " - 13s - loss: 1.1918 - acc: 0.5793\n",
      "Epoch 94/100\n",
      " - 13s - loss: 1.1141 - acc: 0.6119\n",
      "Epoch 95/100\n",
      " - 13s - loss: 1.1242 - acc: 0.5976\n",
      "Epoch 96/100\n",
      " - 13s - loss: 1.1226 - acc: 0.6177\n",
      "Epoch 97/100\n",
      " - 13s - loss: 1.0676 - acc: 0.6382\n",
      "Epoch 98/100\n",
      " - 13s - loss: 1.1652 - acc: 0.5887\n",
      "Epoch 99/100\n",
      " - 13s - loss: 1.1801 - acc: 0.5914\n",
      "Epoch 100/100\n",
      " - 13s - loss: 1.1705 - acc: 0.6078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shourya97/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2: train=0.034853 test=0.086743\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           (None, 7, 7, 512)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_10 (Global (None, 512)          0           input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 25, 50)       224250      input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 128)          65664       global_max_pooling2d_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_19 (LSTM)                  (None, 25, 256)      314368      embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_10 (RepeatVector) (None, 25, 128)      0           dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistri (None, 25, 128)      32896       lstm_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 25, 256)      0           repeat_vector_10[0][0]           \n",
      "                                                                 time_distributed_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_20 (LSTM)                  (None, 500)          1514000     concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 500)          250500      lstm_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 4485)         2246985     dense_39[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,648,663\n",
      "Trainable params: 4,648,663\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      " - 15s - loss: 7.0838 - acc: 0.0497\n",
      "Epoch 2/100\n",
      " - 13s - loss: 5.5910 - acc: 0.0539\n",
      "Epoch 3/100\n",
      " - 13s - loss: 5.4468 - acc: 0.0658\n",
      "Epoch 4/100\n",
      " - 13s - loss: 5.3693 - acc: 0.0620\n",
      "Epoch 5/100\n",
      " - 13s - loss: 5.3443 - acc: 0.0645\n",
      "Epoch 6/100\n",
      " - 13s - loss: 5.3314 - acc: 0.0682\n",
      "Epoch 7/100\n",
      " - 13s - loss: 5.2705 - acc: 0.0731\n",
      "Epoch 8/100\n",
      " - 13s - loss: 5.2397 - acc: 0.0747\n",
      "Epoch 9/100\n",
      " - 13s - loss: 5.2654 - acc: 0.0723\n",
      "Epoch 10/100\n",
      " - 13s - loss: 5.1860 - acc: 0.0741\n",
      "Epoch 11/100\n",
      " - 13s - loss: 5.1413 - acc: 0.0771\n",
      "Epoch 12/100\n",
      " - 13s - loss: 5.1192 - acc: 0.0762\n",
      "Epoch 13/100\n",
      " - 13s - loss: 5.0749 - acc: 0.0774\n",
      "Epoch 14/100\n",
      " - 13s - loss: 5.0551 - acc: 0.0777\n",
      "Epoch 15/100\n",
      " - 13s - loss: 5.0374 - acc: 0.0746\n",
      "Epoch 16/100\n",
      " - 13s - loss: 5.0182 - acc: 0.0795\n",
      "Epoch 17/100\n",
      " - 13s - loss: 4.9517 - acc: 0.0810\n",
      "Epoch 18/100\n",
      " - 13s - loss: 4.8920 - acc: 0.0783\n",
      "Epoch 19/100\n",
      " - 13s - loss: 4.8278 - acc: 0.0838\n",
      "Epoch 20/100\n",
      " - 13s - loss: 4.7692 - acc: 0.0820\n",
      "Epoch 21/100\n",
      " - 13s - loss: 4.7576 - acc: 0.0821\n",
      "Epoch 22/100\n",
      " - 13s - loss: 4.6713 - acc: 0.0826\n",
      "Epoch 23/100\n",
      " - 13s - loss: 4.6047 - acc: 0.0867\n",
      "Epoch 24/100\n",
      " - 13s - loss: 4.5765 - acc: 0.0857\n",
      "Epoch 25/100\n",
      " - 13s - loss: 4.5073 - acc: 0.0803\n",
      "Epoch 26/100\n",
      " - 13s - loss: 4.4926 - acc: 0.0815\n",
      "Epoch 27/100\n",
      " - 13s - loss: 4.4424 - acc: 0.0836\n",
      "Epoch 28/100\n",
      " - 13s - loss: 4.3739 - acc: 0.0838\n",
      "Epoch 29/100\n",
      " - 13s - loss: 4.3700 - acc: 0.1015\n",
      "Epoch 30/100\n",
      " - 13s - loss: 4.3018 - acc: 0.0849\n",
      "Epoch 31/100\n",
      " - 13s - loss: 4.2457 - acc: 0.0998\n",
      "Epoch 32/100\n",
      " - 13s - loss: 4.1936 - acc: 0.1009\n",
      "Epoch 33/100\n",
      " - 13s - loss: 4.1174 - acc: 0.1026\n",
      "Epoch 34/100\n",
      " - 13s - loss: 4.0932 - acc: 0.1063\n",
      "Epoch 35/100\n",
      " - 13s - loss: 4.0138 - acc: 0.1064\n",
      "Epoch 36/100\n",
      " - 13s - loss: 4.2853 - acc: 0.1077\n",
      "Epoch 37/100\n",
      " - 13s - loss: 4.2211 - acc: 0.0977\n",
      "Epoch 38/100\n",
      " - 13s - loss: 4.0195 - acc: 0.1078\n",
      "Epoch 39/100\n",
      " - 13s - loss: 3.9009 - acc: 0.1092\n",
      "Epoch 40/100\n",
      " - 13s - loss: 3.8141 - acc: 0.1053\n",
      "Epoch 41/100\n",
      " - 13s - loss: 3.7563 - acc: 0.1126\n",
      "Epoch 42/100\n",
      " - 13s - loss: 3.7748 - acc: 0.1167\n",
      "Epoch 43/100\n",
      " - 13s - loss: 3.7145 - acc: 0.1266\n",
      "Epoch 44/100\n",
      " - 13s - loss: 3.6637 - acc: 0.1203\n",
      "Epoch 45/100\n",
      " - 13s - loss: 3.6308 - acc: 0.1307\n",
      "Epoch 46/100\n",
      " - 13s - loss: 3.5000 - acc: 0.1434\n",
      "Epoch 47/100\n",
      " - 13s - loss: 3.4481 - acc: 0.1476\n",
      "Epoch 48/100\n",
      " - 13s - loss: 3.3828 - acc: 0.1576\n",
      "Epoch 49/100\n",
      " - 13s - loss: 3.3426 - acc: 0.1536\n",
      "Epoch 50/100\n",
      " - 13s - loss: 3.2503 - acc: 0.1734\n",
      "Epoch 51/100\n",
      " - 13s - loss: 3.1393 - acc: 0.1777\n",
      "Epoch 52/100\n",
      " - 13s - loss: 3.0746 - acc: 0.1864\n",
      "Epoch 53/100\n",
      " - 13s - loss: 3.0245 - acc: 0.2054\n",
      "Epoch 54/100\n",
      " - 13s - loss: 2.9596 - acc: 0.2306\n",
      "Epoch 55/100\n",
      " - 13s - loss: 2.9506 - acc: 0.2131\n",
      "Epoch 56/100\n",
      " - 13s - loss: 2.8825 - acc: 0.2085\n",
      "Epoch 57/100\n",
      " - 13s - loss: 2.8216 - acc: 0.2295\n",
      "Epoch 58/100\n",
      " - 13s - loss: 2.7498 - acc: 0.2169\n",
      "Epoch 59/100\n",
      " - 13s - loss: 2.6751 - acc: 0.2506\n",
      "Epoch 60/100\n",
      " - 13s - loss: 2.5561 - acc: 0.2698\n",
      "Epoch 61/100\n",
      " - 13s - loss: 2.4792 - acc: 0.2857\n",
      "Epoch 62/100\n",
      " - 13s - loss: 2.3653 - acc: 0.3137\n",
      "Epoch 63/100\n",
      " - 13s - loss: 2.3978 - acc: 0.2959\n",
      "Epoch 64/100\n",
      " - 13s - loss: 2.2992 - acc: 0.3136\n",
      "Epoch 65/100\n",
      " - 13s - loss: 2.2386 - acc: 0.3152\n",
      "Epoch 66/100\n",
      " - 13s - loss: 2.1863 - acc: 0.3393\n",
      "Epoch 67/100\n",
      " - 13s - loss: 2.1052 - acc: 0.3632\n",
      "Epoch 68/100\n",
      " - 13s - loss: 2.0282 - acc: 0.3969\n",
      "Epoch 69/100\n",
      " - 13s - loss: 2.0814 - acc: 0.3861\n",
      "Epoch 70/100\n",
      " - 13s - loss: 1.9977 - acc: 0.3877\n",
      "Epoch 71/100\n",
      " - 13s - loss: 1.9615 - acc: 0.3880\n",
      "Epoch 72/100\n",
      " - 13s - loss: 1.9429 - acc: 0.4184\n",
      "Epoch 73/100\n",
      " - 13s - loss: 1.8794 - acc: 0.4131\n",
      "Epoch 74/100\n",
      " - 13s - loss: 1.8967 - acc: 0.4054\n",
      "Epoch 75/100\n",
      " - 13s - loss: 1.8584 - acc: 0.4013\n",
      "Epoch 76/100\n",
      " - 13s - loss: 1.8107 - acc: 0.4363\n",
      "Epoch 77/100\n",
      " - 13s - loss: 1.8254 - acc: 0.4214\n",
      "Epoch 78/100\n",
      " - 13s - loss: 1.7506 - acc: 0.4387\n",
      "Epoch 79/100\n",
      " - 13s - loss: 1.6836 - acc: 0.4589\n",
      "Epoch 80/100\n",
      " - 13s - loss: 1.6325 - acc: 0.4643\n",
      "Epoch 81/100\n",
      " - 13s - loss: 1.5788 - acc: 0.4877\n",
      "Epoch 82/100\n",
      " - 13s - loss: 1.5759 - acc: 0.4917\n",
      "Epoch 83/100\n",
      " - 14s - loss: 1.5467 - acc: 0.4894\n",
      "Epoch 84/100\n",
      " - 13s - loss: 1.4996 - acc: 0.4982\n",
      "Epoch 85/100\n",
      " - 13s - loss: 1.5070 - acc: 0.4767\n",
      "Epoch 86/100\n",
      " - 13s - loss: 1.4179 - acc: 0.5253\n",
      "Epoch 87/100\n",
      " - 13s - loss: 1.4302 - acc: 0.5310\n",
      "Epoch 88/100\n",
      " - 13s - loss: 1.4111 - acc: 0.5261\n",
      "Epoch 89/100\n",
      " - 13s - loss: 1.4159 - acc: 0.5265\n",
      "Epoch 90/100\n",
      " - 13s - loss: 1.4274 - acc: 0.5245\n",
      "Epoch 91/100\n",
      " - 13s - loss: 1.3814 - acc: 0.5358\n",
      "Epoch 92/100\n",
      " - 13s - loss: 1.3170 - acc: 0.5711\n",
      "Epoch 93/100\n",
      " - 13s - loss: 1.2707 - acc: 0.5699\n",
      "Epoch 94/100\n",
      " - 13s - loss: 1.2183 - acc: 0.5991\n",
      "Epoch 95/100\n",
      " - 13s - loss: 1.2229 - acc: 0.6122\n",
      "Epoch 96/100\n",
      " - 13s - loss: 1.1684 - acc: 0.6081\n",
      "Epoch 97/100\n",
      " - 13s - loss: 1.1584 - acc: 0.6026\n",
      "Epoch 98/100\n",
      " - 13s - loss: 1.1847 - acc: 0.5957\n",
      "Epoch 99/100\n",
      " - 13s - loss: 1.1775 - acc: 0.6048\n",
      "Epoch 100/100\n",
      " - 13s - loss: 1.1552 - acc: 0.6025\n",
      ">3: train=0.064094 test=0.017904\n",
      "          train      test\n",
      "count  3.000000  3.000000\n",
      "mean   0.050482  0.078201\n",
      "std    0.014724  0.056513\n",
      "min    0.034853  0.017904\n",
      "25%    0.043676  0.052323\n",
      "50%    0.052499  0.086743\n",
      "75%    0.058296  0.108350\n",
      "max    0.064094  0.129957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shourya97/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# define experiment\n",
    "model_name = 'baseline1'\n",
    "n_verbose = 2\n",
    "n_epochs = 100\n",
    "n_photos_per_update = 2\n",
    "n_batches_per_epoch = int(len(train) / n_photos_per_update)\n",
    "n_repeats = 3\n",
    "\n",
    "# run experiment\n",
    "train_results, test_results = list(), list()\n",
    "for i in range(n_repeats):\n",
    "    # define the model\n",
    "    model = define_model(vocab_size, max_length)\n",
    "    # define checkpoint callback\n",
    "#     filepath = 'baseline1-model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "#     checkpoint = ModelCheckpoint(filepath, monitor='val_loss',\n",
    "#                                  verbose=1,\n",
    "#                                  save_best_only=True,\n",
    "#                                  mode='min')\n",
    "    # fit model\n",
    "    model.fit_generator(data_generator(train_descriptions, train_features, tokenizer, max_length, n_photos_per_update), \n",
    "                        steps_per_epoch=n_batches_per_epoch, \n",
    "                        epochs=n_epochs,\n",
    "                        verbose=n_verbose)\n",
    "    # evaluate model on training data\n",
    "    train_score = evaluate_model(model, train_descriptions, train_features, tokenizer, max_length)\n",
    "    test_score = evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)\n",
    "    # store\n",
    "    train_results.append(train_score)\n",
    "    test_results.append(test_score)\n",
    "    print('>%d: train=%f test=%f' % ((i+1), train_score, test_score))\n",
    "# save results to file\n",
    "df = DataFrame()\n",
    "df['train'] = train_results\n",
    "df['test'] = test_results\n",
    "print(df.describe())\n",
    "df.to_csv(model_name+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU score table\n",
    "|      |    train |    test |\n",
    "|------|----------|---------|\n",
    "|count | 3.000000 | 3.000000|\n",
    "|mean  | 0.050482 | 0.078201|\n",
    "|std   | 0.014724 | 0.056513|\n",
    "|min   | 0.034853 | 0.017904|\n",
    "|25%   | 0.043676 | 0.052323|\n",
    "|50%   | 0.052499 | 0.086743|\n",
    "|75%   | 0.058296 | 0.108350|\n",
    "|max   | 0.064094 | 0.129957|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Photo Captions\n",
    "We expect the model is under-trained and maybe even under provisioned, but can it generate any kind of readable text at all?\n",
    "\n",
    "It is important that the baseline model have some modicum of capability so that we can relate the BLEU scores of the baseline to an idea of what kind of quality of descriptions are being generated.\n",
    "\n",
    "Let’s train a single model and generate a few descriptions from the train and test sets as a sanity check.\n",
    "\n",
    "Change the number of repeats to 1 and the name of the run to *‘baseline_generate‘*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'baseline_generate'\n",
    "one_repeats = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then update the *evaluate_model()* function to only evaluate the first 5 photos in the dataset and print the descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the skill of the model\n",
    "def evaluate_model_five(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    # step over the whole set\n",
    "    for key, desc in descriptions.items():\n",
    "        # generate descriptions\n",
    "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "        # store actual and predicted\n",
    "        actual.append([desc.split()])\n",
    "        predicted.append(yhat.split())\n",
    "        print('Actual : %s'%desc)\n",
    "        print('Predicted : %s' %yhat)\n",
    "        if len(actual) >= 5:\n",
    "            break\n",
    "        # calculate the BLEU score\n",
    "        bleu = corpus_bleu(actual, predicted)\n",
    "        return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual : startseq child and woman are at waters edge in big city endseq\n",
      "Predicted : startseq of people are waters waters near in city city city city city city city city city city city city city city city city city city\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shourya97/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual : startseq couple with young child wrapped in blanket sitting on concrete step endseq\n",
      "Predicted : startseq of in swinging is sword jumping bicycle is front is front is front is front is front is front is front is front is front\n",
      ">1: train=0.662215 test=0.526640\n"
     ]
    }
   ],
   "source": [
    "# Re-run the example.\n",
    "for i in range(one_repeats):\n",
    "    \n",
    "    # see results for the TRAIN set\n",
    "    # TODO\n",
    "    train_score = evaluate_model_five(model, train_descriptions, train_features, tokenizer, max_length)\n",
    "\n",
    "    # see results on the TEST dataset\n",
    "    # TODO\n",
    "    test_score = evaluate_model_five(model, test_descriptions, test_features, tokenizer, max_length)\n",
    "    \n",
    "    # store\n",
    "    train_results.append(train_score)\n",
    "    test_results.append(test_score)\n",
    "    print('>%d: train=%f test=%f' % ((i+1), train_score, test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Size Parameters\n",
    "In this section, we will see how gross variations to the network structure impact model skill.\n",
    "\n",
    "We will look at the following aspects of the model size:\n",
    "1. Size of the fixed-vector output from the ‘encoders’.\n",
    "2. Size of the sequence encoder model.\n",
    "3. Size of the language model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of the Fixed-Vector\n",
    "In the baseline model, the photo feature extractor and the text sequence encoder both output a 128 element vector. These vectors are then concatenated to be processed by the language model.\n",
    "\n",
    "The 128 element vector from each sub-model contains everything known about the input sequence and photo. We can vary the size of this vector to see if it impacts model skill\n",
    "\n",
    "First, we can **decrease the size by half from 128 elements to 64 elements**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "    \n",
    "    # feature extractor (encoder)\n",
    "    inputs1 = Input(shape=(7, 7, 512))\n",
    "    fe1 = GlobalMaxPooling2D()(inputs1)\n",
    "    fe2 = Dense(64, activation='relu')(fe1)\n",
    "    fe3 = RepeatVector(max_length)(fe2)\n",
    "    \n",
    "    # embedding\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    emb2 = Embedding(vocab_size, 50, mask_zero=True)(inputs2)\n",
    "    emb3 = LSTM(256, return_sequences=True)(emb2)\n",
    "    emb4 = TimeDistributed(Dense(64, activation='relu'))(emb3)\n",
    "    \n",
    "    # merge inputs\n",
    "    merged = concatenate([fe3, emb4])\n",
    "    \n",
    "    # language model (decoder)\n",
    "    lm2 = LSTM(500)(merged)\n",
    "    lm3 = Dense(500, activation='relu')(lm2)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(lm3)\n",
    "    \n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_name = 'size_sm_fixed_vec'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this experiment will give us a set of BLEU scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 16s - loss: 7.0975 - acc: 0.0426\n",
      "Epoch 2/100\n",
      " - 13s - loss: 5.5609 - acc: 0.0552\n",
      "Epoch 3/100\n",
      " - 15s - loss: 5.4550 - acc: 0.0489\n",
      "Epoch 4/100\n",
      " - 14s - loss: 5.4076 - acc: 0.0610\n",
      "Epoch 5/100\n",
      " - 14s - loss: 5.3715 - acc: 0.0610\n",
      "Epoch 6/100\n",
      " - 14s - loss: 5.3339 - acc: 0.0632\n",
      "Epoch 7/100\n",
      " - 13s - loss: 5.3205 - acc: 0.0720\n",
      "Epoch 8/100\n",
      " - 14s - loss: 5.2014 - acc: 0.0789\n",
      "Epoch 9/100\n",
      " - 13s - loss: 5.1589 - acc: 0.0793\n",
      "Epoch 10/100\n",
      " - 13s - loss: 5.0954 - acc: 0.0812\n",
      "Epoch 11/100\n",
      " - 13s - loss: 5.0367 - acc: 0.0822\n",
      "Epoch 12/100\n",
      " - 13s - loss: 4.9917 - acc: 0.0847\n",
      "Epoch 13/100\n",
      " - 13s - loss: 4.9791 - acc: 0.0799\n",
      "Epoch 14/100\n",
      " - 13s - loss: 4.9497 - acc: 0.0755\n",
      "Epoch 15/100\n",
      " - 13s - loss: 4.8993 - acc: 0.0778\n",
      "Epoch 16/100\n",
      " - 13s - loss: 4.8527 - acc: 0.0885\n",
      "Epoch 17/100\n",
      " - 13s - loss: 4.7829 - acc: 0.0856\n",
      "Epoch 18/100\n",
      " - 13s - loss: 4.7074 - acc: 0.0832\n",
      "Epoch 19/100\n",
      " - 13s - loss: 4.6290 - acc: 0.0894\n",
      "Epoch 20/100\n",
      " - 13s - loss: 4.5303 - acc: 0.0924\n",
      "Epoch 21/100\n",
      " - 13s - loss: 4.4911 - acc: 0.0949\n",
      "Epoch 22/100\n",
      " - 13s - loss: 4.4029 - acc: 0.0938\n",
      "Epoch 23/100\n",
      " - 13s - loss: 4.2505 - acc: 0.1022\n",
      "Epoch 24/100\n",
      " - 12s - loss: 4.1595 - acc: 0.0986\n",
      "Epoch 25/100\n",
      " - 13s - loss: 3.9840 - acc: 0.1060\n",
      "Epoch 26/100\n",
      " - 13s - loss: 3.8477 - acc: 0.1144\n",
      "Epoch 27/100\n",
      " - 13s - loss: 3.7462 - acc: 0.1130\n",
      "Epoch 28/100\n",
      " - 13s - loss: 3.5948 - acc: 0.1224\n",
      "Epoch 29/100\n",
      " - 13s - loss: 3.4866 - acc: 0.1432\n",
      "Epoch 30/100\n",
      " - 13s - loss: 3.4148 - acc: 0.1513\n",
      "Epoch 31/100\n",
      " - 13s - loss: 3.3274 - acc: 0.1517\n",
      "Epoch 32/100\n",
      " - 12s - loss: 3.2980 - acc: 0.1489\n",
      "Epoch 33/100\n",
      " - 13s - loss: 3.1842 - acc: 0.1624\n",
      "Epoch 34/100\n",
      " - 13s - loss: 3.1774 - acc: 0.1751\n",
      "Epoch 35/100\n",
      " - 12s - loss: 3.0171 - acc: 0.2021\n",
      "Epoch 36/100\n",
      " - 13s - loss: 2.8819 - acc: 0.2112\n",
      "Epoch 37/100\n",
      " - 13s - loss: 2.7808 - acc: 0.2000\n",
      "Epoch 38/100\n",
      " - 12s - loss: 2.6132 - acc: 0.2273\n",
      "Epoch 39/100\n",
      " - 13s - loss: 2.6005 - acc: 0.2487\n",
      "Epoch 40/100\n",
      " - 12s - loss: 2.3442 - acc: 0.2849\n",
      "Epoch 41/100\n",
      " - 13s - loss: 2.2731 - acc: 0.2981\n",
      "Epoch 42/100\n",
      " - 13s - loss: 2.1944 - acc: 0.2993\n",
      "Epoch 43/100\n",
      " - 12s - loss: 2.1383 - acc: 0.3260\n",
      "Epoch 44/100\n",
      " - 12s - loss: 2.0739 - acc: 0.3137\n",
      "Epoch 45/100\n",
      " - 13s - loss: 2.0911 - acc: 0.3130\n",
      "Epoch 46/100\n",
      " - 12s - loss: 2.0685 - acc: 0.2996\n",
      "Epoch 47/100\n",
      " - 13s - loss: 1.8787 - acc: 0.3449\n",
      "Epoch 48/100\n",
      " - 12s - loss: 1.8248 - acc: 0.3811\n",
      "Epoch 49/100\n",
      " - 13s - loss: 1.7436 - acc: 0.3941\n",
      "Epoch 50/100\n",
      " - 13s - loss: 1.6606 - acc: 0.4121\n",
      "Epoch 51/100\n",
      " - 13s - loss: 1.5925 - acc: 0.4363\n",
      "Epoch 52/100\n",
      " - 13s - loss: 1.5619 - acc: 0.4438\n",
      "Epoch 53/100\n",
      " - 13s - loss: 1.5812 - acc: 0.4466\n",
      "Epoch 54/100\n",
      " - 13s - loss: 1.5505 - acc: 0.4554\n",
      "Epoch 55/100\n",
      " - 13s - loss: 1.5363 - acc: 0.4397\n",
      "Epoch 56/100\n",
      " - 13s - loss: 1.4676 - acc: 0.4712\n",
      "Epoch 57/100\n",
      " - 12s - loss: 1.3747 - acc: 0.4781\n",
      "Epoch 58/100\n",
      " - 13s - loss: 1.3186 - acc: 0.5118\n",
      "Epoch 59/100\n",
      " - 13s - loss: 1.3012 - acc: 0.5341\n",
      "Epoch 60/100\n",
      " - 13s - loss: 1.3084 - acc: 0.5158\n",
      "Epoch 61/100\n",
      " - 13s - loss: 1.3080 - acc: 0.5366\n",
      "Epoch 62/100\n",
      " - 12s - loss: 1.2515 - acc: 0.5491\n",
      "Epoch 63/100\n",
      " - 13s - loss: 1.2274 - acc: 0.5566\n",
      "Epoch 64/100\n",
      " - 13s - loss: 1.1953 - acc: 0.5702\n",
      "Epoch 65/100\n",
      " - 13s - loss: 1.1625 - acc: 0.5688\n",
      "Epoch 66/100\n",
      " - 13s - loss: 1.1192 - acc: 0.5900\n",
      "Epoch 67/100\n",
      " - 13s - loss: 1.0499 - acc: 0.5972\n",
      "Epoch 68/100\n",
      " - 13s - loss: 1.0638 - acc: 0.6060\n",
      "Epoch 69/100\n",
      " - 13s - loss: 1.1158 - acc: 0.5838\n",
      "Epoch 70/100\n",
      " - 13s - loss: 1.0379 - acc: 0.6063\n",
      "Epoch 71/100\n",
      " - 13s - loss: 1.0157 - acc: 0.6355\n",
      "Epoch 72/100\n",
      " - 12s - loss: 1.0145 - acc: 0.6203\n",
      "Epoch 73/100\n",
      " - 13s - loss: 0.9470 - acc: 0.6712\n",
      "Epoch 74/100\n",
      " - 13s - loss: 0.9072 - acc: 0.6679\n",
      "Epoch 75/100\n",
      " - 13s - loss: 0.9045 - acc: 0.6776\n",
      "Epoch 76/100\n",
      " - 14s - loss: 0.8987 - acc: 0.6677\n",
      "Epoch 77/100\n",
      " - 12s - loss: 0.9211 - acc: 0.6700\n",
      "Epoch 78/100\n",
      " - 12s - loss: 0.9374 - acc: 0.6576\n",
      "Epoch 79/100\n",
      " - 12s - loss: 0.9513 - acc: 0.6385\n",
      "Epoch 80/100\n",
      " - 12s - loss: 0.9612 - acc: 0.6496\n",
      "Epoch 81/100\n",
      " - 12s - loss: 0.9123 - acc: 0.6588\n",
      "Epoch 82/100\n",
      " - 12s - loss: 0.8946 - acc: 0.6695\n",
      "Epoch 83/100\n",
      " - 12s - loss: 0.8990 - acc: 0.6661\n",
      "Epoch 84/100\n",
      " - 12s - loss: 0.9040 - acc: 0.6614\n",
      "Epoch 85/100\n",
      " - 12s - loss: 0.8444 - acc: 0.6804\n",
      "Epoch 86/100\n",
      " - 12s - loss: 0.8938 - acc: 0.6690\n",
      "Epoch 87/100\n",
      " - 12s - loss: 0.8438 - acc: 0.6998\n",
      "Epoch 88/100\n",
      " - 12s - loss: 0.8567 - acc: 0.6816\n",
      "Epoch 89/100\n",
      " - 12s - loss: 0.8941 - acc: 0.6822\n",
      "Epoch 90/100\n",
      " - 12s - loss: 0.8534 - acc: 0.6770\n",
      "Epoch 91/100\n",
      " - 12s - loss: 0.8448 - acc: 0.6946\n",
      "Epoch 92/100\n",
      " - 12s - loss: 0.7618 - acc: 0.7074\n",
      "Epoch 93/100\n",
      " - 12s - loss: 0.6970 - acc: 0.7445\n",
      "Epoch 94/100\n",
      " - 12s - loss: 0.6114 - acc: 0.7691\n",
      "Epoch 95/100\n",
      " - 12s - loss: 0.5910 - acc: 0.7806\n",
      "Epoch 96/100\n",
      " - 12s - loss: 0.5684 - acc: 0.7927\n",
      "Epoch 97/100\n",
      " - 12s - loss: 0.5352 - acc: 0.8021\n",
      "Epoch 98/100\n",
      " - 12s - loss: 0.5254 - acc: 0.8104\n",
      "Epoch 99/100\n",
      " - 12s - loss: 0.5381 - acc: 0.8142\n",
      "Epoch 100/100\n",
      " - 12s - loss: 0.5357 - acc: 0.8133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shourya97/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1: train=0.075113 test=0.094529\n",
      "Epoch 1/100\n",
      " - 15s - loss: 7.0440 - acc: 0.0450\n",
      "Epoch 2/100\n",
      " - 12s - loss: 5.6042 - acc: 0.0541\n",
      "Epoch 3/100\n",
      " - 12s - loss: 5.4410 - acc: 0.0693\n",
      "Epoch 4/100\n",
      " - 12s - loss: 5.3601 - acc: 0.0735\n",
      "Epoch 5/100\n",
      " - 12s - loss: 5.2654 - acc: 0.0721\n",
      "Epoch 6/100\n",
      " - 12s - loss: 5.2107 - acc: 0.0734\n",
      "Epoch 7/100\n",
      " - 12s - loss: 5.1355 - acc: 0.0791\n",
      "Epoch 8/100\n",
      " - 12s - loss: 5.0861 - acc: 0.0773\n",
      "Epoch 9/100\n",
      " - 12s - loss: 5.0190 - acc: 0.0778\n",
      "Epoch 10/100\n",
      " - 12s - loss: 4.9688 - acc: 0.0810\n",
      "Epoch 11/100\n",
      " - 12s - loss: 4.9052 - acc: 0.0807\n",
      "Epoch 12/100\n",
      " - 12s - loss: 4.7889 - acc: 0.0915\n",
      "Epoch 13/100\n",
      " - 12s - loss: 4.6790 - acc: 0.0865\n",
      "Epoch 14/100\n",
      " - 12s - loss: 4.5704 - acc: 0.0888\n",
      "Epoch 15/100\n",
      " - 12s - loss: 4.4163 - acc: 0.0959\n",
      "Epoch 16/100\n",
      " - 12s - loss: 4.3186 - acc: 0.0968\n",
      "Epoch 17/100\n",
      " - 12s - loss: 4.2112 - acc: 0.0826\n",
      "Epoch 18/100\n",
      " - 12s - loss: 4.0847 - acc: 0.0955\n",
      "Epoch 19/100\n",
      " - 12s - loss: 4.0030 - acc: 0.1091\n",
      "Epoch 20/100\n",
      " - 12s - loss: 3.8439 - acc: 0.1172\n",
      "Epoch 21/100\n",
      " - 12s - loss: 3.7092 - acc: 0.1211\n",
      "Epoch 22/100\n",
      " - 12s - loss: 3.6005 - acc: 0.1289\n",
      "Epoch 23/100\n",
      " - 12s - loss: 3.5280 - acc: 0.1339\n",
      "Epoch 24/100\n",
      " - 12s - loss: 3.3955 - acc: 0.1319\n",
      "Epoch 25/100\n",
      " - 12s - loss: 3.3127 - acc: 0.1531\n",
      "Epoch 26/100\n",
      " - 12s - loss: 3.4109 - acc: 0.1319\n",
      "Epoch 27/100\n",
      " - 12s - loss: 3.2378 - acc: 0.1467\n",
      "Epoch 28/100\n",
      " - 12s - loss: 3.2126 - acc: 0.1447\n",
      "Epoch 29/100\n",
      " - 12s - loss: 3.0187 - acc: 0.1928\n",
      "Epoch 30/100\n",
      " - 12s - loss: 2.8480 - acc: 0.2014\n",
      "Epoch 31/100\n",
      " - 12s - loss: 2.7021 - acc: 0.2290\n",
      "Epoch 32/100\n",
      " - 12s - loss: 2.5264 - acc: 0.2491\n",
      "Epoch 33/100\n",
      " - 12s - loss: 2.4064 - acc: 0.2766\n",
      "Epoch 34/100\n",
      " - 12s - loss: 2.3630 - acc: 0.2754\n",
      "Epoch 35/100\n",
      " - 12s - loss: 2.2137 - acc: 0.3154\n",
      "Epoch 36/100\n",
      " - 12s - loss: 2.0858 - acc: 0.3088\n",
      "Epoch 37/100\n",
      " - 12s - loss: 2.0205 - acc: 0.3518\n",
      "Epoch 38/100\n",
      " - 12s - loss: 1.9869 - acc: 0.3449\n",
      "Epoch 39/100\n",
      " - 12s - loss: 1.9353 - acc: 0.3379\n",
      "Epoch 40/100\n",
      " - 12s - loss: 1.8375 - acc: 0.3749\n",
      "Epoch 41/100\n",
      " - 12s - loss: 1.8239 - acc: 0.3620\n",
      "Epoch 42/100\n",
      " - 12s - loss: 1.8010 - acc: 0.3852\n",
      "Epoch 43/100\n",
      " - 12s - loss: 1.7307 - acc: 0.3998\n",
      "Epoch 44/100\n",
      " - 12s - loss: 1.6938 - acc: 0.4046\n",
      "Epoch 45/100\n",
      " - 13s - loss: 1.6749 - acc: 0.4215\n",
      "Epoch 46/100\n",
      " - 12s - loss: 1.6010 - acc: 0.4566\n",
      "Epoch 47/100\n",
      " - 12s - loss: 1.5754 - acc: 0.4533\n",
      "Epoch 48/100\n",
      " - 12s - loss: 1.5530 - acc: 0.4489\n",
      "Epoch 49/100\n",
      " - 12s - loss: 1.5291 - acc: 0.4679\n",
      "Epoch 50/100\n",
      " - 13s - loss: 1.6141 - acc: 0.4546\n",
      "Epoch 51/100\n",
      " - 14s - loss: 1.6202 - acc: 0.4349\n",
      "Epoch 52/100\n",
      " - 13s - loss: 1.5281 - acc: 0.4661\n",
      "Epoch 53/100\n",
      " - 13s - loss: 1.4959 - acc: 0.4567\n",
      "Epoch 54/100\n",
      " - 13s - loss: 1.4744 - acc: 0.4553\n",
      "Epoch 55/100\n",
      " - 13s - loss: 1.4044 - acc: 0.4966\n",
      "Epoch 56/100\n",
      " - 15s - loss: 1.2991 - acc: 0.5321\n",
      "Epoch 57/100\n",
      " - 16s - loss: 1.2526 - acc: 0.5375\n",
      "Epoch 58/100\n",
      " - 16s - loss: 1.1919 - acc: 0.5692\n",
      "Epoch 59/100\n",
      " - 15s - loss: 1.1713 - acc: 0.5689\n",
      "Epoch 60/100\n",
      " - 14s - loss: 1.2203 - acc: 0.5711\n",
      "Epoch 61/100\n",
      " - 14s - loss: 1.1389 - acc: 0.5802\n",
      "Epoch 62/100\n",
      " - 12s - loss: 1.0693 - acc: 0.6093\n",
      "Epoch 63/100\n",
      " - 12s - loss: 1.0476 - acc: 0.6075\n",
      "Epoch 64/100\n",
      " - 12s - loss: 1.0795 - acc: 0.5992\n",
      "Epoch 65/100\n",
      " - 12s - loss: 1.0289 - acc: 0.6177\n",
      "Epoch 66/100\n",
      " - 12s - loss: 1.0139 - acc: 0.6292\n",
      "Epoch 67/100\n",
      " - 12s - loss: 0.9572 - acc: 0.6408\n",
      "Epoch 68/100\n",
      " - 12s - loss: 0.9727 - acc: 0.6403\n",
      "Epoch 69/100\n",
      " - 12s - loss: 0.9538 - acc: 0.6469\n",
      "Epoch 70/100\n",
      " - 12s - loss: 0.9695 - acc: 0.6479\n",
      "Epoch 71/100\n",
      " - 12s - loss: 1.0136 - acc: 0.6287\n",
      "Epoch 72/100\n",
      " - 12s - loss: 0.9567 - acc: 0.6403\n",
      "Epoch 73/100\n",
      " - 13s - loss: 0.9711 - acc: 0.6301\n",
      "Epoch 74/100\n",
      " - 13s - loss: 0.9744 - acc: 0.6339\n",
      "Epoch 75/100\n",
      " - 13s - loss: 0.9708 - acc: 0.6398\n",
      "Epoch 76/100\n",
      " - 13s - loss: 0.9283 - acc: 0.6527\n",
      "Epoch 77/100\n",
      " - 13s - loss: 0.9540 - acc: 0.6692\n",
      "Epoch 78/100\n",
      " - 13s - loss: 0.9474 - acc: 0.6510\n",
      "Epoch 79/100\n",
      " - 13s - loss: 0.8939 - acc: 0.6782\n",
      "Epoch 80/100\n",
      " - 13s - loss: 0.8873 - acc: 0.6696\n",
      "Epoch 81/100\n",
      " - 13s - loss: 0.8948 - acc: 0.6567\n",
      "Epoch 82/100\n",
      " - 13s - loss: 0.8569 - acc: 0.6883\n",
      "Epoch 83/100\n",
      " - 13s - loss: 0.7685 - acc: 0.7271\n",
      "Epoch 84/100\n",
      " - 13s - loss: 0.7841 - acc: 0.7032\n",
      "Epoch 85/100\n",
      " - 13s - loss: 0.8224 - acc: 0.7089\n",
      "Epoch 86/100\n",
      " - 13s - loss: 0.7976 - acc: 0.6966\n",
      "Epoch 87/100\n",
      " - 13s - loss: 0.7142 - acc: 0.7381\n",
      "Epoch 88/100\n",
      " - 15s - loss: 0.6952 - acc: 0.7492\n",
      "Epoch 89/100\n",
      " - 13s - loss: 0.6975 - acc: 0.7440\n",
      "Epoch 90/100\n",
      " - 13s - loss: 0.6639 - acc: 0.7721\n",
      "Epoch 91/100\n",
      " - 13s - loss: 0.6298 - acc: 0.7753\n",
      "Epoch 92/100\n",
      " - 13s - loss: 0.6101 - acc: 0.7738\n",
      "Epoch 93/100\n",
      " - 14s - loss: 0.6405 - acc: 0.7679\n",
      "Epoch 94/100\n",
      " - 13s - loss: 0.6276 - acc: 0.7668\n",
      "Epoch 95/100\n",
      " - 12s - loss: 0.6238 - acc: 0.7683\n",
      "Epoch 96/100\n",
      " - 12s - loss: 0.6262 - acc: 0.7710\n",
      "Epoch 97/100\n",
      " - 12s - loss: 0.6509 - acc: 0.7550\n",
      "Epoch 98/100\n",
      " - 12s - loss: 0.6522 - acc: 0.7701\n",
      "Epoch 99/100\n",
      " - 12s - loss: 0.6226 - acc: 0.7755\n",
      "Epoch 100/100\n",
      " - 12s - loss: 0.6411 - acc: 0.7701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shourya97/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2: train=0.080115 test=0.020442\n",
      "Epoch 1/100\n",
      " - 15s - loss: 7.0697 - acc: 0.0457\n",
      "Epoch 2/100\n",
      " - 12s - loss: 5.5563 - acc: 0.0612\n",
      "Epoch 3/100\n",
      " - 12s - loss: 5.3860 - acc: 0.0745\n",
      "Epoch 4/100\n",
      " - 12s - loss: 5.3012 - acc: 0.0719\n",
      "Epoch 5/100\n",
      " - 12s - loss: 5.1845 - acc: 0.0793\n",
      "Epoch 6/100\n",
      " - 12s - loss: 5.1176 - acc: 0.0793\n",
      "Epoch 7/100\n",
      " - 12s - loss: 5.0335 - acc: 0.0812\n",
      "Epoch 8/100\n",
      " - 12s - loss: 4.9590 - acc: 0.0803\n",
      "Epoch 9/100\n",
      " - 12s - loss: 4.8838 - acc: 0.0838\n",
      "Epoch 10/100\n",
      " - 12s - loss: 4.8981 - acc: 0.0783\n",
      "Epoch 11/100\n",
      " - 12s - loss: 4.7327 - acc: 0.0835\n",
      "Epoch 12/100\n",
      " - 12s - loss: 4.6794 - acc: 0.0727\n",
      "Epoch 13/100\n",
      " - 12s - loss: 4.6102 - acc: 0.0893\n",
      "Epoch 14/100\n",
      " - 12s - loss: 4.4654 - acc: 0.0880\n",
      "Epoch 15/100\n",
      " - 12s - loss: 4.2754 - acc: 0.0925\n",
      "Epoch 16/100\n",
      " - 12s - loss: 4.1146 - acc: 0.0987\n",
      "Epoch 17/100\n",
      " - 12s - loss: 3.9607 - acc: 0.1241\n",
      "Epoch 18/100\n",
      " - 12s - loss: 3.8333 - acc: 0.1184\n",
      "Epoch 19/100\n",
      " - 12s - loss: 3.5637 - acc: 0.1361\n",
      "Epoch 20/100\n",
      " - 12s - loss: 3.4268 - acc: 0.1591\n",
      "Epoch 21/100\n",
      " - 12s - loss: 3.2552 - acc: 0.1551\n",
      "Epoch 22/100\n",
      " - 12s - loss: 3.1604 - acc: 0.1757\n",
      "Epoch 23/100\n",
      " - 12s - loss: 3.0709 - acc: 0.1739\n",
      "Epoch 24/100\n",
      " - 12s - loss: 3.0392 - acc: 0.1756\n",
      "Epoch 25/100\n",
      " - 12s - loss: 2.9075 - acc: 0.1810\n",
      "Epoch 26/100\n",
      " - 12s - loss: 2.6770 - acc: 0.2222\n",
      "Epoch 27/100\n",
      " - 12s - loss: 2.6458 - acc: 0.2495\n",
      "Epoch 28/100\n",
      " - 12s - loss: 2.4295 - acc: 0.2600\n",
      "Epoch 29/100\n",
      " - 12s - loss: 2.3215 - acc: 0.2688\n",
      "Epoch 30/100\n",
      " - 12s - loss: 2.2061 - acc: 0.2924\n",
      "Epoch 31/100\n",
      " - 12s - loss: 2.1241 - acc: 0.3151\n",
      "Epoch 32/100\n",
      " - 12s - loss: 2.0172 - acc: 0.3124\n",
      "Epoch 33/100\n",
      " - 12s - loss: 1.9498 - acc: 0.3537\n",
      "Epoch 34/100\n",
      " - 12s - loss: 1.9144 - acc: 0.3421\n",
      "Epoch 35/100\n",
      " - 12s - loss: 1.9258 - acc: 0.3566\n",
      "Epoch 36/100\n",
      " - 12s - loss: 1.8861 - acc: 0.3380\n",
      "Epoch 37/100\n",
      " - 12s - loss: 1.8428 - acc: 0.3570\n",
      "Epoch 38/100\n",
      " - 12s - loss: 1.7439 - acc: 0.3687\n",
      "Epoch 39/100\n",
      " - 12s - loss: 1.6933 - acc: 0.3742\n",
      "Epoch 40/100\n",
      " - 12s - loss: 1.6853 - acc: 0.3950\n",
      "Epoch 41/100\n",
      " - 12s - loss: 1.6570 - acc: 0.4205\n",
      "Epoch 42/100\n",
      " - 12s - loss: 1.6953 - acc: 0.3993\n",
      "Epoch 43/100\n",
      " - 12s - loss: 1.5250 - acc: 0.4577\n",
      "Epoch 44/100\n",
      " - 12s - loss: 1.4887 - acc: 0.4433\n",
      "Epoch 45/100\n",
      " - 12s - loss: 1.5217 - acc: 0.4561\n",
      "Epoch 46/100\n",
      " - 12s - loss: 1.4890 - acc: 0.4618\n",
      "Epoch 47/100\n",
      " - 12s - loss: 1.3738 - acc: 0.4771\n",
      "Epoch 48/100\n",
      " - 12s - loss: 1.3922 - acc: 0.4856\n",
      "Epoch 49/100\n",
      " - 12s - loss: 1.3795 - acc: 0.4974\n",
      "Epoch 50/100\n",
      " - 12s - loss: 1.2943 - acc: 0.5292\n",
      "Epoch 51/100\n",
      " - 12s - loss: 1.2228 - acc: 0.5494\n",
      "Epoch 52/100\n",
      " - 12s - loss: 1.2094 - acc: 0.5612\n",
      "Epoch 53/100\n",
      " - 12s - loss: 1.2064 - acc: 0.5585\n",
      "Epoch 54/100\n",
      " - 12s - loss: 1.1615 - acc: 0.5797\n",
      "Epoch 55/100\n",
      " - 12s - loss: 1.1126 - acc: 0.5897\n",
      "Epoch 56/100\n",
      " - 12s - loss: 1.0906 - acc: 0.6023\n",
      "Epoch 57/100\n",
      " - 12s - loss: 1.1305 - acc: 0.5830\n",
      "Epoch 58/100\n",
      " - 12s - loss: 1.0982 - acc: 0.6061\n",
      "Epoch 59/100\n",
      " - 12s - loss: 1.1224 - acc: 0.5924\n",
      "Epoch 60/100\n",
      " - 12s - loss: 1.0990 - acc: 0.5982\n",
      "Epoch 61/100\n",
      " - 12s - loss: 1.1046 - acc: 0.5821\n",
      "Epoch 62/100\n",
      " - 12s - loss: 1.0852 - acc: 0.5926\n",
      "Epoch 63/100\n",
      " - 12s - loss: 0.9810 - acc: 0.6357\n",
      "Epoch 64/100\n",
      " - 12s - loss: 0.9904 - acc: 0.6263\n",
      "Epoch 65/100\n",
      " - 12s - loss: 0.9872 - acc: 0.6469\n",
      "Epoch 66/100\n",
      " - 12s - loss: 0.9580 - acc: 0.6701\n",
      "Epoch 67/100\n",
      " - 12s - loss: 0.9446 - acc: 0.6451\n",
      "Epoch 68/100\n",
      " - 12s - loss: 0.8427 - acc: 0.6822\n",
      "Epoch 69/100\n",
      " - 12s - loss: 0.7974 - acc: 0.7047\n",
      "Epoch 70/100\n",
      " - 12s - loss: 0.7599 - acc: 0.7090\n",
      "Epoch 71/100\n",
      " - 12s - loss: 0.7582 - acc: 0.7165\n",
      "Epoch 72/100\n",
      " - 12s - loss: 0.7153 - acc: 0.7357\n",
      "Epoch 73/100\n",
      " - 12s - loss: 0.6534 - acc: 0.7674\n",
      "Epoch 74/100\n",
      " - 12s - loss: 0.6192 - acc: 0.7889\n",
      "Epoch 75/100\n",
      " - 12s - loss: 0.6399 - acc: 0.7734\n",
      "Epoch 76/100\n",
      " - 12s - loss: 0.6503 - acc: 0.7659\n",
      "Epoch 77/100\n",
      " - 12s - loss: 0.5677 - acc: 0.7957\n",
      "Epoch 78/100\n",
      " - 12s - loss: 0.5707 - acc: 0.8098\n",
      "Epoch 79/100\n",
      " - 12s - loss: 0.5569 - acc: 0.8022\n",
      "Epoch 80/100\n",
      " - 12s - loss: 0.5714 - acc: 0.8009\n",
      "Epoch 81/100\n",
      " - 12s - loss: 0.5552 - acc: 0.8058\n",
      "Epoch 82/100\n",
      " - 12s - loss: 0.5502 - acc: 0.8105\n",
      "Epoch 83/100\n",
      " - 12s - loss: 0.5079 - acc: 0.8060\n",
      "Epoch 84/100\n",
      " - 12s - loss: 0.4731 - acc: 0.8208\n",
      "Epoch 85/100\n",
      " - 12s - loss: 0.4569 - acc: 0.8458\n",
      "Epoch 86/100\n",
      " - 12s - loss: 0.4385 - acc: 0.8363\n",
      "Epoch 87/100\n",
      " - 12s - loss: 0.4146 - acc: 0.8587\n",
      "Epoch 88/100\n",
      " - 12s - loss: 0.3935 - acc: 0.8745\n",
      "Epoch 89/100\n",
      " - 12s - loss: 0.3670 - acc: 0.8779\n",
      "Epoch 90/100\n",
      " - 12s - loss: 0.3925 - acc: 0.8605\n",
      "Epoch 91/100\n",
      " - 12s - loss: 0.3758 - acc: 0.8667\n",
      "Epoch 92/100\n",
      " - 12s - loss: 0.3533 - acc: 0.8793\n",
      "Epoch 93/100\n",
      " - 12s - loss: 0.3451 - acc: 0.8787\n",
      "Epoch 94/100\n",
      " - 12s - loss: 0.3212 - acc: 0.9020\n",
      "Epoch 95/100\n",
      " - 12s - loss: 0.3084 - acc: 0.8849\n",
      "Epoch 96/100\n",
      " - 12s - loss: 0.3364 - acc: 0.8807\n",
      "Epoch 97/100\n",
      " - 12s - loss: 0.3089 - acc: 0.9000\n",
      "Epoch 98/100\n",
      " - 12s - loss: 0.3072 - acc: 0.9091\n",
      "Epoch 99/100\n",
      " - 12s - loss: 0.2697 - acc: 0.9113\n",
      "Epoch 100/100\n",
      " - 12s - loss: 0.2548 - acc: 0.9170\n",
      ">3: train=0.102938 test=0.071407\n",
      "          train      test\n",
      "count  3.000000  3.000000\n",
      "mean   0.086055  0.062126\n",
      "std    0.014833  0.037905\n",
      "min    0.075113  0.020442\n",
      "25%    0.077614  0.045925\n",
      "50%    0.080115  0.071407\n",
      "75%    0.091526  0.082968\n",
      "max    0.102938  0.094529\n"
     ]
    }
   ],
   "source": [
    "verbose = 2\n",
    "n_epochs = 100\n",
    "n_photos_per_update = 2\n",
    "n_batches_per_epoch = int(len(train) / n_photos_per_update)\n",
    "n_repeats = 3\n",
    "\n",
    "# run experiment\n",
    "train_results, test_results = list(), list()\n",
    "for i in range(n_repeats):\n",
    "    # define the model\n",
    "    model = define_model(vocab_size, max_length)\n",
    "    # define checkpoint callback\n",
    "#     filepath = 'size_sm_fixed_vec-model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "#     checkpoint = ModelCheckpoint(filepath, monitor='val_loss',\n",
    "#                                  verbose=1,\n",
    "#                                  save_best_only=True,\n",
    "#                                  mode='min')\n",
    "    # fit model\n",
    "    model.fit_generator(data_generator(train_descriptions, train_features, tokenizer, max_length, n_photos_per_update), \n",
    "                        steps_per_epoch=n_batches_per_epoch, \n",
    "                        epochs=n_epochs,\n",
    "                        verbose=verbose)\n",
    "    # evaluate model on training data\n",
    "    train_score = evaluate_model(model, train_descriptions, train_features, tokenizer, max_length)\n",
    "    test_score = evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)\n",
    "    # store\n",
    "    train_results.append(train_score)\n",
    "    test_results.append(test_score)\n",
    "    print('>%d: train=%f test=%f' % ((i+1), train_score, test_score))\n",
    "# save results to file\n",
    "df = DataFrame()\n",
    "df['train'] = train_results\n",
    "df['test'] = test_results\n",
    "print(df.describe())\n",
    "df.to_csv(model_name+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU score table\n",
    "|      |    train |    test |\n",
    "|------|----------|---------|\n",
    "|count | 3.000000 | 3.000000|\n",
    "|mean  | 0.086055 | 0.062126|\n",
    "|std   | 0.014833 | 0.037905|\n",
    "|min   | 0.075113 | 0.020442|\n",
    "|25%   | 0.077614 | 0.045925|\n",
    "|50%   | 0.080115 | 0.071407|\n",
    "|75%   | 0.091526 | 0.082968|\n",
    "|max   | 0.102938 | 0.094529|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also **double the size of the fixed-length vector from 128 to 256 units**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(vocab_size, max_length):\n",
    "    # feature extractor (encoder)\n",
    "    inputs1 = Input(shape=(7, 7, 512))\n",
    "    fe1 = GlobalMaxPooling2D()(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    fe3 = RepeatVector(max_length)(fe2)\n",
    "    # embedding\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    emb2 = Embedding(vocab_size, 50, mask_zero=True)(inputs2)\n",
    "    emb3 = LSTM(256, return_sequences=True)(emb2)\n",
    "    emb4 = TimeDistributed(Dense(256, activation='relu'))(emb3)\n",
    "    # merge inputs\n",
    "    merged = concatenate([fe3, emb4])\n",
    "    # language model (decoder)\n",
    "    lm2 = LSTM(500)(merged)\n",
    "    lm3 = Dense(500, activation='relu')(lm2)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(lm3)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_name = 'size_lg_fixed_vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 21s - loss: 7.0605 - acc: 0.0459\n",
      "Epoch 2/100\n",
      " - 16s - loss: 5.6234 - acc: 0.0540\n",
      "Epoch 3/100\n",
      " - 16s - loss: 5.4336 - acc: 0.0574\n",
      "Epoch 4/100\n",
      " - 16s - loss: 5.3677 - acc: 0.0572\n",
      "Epoch 5/100\n",
      " - 16s - loss: 5.3405 - acc: 0.0606\n",
      "Epoch 6/100\n",
      " - 16s - loss: 5.3216 - acc: 0.0630\n",
      "Epoch 7/100\n",
      " - 16s - loss: 5.3107 - acc: 0.0630\n",
      "Epoch 8/100\n",
      " - 16s - loss: 5.3215 - acc: 0.0630\n",
      "Epoch 9/100\n",
      " - 16s - loss: 5.3013 - acc: 0.0630\n",
      "Epoch 10/100\n",
      " - 16s - loss: 5.2888 - acc: 0.0630\n",
      "Epoch 11/100\n",
      " - 16s - loss: 5.2809 - acc: 0.0630\n",
      "Epoch 12/100\n",
      " - 16s - loss: 5.2736 - acc: 0.0630\n",
      "Epoch 13/100\n",
      " - 18s - loss: 5.2692 - acc: 0.0630\n",
      "Epoch 14/100\n",
      " - 16s - loss: 5.2641 - acc: 0.0630\n",
      "Epoch 15/100\n",
      " - 16s - loss: 5.2697 - acc: 0.0630\n",
      "Epoch 16/100\n",
      " - 16s - loss: 5.2962 - acc: 0.0630\n",
      "Epoch 17/100\n",
      " - 16s - loss: 5.3100 - acc: 0.0630\n",
      "Epoch 18/100\n",
      " - 16s - loss: 5.2929 - acc: 0.0608\n",
      "Epoch 19/100\n",
      " - 16s - loss: 5.2835 - acc: 0.0630\n",
      "Epoch 20/100\n",
      " - 16s - loss: 5.2698 - acc: 0.0630\n",
      "Epoch 21/100\n",
      " - 16s - loss: 5.2563 - acc: 0.0630\n",
      "Epoch 22/100\n",
      " - 16s - loss: 5.2436 - acc: 0.0646\n",
      "Epoch 23/100\n",
      " - 16s - loss: 5.2319 - acc: 0.0606\n",
      "Epoch 24/100\n",
      " - 17s - loss: 5.2241 - acc: 0.0638\n",
      "Epoch 25/100\n",
      " - 17s - loss: 5.2167 - acc: 0.0628\n",
      "Epoch 26/100\n",
      " - 16s - loss: 5.2088 - acc: 0.0634\n",
      "Epoch 27/100\n",
      " - 16s - loss: 5.2021 - acc: 0.0611\n",
      "Epoch 28/100\n",
      " - 16s - loss: 5.1981 - acc: 0.0623\n",
      "Epoch 29/100\n",
      " - 16s - loss: 5.1948 - acc: 0.0592\n",
      "Epoch 30/100\n",
      " - 16s - loss: 5.1915 - acc: 0.0581\n",
      "Epoch 31/100\n",
      " - 16s - loss: 5.1904 - acc: 0.0614\n",
      "Epoch 32/100\n",
      " - 16s - loss: 5.1862 - acc: 0.0648\n",
      "Epoch 33/100\n",
      " - 16s - loss: 5.1845 - acc: 0.0650\n",
      "Epoch 34/100\n",
      " - 16s - loss: 5.1817 - acc: 0.0633\n",
      "Epoch 35/100\n",
      " - 16s - loss: 5.1749 - acc: 0.0621\n",
      "Epoch 36/100\n",
      " - 16s - loss: 5.1638 - acc: 0.0647\n",
      "Epoch 37/100\n",
      " - 16s - loss: 5.1536 - acc: 0.0641\n",
      "Epoch 38/100\n",
      " - 16s - loss: 5.1445 - acc: 0.0646\n",
      "Epoch 39/100\n",
      " - 16s - loss: 5.1297 - acc: 0.0644\n",
      "Epoch 40/100\n",
      " - 16s - loss: 5.1226 - acc: 0.0653\n",
      "Epoch 41/100\n",
      " - 16s - loss: 5.1138 - acc: 0.0653\n",
      "Epoch 42/100\n",
      " - 16s - loss: 5.1045 - acc: 0.0663\n",
      "Epoch 43/100\n",
      " - 16s - loss: 5.0952 - acc: 0.0640\n",
      "Epoch 44/100\n",
      " - 16s - loss: 5.0888 - acc: 0.0660\n",
      "Epoch 45/100\n",
      " - 16s - loss: 5.0824 - acc: 0.0647\n",
      "Epoch 46/100\n",
      " - 16s - loss: 5.0779 - acc: 0.0582\n",
      "Epoch 47/100\n",
      " - 16s - loss: 5.0725 - acc: 0.0568\n",
      "Epoch 48/100\n",
      " - 16s - loss: 5.0705 - acc: 0.0570\n",
      "Epoch 49/100\n",
      " - 16s - loss: 5.0646 - acc: 0.0559\n",
      "Epoch 50/100\n",
      " - 16s - loss: 5.0595 - acc: 0.0583\n",
      "Epoch 51/100\n",
      " - 16s - loss: 5.0592 - acc: 0.0578\n",
      "Epoch 52/100\n",
      " - 16s - loss: 5.0524 - acc: 0.0604\n",
      "Epoch 53/100\n",
      " - 16s - loss: 5.0516 - acc: 0.0588\n",
      "Epoch 54/100\n",
      " - 16s - loss: 5.0471 - acc: 0.0577\n",
      "Epoch 55/100\n",
      " - 16s - loss: 5.0460 - acc: 0.0597\n",
      "Epoch 56/100\n",
      " - 16s - loss: 5.0443 - acc: 0.0568\n",
      "Epoch 57/100\n",
      " - 16s - loss: 5.0424 - acc: 0.0589\n",
      "Epoch 58/100\n",
      " - 16s - loss: 5.0450 - acc: 0.0551\n",
      "Epoch 59/100\n",
      " - 16s - loss: 5.0427 - acc: 0.0596\n",
      "Epoch 60/100\n",
      " - 16s - loss: 5.0416 - acc: 0.0588\n",
      "Epoch 61/100\n",
      " - 16s - loss: 5.0466 - acc: 0.0562\n",
      "Epoch 62/100\n",
      " - 16s - loss: 5.0439 - acc: 0.0586\n",
      "Epoch 63/100\n",
      " - 16s - loss: 5.0358 - acc: 0.0584\n",
      "Epoch 64/100\n",
      " - 16s - loss: 5.0352 - acc: 0.0551\n",
      "Epoch 65/100\n",
      " - 16s - loss: 5.0378 - acc: 0.0578\n",
      "Epoch 66/100\n",
      " - 16s - loss: 5.0321 - acc: 0.0586\n",
      "Epoch 67/100\n",
      " - 16s - loss: 5.0375 - acc: 0.0621\n",
      "Epoch 68/100\n",
      " - 16s - loss: 5.0315 - acc: 0.0612\n",
      "Epoch 69/100\n",
      " - 16s - loss: 5.0324 - acc: 0.0607\n",
      "Epoch 70/100\n",
      " - 16s - loss: 5.0297 - acc: 0.0607\n",
      "Epoch 71/100\n",
      " - 16s - loss: 5.0240 - acc: 0.0619\n",
      "Epoch 72/100\n",
      " - 16s - loss: 5.0226 - acc: 0.0591\n",
      "Epoch 73/100\n",
      " - 16s - loss: 5.0311 - acc: 0.0591\n",
      "Epoch 74/100\n",
      " - 16s - loss: 5.0230 - acc: 0.0599\n",
      "Epoch 75/100\n",
      " - 16s - loss: 5.0359 - acc: 0.0572\n",
      "Epoch 76/100\n",
      " - 16s - loss: 5.0283 - acc: 0.0572\n",
      "Epoch 77/100\n",
      " - 16s - loss: 5.0358 - acc: 0.0571\n",
      "Epoch 78/100\n",
      " - 16s - loss: 5.0484 - acc: 0.0567\n",
      "Epoch 79/100\n",
      " - 16s - loss: 5.0484 - acc: 0.0539\n",
      "Epoch 80/100\n",
      " - 16s - loss: 5.0885 - acc: 0.0621\n",
      "Epoch 81/100\n",
      " - 16s - loss: 5.0774 - acc: 0.0609\n",
      "Epoch 82/100\n",
      " - 16s - loss: 5.1216 - acc: 0.0621\n",
      "Epoch 83/100\n",
      " - 16s - loss: 5.1293 - acc: 0.0667\n",
      "Epoch 84/100\n",
      " - 16s - loss: 5.1407 - acc: 0.0630\n",
      "Epoch 85/100\n",
      " - 16s - loss: 5.1222 - acc: 0.0647\n",
      "Epoch 86/100\n",
      " - 16s - loss: 5.1155 - acc: 0.0607\n",
      "Epoch 87/100\n",
      " - 16s - loss: 5.0948 - acc: 0.0622\n",
      "Epoch 88/100\n",
      " - 16s - loss: 5.0907 - acc: 0.0655\n",
      "Epoch 89/100\n",
      " - 16s - loss: 5.0753 - acc: 0.0627\n",
      "Epoch 90/100\n",
      " - 16s - loss: 5.0967 - acc: 0.0608\n",
      "Epoch 91/100\n",
      " - 16s - loss: 5.1037 - acc: 0.0599\n",
      "Epoch 92/100\n",
      " - 16s - loss: 5.0694 - acc: 0.0615\n",
      "Epoch 93/100\n",
      " - 16s - loss: 5.0744 - acc: 0.0601\n",
      "Epoch 94/100\n",
      " - 16s - loss: 5.0767 - acc: 0.0632\n",
      "Epoch 95/100\n",
      " - 16s - loss: 5.0686 - acc: 0.0612\n",
      "Epoch 96/100\n",
      " - 16s - loss: 5.0713 - acc: 0.0613\n",
      "Epoch 97/100\n",
      " - 16s - loss: 5.0677 - acc: 0.0625\n",
      "Epoch 98/100\n",
      " - 16s - loss: 5.0699 - acc: 0.0621\n",
      "Epoch 99/100\n",
      " - 16s - loss: 5.0523 - acc: 0.0606\n",
      "Epoch 100/100\n",
      " - 16s - loss: 5.0627 - acc: 0.0616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shourya97/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1: train=0.493329 test=0.491720\n",
      "Epoch 1/100\n",
      " - 18s - loss: 7.0667 - acc: 0.0414\n",
      "Epoch 2/100\n",
      " - 16s - loss: 5.6120 - acc: 0.0516\n",
      "Epoch 3/100\n",
      " - 16s - loss: 5.4611 - acc: 0.0565\n",
      "Epoch 4/100\n",
      " - 16s - loss: 5.3836 - acc: 0.0610\n",
      "Epoch 5/100\n",
      " - 16s - loss: 5.3470 - acc: 0.0618\n",
      "Epoch 6/100\n",
      " - 16s - loss: 5.3296 - acc: 0.0630\n",
      "Epoch 7/100\n",
      " - 16s - loss: 5.3154 - acc: 0.0630\n",
      "Epoch 8/100\n",
      " - 16s - loss: 5.3013 - acc: 0.0630\n",
      "Epoch 9/100\n",
      " - 16s - loss: 5.2903 - acc: 0.0630\n",
      "Epoch 10/100\n",
      " - 16s - loss: 5.2811 - acc: 0.0630\n",
      "Epoch 11/100\n",
      " - 16s - loss: 5.2742 - acc: 0.0630\n",
      "Epoch 12/100\n",
      " - 16s - loss: 5.2658 - acc: 0.0630\n",
      "Epoch 13/100\n",
      " - 16s - loss: 5.2624 - acc: 0.0630\n",
      "Epoch 14/100\n",
      " - 16s - loss: 5.2517 - acc: 0.0630\n",
      "Epoch 15/100\n",
      " - 16s - loss: 5.2440 - acc: 0.0630\n",
      "Epoch 16/100\n",
      " - 16s - loss: 5.2358 - acc: 0.0630\n",
      "Epoch 17/100\n",
      " - 16s - loss: 5.2273 - acc: 0.0630\n",
      "Epoch 18/100\n",
      " - 16s - loss: 5.2203 - acc: 0.0630\n",
      "Epoch 19/100\n",
      " - 16s - loss: 5.2102 - acc: 0.0630\n",
      "Epoch 20/100\n",
      " - 16s - loss: 5.2018 - acc: 0.0630\n",
      "Epoch 21/100\n",
      " - 16s - loss: 5.1944 - acc: 0.0630\n",
      "Epoch 22/100\n",
      " - 16s - loss: 5.1852 - acc: 0.0600\n",
      "Epoch 23/100\n",
      " - 16s - loss: 5.1799 - acc: 0.0610\n",
      "Epoch 24/100\n",
      " - 16s - loss: 5.1672 - acc: 0.0647\n",
      "Epoch 25/100\n",
      " - 16s - loss: 5.1636 - acc: 0.0629\n",
      "Epoch 26/100\n",
      " - 16s - loss: 5.1531 - acc: 0.0646\n",
      "Epoch 27/100\n",
      " - 16s - loss: 5.1444 - acc: 0.0646\n",
      "Epoch 28/100\n",
      " - 16s - loss: 5.1380 - acc: 0.0651\n",
      "Epoch 29/100\n",
      " - 16s - loss: 5.1303 - acc: 0.0641\n",
      "Epoch 30/100\n",
      " - 16s - loss: 5.1223 - acc: 0.0626\n",
      "Epoch 31/100\n",
      " - 16s - loss: 5.1198 - acc: 0.0602\n",
      "Epoch 32/100\n",
      " - 16s - loss: 5.1162 - acc: 0.0613\n",
      "Epoch 33/100\n",
      " - 16s - loss: 5.1205 - acc: 0.0654\n",
      "Epoch 34/100\n",
      " - 16s - loss: 5.1289 - acc: 0.0630\n",
      "Epoch 35/100\n",
      " - 16s - loss: 5.1472 - acc: 0.0646\n",
      "Epoch 36/100\n",
      " - 16s - loss: 5.1427 - acc: 0.0663\n",
      "Epoch 37/100\n",
      " - 16s - loss: 5.1383 - acc: 0.0678\n",
      "Epoch 38/100\n",
      " - 16s - loss: 5.1397 - acc: 0.0673\n",
      "Epoch 39/100\n",
      " - 16s - loss: 5.1161 - acc: 0.0649\n",
      "Epoch 40/100\n",
      " - 16s - loss: 5.1240 - acc: 0.0670\n",
      "Epoch 41/100\n",
      " - 16s - loss: 5.0975 - acc: 0.0682\n",
      "Epoch 42/100\n",
      " - 16s - loss: 5.0936 - acc: 0.0703\n",
      "Epoch 43/100\n",
      " - 16s - loss: 5.0764 - acc: 0.0659\n",
      "Epoch 44/100\n",
      " - 16s - loss: 5.0801 - acc: 0.0623\n",
      "Epoch 45/100\n",
      " - 16s - loss: 5.0674 - acc: 0.0598\n",
      "Epoch 46/100\n",
      " - 16s - loss: 5.0659 - acc: 0.0583\n",
      "Epoch 47/100\n",
      " - 16s - loss: 5.0628 - acc: 0.0573\n",
      "Epoch 48/100\n",
      " - 16s - loss: 5.0594 - acc: 0.0583\n",
      "Epoch 49/100\n",
      " - 16s - loss: 5.0757 - acc: 0.0583\n",
      "Epoch 50/100\n",
      " - 16s - loss: 5.0625 - acc: 0.0574\n",
      "Epoch 51/100\n",
      " - 16s - loss: 5.0772 - acc: 0.0603\n",
      "Epoch 52/100\n",
      " - 16s - loss: 5.0698 - acc: 0.0561\n",
      "Epoch 53/100\n",
      " - 16s - loss: 5.0813 - acc: 0.0591\n",
      "Epoch 54/100\n",
      " - 16s - loss: 5.0760 - acc: 0.0591\n",
      "Epoch 55/100\n",
      " - 16s - loss: 5.0943 - acc: 0.0597\n",
      "Epoch 56/100\n",
      " - 16s - loss: 5.0854 - acc: 0.0597\n",
      "Epoch 57/100\n",
      " - 16s - loss: 5.1021 - acc: 0.0602\n",
      "Epoch 58/100\n",
      " - 16s - loss: 5.1157 - acc: 0.0626\n",
      "Epoch 59/100\n",
      " - 16s - loss: 5.1088 - acc: 0.0601\n",
      "Epoch 60/100\n",
      " - 16s - loss: 5.1346 - acc: 0.0607\n",
      "Epoch 61/100\n",
      " - 16s - loss: 5.1397 - acc: 0.0591\n",
      "Epoch 62/100\n",
      " - 16s - loss: 5.1216 - acc: 0.0587\n",
      "Epoch 63/100\n",
      " - 16s - loss: 5.1482 - acc: 0.0578\n",
      "Epoch 64/100\n",
      " - 16s - loss: 5.1275 - acc: 0.0578\n",
      "Epoch 65/100\n",
      " - 16s - loss: 5.1157 - acc: 0.0596\n",
      "Epoch 66/100\n",
      " - 16s - loss: 5.1500 - acc: 0.0599\n",
      "Epoch 67/100\n",
      " - 16s - loss: 5.1115 - acc: 0.0606\n",
      "Epoch 68/100\n",
      " - 16s - loss: 5.1018 - acc: 0.0586\n",
      "Epoch 69/100\n",
      " - 16s - loss: 5.1104 - acc: 0.0589\n",
      "Epoch 70/100\n",
      " - 16s - loss: 5.1056 - acc: 0.0645\n",
      "Epoch 71/100\n",
      " - 16s - loss: 5.0654 - acc: 0.0555\n",
      "Epoch 72/100\n",
      " - 16s - loss: 5.0921 - acc: 0.0621\n",
      "Epoch 73/100\n",
      " - 16s - loss: 5.0773 - acc: 0.0621\n",
      "Epoch 74/100\n",
      " - 16s - loss: 5.0766 - acc: 0.0650\n",
      "Epoch 75/100\n",
      " - 16s - loss: 5.0727 - acc: 0.0661\n",
      "Epoch 76/100\n",
      " - 16s - loss: 5.0635 - acc: 0.0645\n",
      "Epoch 77/100\n",
      " - 16s - loss: 5.0597 - acc: 0.0621\n",
      "Epoch 78/100\n",
      " - 16s - loss: 5.0583 - acc: 0.0668\n",
      "Epoch 79/100\n",
      " - 16s - loss: 5.0525 - acc: 0.0647\n",
      "Epoch 80/100\n",
      " - 16s - loss: 5.0516 - acc: 0.0636\n",
      "Epoch 81/100\n",
      " - 16s - loss: 5.0604 - acc: 0.0624\n",
      "Epoch 82/100\n",
      " - 16s - loss: 5.0508 - acc: 0.0611\n",
      "Epoch 83/100\n",
      " - 16s - loss: 5.0542 - acc: 0.0657\n",
      "Epoch 84/100\n",
      " - 16s - loss: 5.0433 - acc: 0.0684\n",
      "Epoch 85/100\n",
      " - 16s - loss: 5.0277 - acc: 0.0644\n",
      "Epoch 86/100\n",
      " - 16s - loss: 5.0489 - acc: 0.0635\n",
      "Epoch 87/100\n",
      " - 16s - loss: 5.0408 - acc: 0.0653\n",
      "Epoch 88/100\n",
      " - 16s - loss: 5.0353 - acc: 0.0647\n",
      "Epoch 89/100\n",
      " - 16s - loss: 5.0436 - acc: 0.0627\n",
      "Epoch 90/100\n",
      " - 16s - loss: 5.0362 - acc: 0.0683\n",
      "Epoch 91/100\n",
      " - 16s - loss: 5.0273 - acc: 0.0666\n",
      "Epoch 92/100\n",
      " - 16s - loss: 5.0248 - acc: 0.0633\n",
      "Epoch 93/100\n",
      " - 16s - loss: 5.0362 - acc: 0.0591\n",
      "Epoch 94/100\n",
      " - 16s - loss: 5.0318 - acc: 0.0628\n",
      "Epoch 95/100\n",
      " - 16s - loss: 5.0166 - acc: 0.0647\n",
      "Epoch 96/100\n",
      " - 16s - loss: 5.0296 - acc: 0.0651\n",
      "Epoch 97/100\n",
      " - 16s - loss: 5.0193 - acc: 0.0674\n",
      "Epoch 98/100\n",
      " - 16s - loss: 5.0170 - acc: 0.0655\n",
      "Epoch 99/100\n",
      " - 16s - loss: 5.0372 - acc: 0.0675\n",
      "Epoch 100/100\n",
      " - 16s - loss: 5.0150 - acc: 0.0627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shourya97/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2: train=0.086027 test=0.085906\n",
      "Epoch 1/100\n",
      " - 19s - loss: 7.0645 - acc: 0.0390\n",
      "Epoch 2/100\n",
      " - 16s - loss: 5.6390 - acc: 0.0568\n",
      "Epoch 3/100\n",
      " - 16s - loss: 5.4739 - acc: 0.0630\n",
      "Epoch 4/100\n",
      " - 16s - loss: 5.3894 - acc: 0.0630\n",
      "Epoch 5/100\n",
      " - 16s - loss: 5.3588 - acc: 0.0630\n",
      "Epoch 6/100\n",
      " - 16s - loss: 5.3383 - acc: 0.0630\n",
      "Epoch 7/100\n",
      " - 16s - loss: 5.3203 - acc: 0.0630\n",
      "Epoch 8/100\n",
      " - 16s - loss: 5.3035 - acc: 0.0630\n",
      "Epoch 9/100\n",
      " - 16s - loss: 5.2894 - acc: 0.0630\n",
      "Epoch 10/100\n",
      " - 16s - loss: 5.3163 - acc: 0.0630\n",
      "Epoch 11/100\n",
      " - 16s - loss: 5.2899 - acc: 0.0630\n",
      "Epoch 12/100\n",
      " - 16s - loss: 5.2831 - acc: 0.0630\n",
      "Epoch 13/100\n",
      " - 16s - loss: 5.2854 - acc: 0.0630\n",
      "Epoch 14/100\n",
      " - 16s - loss: 5.2841 - acc: 0.0630\n",
      "Epoch 15/100\n",
      " - 16s - loss: 5.2770 - acc: 0.0630\n",
      "Epoch 16/100\n",
      " - 16s - loss: 5.2904 - acc: 0.0630\n",
      "Epoch 17/100\n",
      " - 16s - loss: 5.2517 - acc: 0.0630\n",
      "Epoch 18/100\n",
      " - 16s - loss: 5.2671 - acc: 0.0630\n",
      "Epoch 19/100\n",
      " - 16s - loss: 5.2454 - acc: 0.0630\n",
      "Epoch 20/100\n",
      " - 16s - loss: 5.2484 - acc: 0.0630\n",
      "Epoch 21/100\n",
      " - 16s - loss: 5.2411 - acc: 0.0630\n",
      "Epoch 22/100\n",
      " - 16s - loss: 5.2124 - acc: 0.0630\n",
      "Epoch 23/100\n",
      " - 16s - loss: 5.2023 - acc: 0.0648\n",
      "Epoch 24/100\n",
      " - 16s - loss: 5.1938 - acc: 0.0640\n",
      "Epoch 25/100\n",
      " - 16s - loss: 5.1845 - acc: 0.0638\n",
      "Epoch 26/100\n",
      " - 16s - loss: 5.1753 - acc: 0.0618\n",
      "Epoch 27/100\n",
      " - 16s - loss: 5.1667 - acc: 0.0596\n",
      "Epoch 28/100\n",
      " - 16s - loss: 5.1599 - acc: 0.0612\n",
      "Epoch 29/100\n",
      " - 16s - loss: 5.1531 - acc: 0.0590\n",
      "Epoch 30/100\n",
      " - 16s - loss: 5.1492 - acc: 0.0574\n",
      "Epoch 31/100\n",
      " - 16s - loss: 5.1442 - acc: 0.0591\n",
      "Epoch 32/100\n",
      " - 16s - loss: 5.1408 - acc: 0.0554\n",
      "Epoch 33/100\n",
      " - 16s - loss: 5.1449 - acc: 0.0627\n",
      "Epoch 34/100\n",
      " - 16s - loss: 5.1514 - acc: 0.0576\n",
      "Epoch 35/100\n",
      " - 16s - loss: 5.1615 - acc: 0.0577\n",
      "Epoch 36/100\n",
      " - 16s - loss: 5.1723 - acc: 0.0585\n",
      "Epoch 37/100\n",
      " - 16s - loss: 5.1700 - acc: 0.0621\n",
      "Epoch 38/100\n",
      " - 16s - loss: 5.1556 - acc: 0.0679\n",
      "Epoch 39/100\n",
      " - 16s - loss: 5.1446 - acc: 0.0652\n",
      "Epoch 40/100\n",
      " - 16s - loss: 5.1280 - acc: 0.0660\n",
      "Epoch 41/100\n",
      " - 16s - loss: 5.1211 - acc: 0.0662\n",
      "Epoch 42/100\n",
      " - 16s - loss: 5.1063 - acc: 0.0620\n",
      "Epoch 43/100\n",
      " - 16s - loss: 5.0953 - acc: 0.0647\n",
      "Epoch 44/100\n",
      " - 16s - loss: 5.0900 - acc: 0.0620\n",
      "Epoch 45/100\n",
      " - 16s - loss: 5.0791 - acc: 0.0630\n",
      "Epoch 46/100\n",
      " - 16s - loss: 5.0778 - acc: 0.0592\n",
      "Epoch 47/100\n",
      " - 16s - loss: 5.0675 - acc: 0.0607\n",
      "Epoch 48/100\n",
      " - 16s - loss: 5.0609 - acc: 0.0644\n",
      "Epoch 49/100\n",
      " - 16s - loss: 5.0486 - acc: 0.0620\n",
      "Epoch 50/100\n",
      " - 16s - loss: 5.0498 - acc: 0.0650\n",
      "Epoch 51/100\n",
      " - 16s - loss: 5.0455 - acc: 0.0658\n",
      "Epoch 52/100\n",
      " - 16s - loss: 5.0460 - acc: 0.0613\n",
      "Epoch 53/100\n",
      " - 16s - loss: 5.0437 - acc: 0.0630\n",
      "Epoch 54/100\n",
      " - 16s - loss: 5.0469 - acc: 0.0612\n",
      "Epoch 55/100\n",
      " - 16s - loss: 5.0399 - acc: 0.0612\n",
      "Epoch 56/100\n",
      " - 16s - loss: 5.0432 - acc: 0.0602\n",
      "Epoch 57/100\n",
      " - 16s - loss: 5.0474 - acc: 0.0591\n",
      "Epoch 58/100\n",
      " - 16s - loss: 5.0570 - acc: 0.0591\n",
      "Epoch 59/100\n",
      " - 16s - loss: 5.0689 - acc: 0.0559\n",
      "Epoch 60/100\n",
      " - 16s - loss: 5.0778 - acc: 0.0586\n",
      "Epoch 61/100\n",
      " - 16s - loss: 5.0954 - acc: 0.0589\n",
      "Epoch 62/100\n",
      " - 16s - loss: 5.1010 - acc: 0.0636\n",
      "Epoch 63/100\n",
      " - 16s - loss: 5.1181 - acc: 0.0587\n",
      "Epoch 64/100\n",
      " - 16s - loss: 5.1349 - acc: 0.0644\n",
      "Epoch 65/100\n",
      " - 16s - loss: 5.1824 - acc: 0.0637\n",
      "Epoch 66/100\n",
      " - 16s - loss: 5.1429 - acc: 0.0637\n",
      "Epoch 67/100\n",
      " - 16s - loss: 5.1739 - acc: 0.0678\n",
      "Epoch 68/100\n",
      " - 16s - loss: 5.1351 - acc: 0.0638\n",
      "Epoch 69/100\n",
      " - 16s - loss: 5.1575 - acc: 0.0673\n",
      "Epoch 70/100\n",
      " - 16s - loss: 5.1212 - acc: 0.0628\n",
      "Epoch 71/100\n",
      " - 16s - loss: 5.1349 - acc: 0.0678\n",
      "Epoch 72/100\n",
      " - 16s - loss: 5.1175 - acc: 0.0645\n",
      "Epoch 73/100\n",
      " - 16s - loss: 5.1090 - acc: 0.0645\n",
      "Epoch 74/100\n",
      " - 16s - loss: 5.1042 - acc: 0.0704\n",
      "Epoch 75/100\n",
      " - 16s - loss: 5.1019 - acc: 0.0643\n",
      "Epoch 76/100\n",
      " - 16s - loss: 5.0840 - acc: 0.0662\n",
      "Epoch 77/100\n",
      " - 16s - loss: 5.0970 - acc: 0.0652\n",
      "Epoch 78/100\n",
      " - 16s - loss: 5.0922 - acc: 0.0653\n",
      "Epoch 79/100\n",
      " - 16s - loss: 5.0907 - acc: 0.0615\n",
      "Epoch 80/100\n",
      " - 16s - loss: 5.0829 - acc: 0.0595\n",
      "Epoch 81/100\n",
      " - 16s - loss: 5.0794 - acc: 0.0585\n",
      "Epoch 82/100\n",
      " - 16s - loss: 5.0585 - acc: 0.0569\n",
      "Epoch 83/100\n",
      " - 16s - loss: 5.0837 - acc: 0.0577\n",
      "Epoch 84/100\n",
      " - 16s - loss: 5.0586 - acc: 0.0571\n",
      "Epoch 85/100\n",
      " - 16s - loss: 5.0662 - acc: 0.0569\n",
      "Epoch 86/100\n",
      " - 16s - loss: 5.0615 - acc: 0.0593\n",
      "Epoch 87/100\n",
      " - 16s - loss: 5.0575 - acc: 0.0603\n",
      "Epoch 88/100\n",
      " - 16s - loss: 5.0545 - acc: 0.0584\n",
      "Epoch 89/100\n",
      " - 16s - loss: 5.0490 - acc: 0.0584\n",
      "Epoch 90/100\n",
      " - 16s - loss: 5.0459 - acc: 0.0584\n",
      "Epoch 91/100\n",
      " - 16s - loss: 5.0393 - acc: 0.0592\n",
      "Epoch 92/100\n",
      " - 16s - loss: 5.0381 - acc: 0.0579\n",
      "Epoch 93/100\n",
      " - 16s - loss: 5.0406 - acc: 0.0569\n",
      "Epoch 94/100\n",
      " - 16s - loss: 5.0418 - acc: 0.0608\n",
      "Epoch 95/100\n",
      " - 16s - loss: 5.0504 - acc: 0.0601\n",
      "Epoch 96/100\n",
      " - 16s - loss: 5.0428 - acc: 0.0564\n",
      "Epoch 97/100\n",
      " - 16s - loss: 5.0531 - acc: 0.0612\n",
      "Epoch 98/100\n",
      " - 16s - loss: 5.0502 - acc: 0.0593\n",
      "Epoch 99/100\n",
      " - 16s - loss: 5.0417 - acc: 0.0624\n",
      "Epoch 100/100\n",
      " - 16s - loss: 5.0468 - acc: 0.0596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shourya97/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">3: train=0.022001 test=0.022292\n",
      "          train      test\n",
      "count  3.000000  3.000000\n",
      "mean   0.200452  0.199972\n",
      "std    0.255651  0.254655\n",
      "min    0.022001  0.022292\n",
      "25%    0.054014  0.054099\n",
      "50%    0.086027  0.085906\n",
      "75%    0.289678  0.288813\n",
      "max    0.493329  0.491720\n"
     ]
    }
   ],
   "source": [
    "verbose = 2\n",
    "n_epochs = 100\n",
    "n_photos_per_update = 2\n",
    "n_batches_per_epoch = int(len(train) / n_photos_per_update)\n",
    "n_repeats = 3\n",
    "\n",
    "# run experiment\n",
    "train_results, test_results = list(), list()\n",
    "for i in range(n_repeats):\n",
    "    # define the model\n",
    "    model = define_model(vocab_size, max_length)\n",
    "    # define checkpoint callback\n",
    "#     filepath = 'size_lg_fixed_vec-model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "#     checkpoint = ModelCheckpoint(filepath, monitor='val_loss',\n",
    "#                                  verbose=1,\n",
    "#                                  save_best_only=True,\n",
    "#                                  mode='min')\n",
    "    # fit model\n",
    "    model.fit_generator(data_generator(train_descriptions, train_features, tokenizer, max_length, n_photos_per_update), \n",
    "                        steps_per_epoch=n_batches_per_epoch, \n",
    "                        epochs=n_epochs,\n",
    "                        verbose=verbose)\n",
    "    # evaluate model on training data\n",
    "    train_score = evaluate_model(model, train_descriptions, train_features, tokenizer, max_length)\n",
    "    test_score = evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)\n",
    "    # store\n",
    "    train_results.append(train_score)\n",
    "    test_results.append(test_score)\n",
    "    print('>%d: train=%f test=%f' % ((i+1), train_score, test_score))\n",
    "# save results to file\n",
    "df = DataFrame()\n",
    "df['train'] = train_results\n",
    "df['test'] = test_results\n",
    "print(df.describe())\n",
    "df.to_csv(model_name+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU scores table\n",
    "|      |    train |     test|\n",
    "|------|----------|---------|\n",
    "|count | 3.000000 | 3.000000|\n",
    "|mean  | 0.200452 | 0.199972|\n",
    "|std   | 0.255651 | 0.254655|\n",
    "|min   | 0.022001 | 0.022292|\n",
    "|25%   | 0.054014 | 0.054099|\n",
    "|50%   | 0.086027 | 0.085906|\n",
    "|75%   | 0.289678 | 0.288813|\n",
    "|max   | 0.493329 | 0.491720|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Encoder Size\n",
    "We can call the sub-model that interprets the input sequence of words generated so far as the sequence encoder.\n",
    "\n",
    "First, we can try to see if decreasing the representational capacity of the sequence encoder impacts model skill. We can **reduce the number of memory units in the LSTM layer from 256 to 128**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "    # feature extractor (encoder)\n",
    "    inputs1 = Input(shape=(7, 7, 512))\n",
    "    fe1 = GlobalMaxPooling2D()(inputs1)\n",
    "    fe2 = Dense(128, activation='relu')(fe1)\n",
    "    fe3 = RepeatVector(max_length)(fe2)\n",
    "    # embedding\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    emb2 = Embedding(vocab_size, 50, mask_zero=True)(inputs2)\n",
    "    emb3 = LSTM(128, return_sequences=True)(emb2)\n",
    "    emb4 = TimeDistributed(Dense(128, activation='relu'))(emb3)\n",
    "    # merge inputs\n",
    "    merged = concatenate([fe3, emb4])\n",
    "    # language model (decoder)\n",
    "    lm2 = LSTM(500)(merged)\n",
    "    lm3 = Dense(500, activation='relu')(lm2)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(lm3)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_name = 'size_sm_seq_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 16s - loss: 7.0777 - acc: 0.0428\n",
      "Epoch 2/100\n",
      " - 13s - loss: 5.6050 - acc: 0.0566\n",
      "Epoch 3/100\n",
      " - 13s - loss: 5.4457 - acc: 0.0622\n",
      "Epoch 4/100\n",
      " - 13s - loss: 5.4096 - acc: 0.0602\n",
      "Epoch 5/100\n",
      " - 13s - loss: 5.3675 - acc: 0.0630\n",
      "Epoch 6/100\n",
      " - 13s - loss: 5.3292 - acc: 0.0656\n",
      "Epoch 7/100\n",
      " - 13s - loss: 5.3230 - acc: 0.0711\n",
      "Epoch 8/100\n",
      " - 13s - loss: 5.2678 - acc: 0.0754\n",
      "Epoch 9/100\n",
      " - 13s - loss: 5.2184 - acc: 0.0734\n",
      "Epoch 10/100\n",
      " - 13s - loss: 5.1838 - acc: 0.0671\n",
      "Epoch 11/100\n",
      " - 13s - loss: 5.1326 - acc: 0.0733\n",
      "Epoch 12/100\n",
      " - 13s - loss: 5.1119 - acc: 0.0719\n",
      "Epoch 13/100\n",
      " - 13s - loss: 5.0824 - acc: 0.0760\n",
      "Epoch 14/100\n",
      " - 13s - loss: 5.0469 - acc: 0.0749\n",
      "Epoch 15/100\n",
      " - 13s - loss: 5.0376 - acc: 0.0762\n",
      "Epoch 16/100\n",
      " - 13s - loss: 5.0000 - acc: 0.0726\n",
      "Epoch 17/100\n",
      " - 13s - loss: 4.9818 - acc: 0.0713\n",
      "Epoch 18/100\n",
      " - 13s - loss: 4.9697 - acc: 0.0785\n",
      "Epoch 19/100\n",
      " - 13s - loss: 4.9477 - acc: 0.0733\n",
      "Epoch 20/100\n",
      " - 13s - loss: 4.9242 - acc: 0.0788\n",
      "Epoch 21/100\n",
      " - 13s - loss: 4.8942 - acc: 0.0821\n",
      "Epoch 22/100\n",
      " - 13s - loss: 4.8899 - acc: 0.0777\n",
      "Epoch 23/100\n",
      " - 13s - loss: 4.8426 - acc: 0.0882\n",
      "Epoch 24/100\n",
      " - 13s - loss: 4.7966 - acc: 0.0903\n",
      "Epoch 25/100\n",
      " - 13s - loss: 4.7680 - acc: 0.0832\n",
      "Epoch 26/100\n",
      " - 13s - loss: 4.7823 - acc: 0.0865\n",
      "Epoch 27/100\n",
      " - 13s - loss: 4.7829 - acc: 0.0887\n",
      "Epoch 28/100\n",
      " - 13s - loss: 4.6971 - acc: 0.0863\n",
      "Epoch 29/100\n",
      " - 13s - loss: 4.6507 - acc: 0.0855\n",
      "Epoch 30/100\n",
      " - 13s - loss: 4.6702 - acc: 0.0855\n",
      "Epoch 31/100\n",
      " - 13s - loss: 4.6629 - acc: 0.0906\n",
      "Epoch 32/100\n",
      " - 13s - loss: 4.6320 - acc: 0.0876\n",
      "Epoch 33/100\n",
      " - 13s - loss: 4.6016 - acc: 0.0836\n",
      "Epoch 34/100\n",
      " - 13s - loss: 4.5465 - acc: 0.0958\n",
      "Epoch 35/100\n",
      " - 13s - loss: 4.5051 - acc: 0.0811\n",
      "Epoch 36/100\n",
      " - 13s - loss: 4.4934 - acc: 0.0908\n",
      "Epoch 37/100\n",
      " - 13s - loss: 4.4130 - acc: 0.0924\n",
      "Epoch 38/100\n",
      " - 13s - loss: 4.4277 - acc: 0.0961\n",
      "Epoch 39/100\n",
      " - 13s - loss: 4.3317 - acc: 0.0898\n",
      "Epoch 40/100\n",
      " - 13s - loss: 4.3270 - acc: 0.0925\n",
      "Epoch 41/100\n",
      " - 13s - loss: 4.3157 - acc: 0.0893\n",
      "Epoch 42/100\n",
      " - 13s - loss: 4.2366 - acc: 0.0889\n",
      "Epoch 43/100\n",
      " - 13s - loss: 4.1957 - acc: 0.0966\n",
      "Epoch 44/100\n",
      " - 13s - loss: 4.1405 - acc: 0.1011\n",
      "Epoch 45/100\n",
      " - 13s - loss: 4.0910 - acc: 0.1034\n",
      "Epoch 46/100\n",
      " - 13s - loss: 4.0077 - acc: 0.1208\n",
      "Epoch 47/100\n",
      " - 13s - loss: 4.0145 - acc: 0.1051\n",
      "Epoch 48/100\n",
      " - 13s - loss: 3.9507 - acc: 0.1077\n",
      "Epoch 49/100\n",
      " - 13s - loss: 3.8704 - acc: 0.1217\n",
      "Epoch 50/100\n",
      " - 13s - loss: 3.8335 - acc: 0.1155\n",
      "Epoch 51/100\n",
      " - 13s - loss: 3.8546 - acc: 0.1137\n",
      "Epoch 52/100\n",
      " - 13s - loss: 3.7843 - acc: 0.1368\n",
      "Epoch 53/100\n",
      " - 13s - loss: 3.7863 - acc: 0.1201\n",
      "Epoch 54/100\n",
      " - 13s - loss: 3.6790 - acc: 0.1346\n",
      "Epoch 55/100\n",
      " - 13s - loss: 3.6103 - acc: 0.1315\n",
      "Epoch 56/100\n",
      " - 13s - loss: 3.5048 - acc: 0.1482\n",
      "Epoch 57/100\n",
      " - 13s - loss: 3.4800 - acc: 0.1485\n",
      "Epoch 58/100\n",
      " - 13s - loss: 3.4714 - acc: 0.1559\n",
      "Epoch 59/100\n",
      " - 13s - loss: 3.4115 - acc: 0.1419\n",
      "Epoch 60/100\n",
      " - 13s - loss: 3.4016 - acc: 0.1379\n",
      "Epoch 61/100\n",
      " - 13s - loss: 3.3543 - acc: 0.1475\n",
      "Epoch 62/100\n",
      " - 13s - loss: 3.3209 - acc: 0.1582\n",
      "Epoch 63/100\n",
      " - 13s - loss: 3.2591 - acc: 0.1623\n",
      "Epoch 64/100\n",
      " - 13s - loss: 3.2611 - acc: 0.1665\n",
      "Epoch 65/100\n",
      " - 13s - loss: 3.2000 - acc: 0.1599\n",
      "Epoch 66/100\n",
      " - 13s - loss: 3.1246 - acc: 0.1958\n",
      "Epoch 67/100\n",
      " - 13s - loss: 3.0306 - acc: 0.1754\n",
      "Epoch 68/100\n",
      " - 13s - loss: 3.0129 - acc: 0.1780\n",
      "Epoch 69/100\n",
      " - 13s - loss: 2.9486 - acc: 0.2082\n",
      "Epoch 70/100\n",
      " - 13s - loss: 2.8355 - acc: 0.2296\n",
      "Epoch 71/100\n",
      " - 13s - loss: 2.8445 - acc: 0.2102\n",
      "Epoch 72/100\n",
      " - 13s - loss: 2.7678 - acc: 0.2267\n",
      "Epoch 73/100\n",
      " - 13s - loss: 2.6933 - acc: 0.2318\n",
      "Epoch 74/100\n",
      " - 13s - loss: 2.6957 - acc: 0.2607\n",
      "Epoch 75/100\n",
      " - 13s - loss: 2.6451 - acc: 0.2542\n",
      "Epoch 76/100\n",
      " - 13s - loss: 2.5856 - acc: 0.2692\n",
      "Epoch 77/100\n",
      " - 13s - loss: 2.5222 - acc: 0.2958\n",
      "Epoch 78/100\n",
      " - 13s - loss: 2.4913 - acc: 0.2725\n",
      "Epoch 79/100\n",
      " - 13s - loss: 2.4577 - acc: 0.2805\n",
      "Epoch 80/100\n",
      " - 13s - loss: 2.4649 - acc: 0.2716\n",
      "Epoch 81/100\n",
      " - 13s - loss: 2.3625 - acc: 0.3236\n",
      "Epoch 82/100\n",
      " - 13s - loss: 2.3020 - acc: 0.3179\n",
      "Epoch 83/100\n",
      " - 13s - loss: 2.2851 - acc: 0.3221\n",
      "Epoch 84/100\n",
      " - 13s - loss: 2.2994 - acc: 0.3399\n",
      "Epoch 85/100\n",
      " - 13s - loss: 2.2548 - acc: 0.3496\n",
      "Epoch 86/100\n",
      " - 13s - loss: 2.1304 - acc: 0.3489\n",
      "Epoch 87/100\n",
      " - 13s - loss: 2.0890 - acc: 0.3828\n",
      "Epoch 88/100\n",
      " - 13s - loss: 2.0839 - acc: 0.3854\n",
      "Epoch 89/100\n",
      " - 13s - loss: 2.0467 - acc: 0.3902\n",
      "Epoch 90/100\n",
      " - 13s - loss: 1.9895 - acc: 0.4169\n",
      "Epoch 91/100\n",
      " - 13s - loss: 1.9700 - acc: 0.3877\n",
      "Epoch 92/100\n",
      " - 13s - loss: 1.8944 - acc: 0.4326\n",
      "Epoch 93/100\n",
      " - 13s - loss: 1.8790 - acc: 0.4364\n",
      "Epoch 94/100\n",
      " - 13s - loss: 1.8599 - acc: 0.4429\n",
      "Epoch 95/100\n",
      " - 13s - loss: 1.8087 - acc: 0.4490\n",
      "Epoch 96/100\n",
      " - 13s - loss: 1.7432 - acc: 0.4603\n",
      "Epoch 97/100\n",
      " - 13s - loss: 1.7278 - acc: 0.4831\n",
      "Epoch 98/100\n",
      " - 13s - loss: 1.7118 - acc: 0.4755\n",
      "Epoch 99/100\n",
      " - 13s - loss: 1.5834 - acc: 0.5061\n",
      "Epoch 100/100\n",
      " - 13s - loss: 1.5940 - acc: 0.5014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shourya97/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1: train=0.032238 test=0.118414\n",
      "Epoch 1/100\n",
      " - 16s - loss: 7.0681 - acc: 0.0452\n",
      "Epoch 2/100\n",
      " - 13s - loss: 5.6050 - acc: 0.0565\n",
      "Epoch 3/100\n",
      " - 13s - loss: 5.4669 - acc: 0.0568\n",
      "Epoch 4/100\n",
      " - 13s - loss: 5.3859 - acc: 0.0704\n",
      "Epoch 5/100\n",
      " - 13s - loss: 5.3634 - acc: 0.0724\n",
      "Epoch 6/100\n",
      " - 13s - loss: 5.2802 - acc: 0.0778\n",
      "Epoch 7/100\n",
      " - 13s - loss: 5.2102 - acc: 0.0768\n",
      "Epoch 8/100\n",
      " - 13s - loss: 5.1801 - acc: 0.0766\n",
      "Epoch 9/100\n",
      " - 13s - loss: 5.1072 - acc: 0.0755\n",
      "Epoch 10/100\n",
      " - 13s - loss: 5.0994 - acc: 0.0774\n",
      "Epoch 11/100\n",
      " - 13s - loss: 5.0632 - acc: 0.0773\n",
      "Epoch 12/100\n",
      " - 13s - loss: 5.0330 - acc: 0.0787\n",
      "Epoch 13/100\n",
      " - 13s - loss: 4.9712 - acc: 0.0756\n",
      "Epoch 14/100\n",
      " - 13s - loss: 4.9320 - acc: 0.0801\n",
      "Epoch 15/100\n",
      " - 13s - loss: 4.8893 - acc: 0.0731\n",
      "Epoch 16/100\n",
      " - 13s - loss: 4.8114 - acc: 0.0802\n",
      "Epoch 17/100\n",
      " - 13s - loss: 4.7870 - acc: 0.0786\n",
      "Epoch 18/100\n",
      " - 13s - loss: 4.7258 - acc: 0.0700\n",
      "Epoch 19/100\n",
      " - 13s - loss: 4.7370 - acc: 0.0719\n",
      "Epoch 20/100\n",
      " - 13s - loss: 4.6511 - acc: 0.0773\n",
      "Epoch 21/100\n",
      " - 13s - loss: 4.5907 - acc: 0.0837\n",
      "Epoch 22/100\n",
      " - 13s - loss: 4.5424 - acc: 0.0765\n",
      "Epoch 23/100\n",
      " - 13s - loss: 4.4785 - acc: 0.0779\n",
      "Epoch 24/100\n",
      " - 13s - loss: 4.4459 - acc: 0.0802\n",
      "Epoch 25/100\n",
      " - 13s - loss: 4.3844 - acc: 0.0814\n",
      "Epoch 26/100\n",
      " - 13s - loss: 4.3355 - acc: 0.0847\n",
      "Epoch 27/100\n",
      " - 13s - loss: 4.2610 - acc: 0.0833\n",
      "Epoch 28/100\n",
      " - 13s - loss: 4.2457 - acc: 0.0905\n",
      "Epoch 29/100\n",
      " - 13s - loss: 4.2587 - acc: 0.0887\n",
      "Epoch 30/100\n",
      " - 13s - loss: 4.2223 - acc: 0.0882\n",
      "Epoch 31/100\n",
      " - 13s - loss: 4.1861 - acc: 0.0930\n",
      "Epoch 32/100\n",
      " - 13s - loss: 4.0769 - acc: 0.0949\n",
      "Epoch 33/100\n",
      " - 13s - loss: 4.0889 - acc: 0.0899\n",
      "Epoch 34/100\n",
      " - 13s - loss: 4.0442 - acc: 0.0863\n",
      "Epoch 35/100\n",
      " - 13s - loss: 3.9748 - acc: 0.0993\n",
      "Epoch 36/100\n",
      " - 13s - loss: 3.8984 - acc: 0.0937\n",
      "Epoch 37/100\n",
      " - 13s - loss: 3.8544 - acc: 0.1040\n",
      "Epoch 38/100\n",
      " - 13s - loss: 3.8566 - acc: 0.1185\n",
      "Epoch 39/100\n",
      " - 13s - loss: 3.7661 - acc: 0.1131\n",
      "Epoch 40/100\n",
      " - 13s - loss: 3.7229 - acc: 0.1018\n",
      "Epoch 41/100\n",
      " - 13s - loss: 3.6809 - acc: 0.1110\n",
      "Epoch 42/100\n",
      " - 13s - loss: 3.6385 - acc: 0.1202\n",
      "Epoch 43/100\n",
      " - 13s - loss: 3.5148 - acc: 0.1243\n",
      "Epoch 44/100\n",
      " - 13s - loss: 3.4433 - acc: 0.1474\n",
      "Epoch 45/100\n",
      " - 13s - loss: 3.4115 - acc: 0.1455\n",
      "Epoch 46/100\n",
      " - 13s - loss: 3.3627 - acc: 0.1558\n",
      "Epoch 47/100\n",
      " - 13s - loss: 3.3846 - acc: 0.1506\n",
      "Epoch 48/100\n",
      " - 13s - loss: 3.3039 - acc: 0.1556\n",
      "Epoch 49/100\n",
      " - 13s - loss: 3.1501 - acc: 0.1631\n",
      "Epoch 50/100\n",
      " - 13s - loss: 3.1079 - acc: 0.1693\n",
      "Epoch 51/100\n",
      " - 13s - loss: 3.0467 - acc: 0.1745\n",
      "Epoch 52/100\n",
      " - 13s - loss: 2.9624 - acc: 0.2037\n",
      "Epoch 53/100\n",
      " - 13s - loss: 2.8987 - acc: 0.2008\n",
      "Epoch 54/100\n",
      " - 13s - loss: 2.8175 - acc: 0.2082\n",
      "Epoch 55/100\n",
      " - 13s - loss: 2.7735 - acc: 0.2145\n",
      "Epoch 56/100\n",
      " - 13s - loss: 2.7405 - acc: 0.2296\n",
      "Epoch 57/100\n",
      " - 13s - loss: 2.6332 - acc: 0.2497\n",
      "Epoch 58/100\n",
      " - 13s - loss: 2.5844 - acc: 0.2441\n",
      "Epoch 59/100\n",
      " - 13s - loss: 2.5287 - acc: 0.2434\n",
      "Epoch 60/100\n",
      " - 13s - loss: 2.4969 - acc: 0.2757\n",
      "Epoch 61/100\n",
      " - 13s - loss: 2.5031 - acc: 0.2736\n",
      "Epoch 62/100\n",
      " - 13s - loss: 2.4282 - acc: 0.2827\n",
      "Epoch 63/100\n",
      " - 13s - loss: 2.4273 - acc: 0.2699\n",
      "Epoch 64/100\n",
      " - 13s - loss: 2.5949 - acc: 0.2534\n",
      "Epoch 65/100\n",
      " - 13s - loss: 2.5429 - acc: 0.2742\n",
      "Epoch 66/100\n",
      " - 13s - loss: 2.3963 - acc: 0.2917\n",
      "Epoch 67/100\n",
      " - 13s - loss: 2.3305 - acc: 0.3104\n",
      "Epoch 68/100\n",
      " - 13s - loss: 2.4239 - acc: 0.2906\n",
      "Epoch 69/100\n",
      " - 13s - loss: 2.3317 - acc: 0.3083\n",
      "Epoch 70/100\n",
      " - 13s - loss: 2.2976 - acc: 0.3136\n",
      "Epoch 71/100\n",
      " - 13s - loss: 2.2206 - acc: 0.3121\n",
      "Epoch 72/100\n",
      " - 13s - loss: 2.1352 - acc: 0.3452\n",
      "Epoch 73/100\n",
      " - 13s - loss: 2.0941 - acc: 0.3604\n",
      "Epoch 74/100\n",
      " - 13s - loss: 1.9944 - acc: 0.3657\n",
      "Epoch 75/100\n",
      " - 13s - loss: 1.9918 - acc: 0.3728\n",
      "Epoch 76/100\n",
      " - 13s - loss: 1.9661 - acc: 0.3589\n",
      "Epoch 77/100\n",
      " - 13s - loss: 1.9348 - acc: 0.3818\n",
      "Epoch 78/100\n",
      " - 13s - loss: 1.9379 - acc: 0.3777\n",
      "Epoch 79/100\n",
      " - 13s - loss: 1.8595 - acc: 0.4105\n",
      "Epoch 80/100\n",
      " - 13s - loss: 1.8081 - acc: 0.4176\n",
      "Epoch 81/100\n",
      " - 13s - loss: 1.7581 - acc: 0.4218\n",
      "Epoch 82/100\n",
      " - 13s - loss: 1.7157 - acc: 0.4404\n",
      "Epoch 83/100\n",
      " - 13s - loss: 1.6834 - acc: 0.4576\n",
      "Epoch 84/100\n",
      " - 13s - loss: 1.6820 - acc: 0.4304\n",
      "Epoch 85/100\n",
      " - 13s - loss: 1.6839 - acc: 0.4374\n",
      "Epoch 86/100\n",
      " - 13s - loss: 1.6776 - acc: 0.4452\n",
      "Epoch 87/100\n",
      " - 13s - loss: 1.5943 - acc: 0.4656\n",
      "Epoch 88/100\n",
      " - 13s - loss: 1.5842 - acc: 0.4734\n",
      "Epoch 89/100\n",
      " - 13s - loss: 1.5590 - acc: 0.4833\n",
      "Epoch 90/100\n",
      " - 13s - loss: 1.5555 - acc: 0.4963\n",
      "Epoch 91/100\n",
      " - 13s - loss: 1.5195 - acc: 0.4858\n",
      "Epoch 92/100\n",
      " - 13s - loss: 1.5082 - acc: 0.5061\n",
      "Epoch 93/100\n",
      " - 13s - loss: 1.4529 - acc: 0.5069\n",
      "Epoch 94/100\n",
      " - 13s - loss: 1.4113 - acc: 0.5191\n",
      "Epoch 95/100\n",
      " - 13s - loss: 1.3549 - acc: 0.5232\n",
      "Epoch 96/100\n",
      " - 13s - loss: 1.3362 - acc: 0.5504\n",
      "Epoch 97/100\n",
      " - 13s - loss: 1.3705 - acc: 0.5393\n",
      "Epoch 98/100\n",
      " - 13s - loss: 1.3261 - acc: 0.5465\n",
      "Epoch 99/100\n",
      " - 13s - loss: 1.3075 - acc: 0.5597\n",
      "Epoch 100/100\n",
      " - 13s - loss: 1.2975 - acc: 0.5705\n",
      ">2: train=0.049478 test=0.104123\n",
      "Epoch 1/100\n",
      " - 16s - loss: 7.0731 - acc: 0.0495\n",
      "Epoch 2/100\n",
      " - 13s - loss: 5.5597 - acc: 0.0596\n",
      "Epoch 3/100\n",
      " - 13s - loss: 5.4872 - acc: 0.0531\n",
      "Epoch 4/100\n",
      " - 13s - loss: 5.4486 - acc: 0.0579\n",
      "Epoch 5/100\n",
      " - 13s - loss: 5.3909 - acc: 0.0587\n",
      "Epoch 6/100\n",
      " - 13s - loss: 5.3545 - acc: 0.0600\n",
      "Epoch 7/100\n",
      " - 13s - loss: 5.3065 - acc: 0.0695\n",
      "Epoch 8/100\n",
      " - 13s - loss: 5.2403 - acc: 0.0789\n",
      "Epoch 9/100\n",
      " - 13s - loss: 5.2138 - acc: 0.0774\n",
      "Epoch 10/100\n",
      " - 13s - loss: 5.1449 - acc: 0.0793\n",
      "Epoch 11/100\n",
      " - 13s - loss: 5.1243 - acc: 0.0776\n",
      "Epoch 12/100\n",
      " - 13s - loss: 5.0800 - acc: 0.0787\n",
      "Epoch 13/100\n",
      " - 13s - loss: 5.0158 - acc: 0.0760\n",
      "Epoch 14/100\n",
      " - 13s - loss: 5.0020 - acc: 0.0801\n",
      "Epoch 15/100\n",
      " - 13s - loss: 4.9763 - acc: 0.0734\n",
      "Epoch 16/100\n",
      " - 13s - loss: 4.9521 - acc: 0.0729\n",
      "Epoch 17/100\n",
      " - 13s - loss: 4.9537 - acc: 0.0722\n",
      "Epoch 18/100\n",
      " - 13s - loss: 4.9131 - acc: 0.0798\n",
      "Epoch 19/100\n",
      " - 13s - loss: 4.8498 - acc: 0.0722\n",
      "Epoch 20/100\n",
      " - 13s - loss: 4.7969 - acc: 0.0746\n",
      "Epoch 21/100\n",
      " - 13s - loss: 4.7545 - acc: 0.0785\n",
      "Epoch 22/100\n",
      " - 13s - loss: 4.6824 - acc: 0.0804\n",
      "Epoch 23/100\n",
      " - 13s - loss: 4.6278 - acc: 0.0836\n",
      "Epoch 24/100\n",
      " - 13s - loss: 4.5470 - acc: 0.0834\n",
      "Epoch 25/100\n",
      " - 13s - loss: 4.5165 - acc: 0.0878\n",
      "Epoch 26/100\n",
      " - 13s - loss: 4.4098 - acc: 0.0893\n",
      "Epoch 27/100\n",
      " - 13s - loss: 4.3575 - acc: 0.0958\n",
      "Epoch 28/100\n",
      " - 13s - loss: 4.3758 - acc: 0.0965\n",
      "Epoch 29/100\n",
      " - 13s - loss: 4.2526 - acc: 0.0976\n",
      "Epoch 30/100\n",
      " - 13s - loss: 4.1655 - acc: 0.1188\n",
      "Epoch 31/100\n",
      " - 13s - loss: 4.0777 - acc: 0.1185\n",
      "Epoch 32/100\n",
      " - 13s - loss: 4.0567 - acc: 0.1202\n",
      "Epoch 33/100\n",
      " - 13s - loss: 4.0292 - acc: 0.1187\n",
      "Epoch 34/100\n",
      " - 13s - loss: 3.8798 - acc: 0.1146\n",
      "Epoch 35/100\n",
      " - 13s - loss: 3.8273 - acc: 0.1191\n",
      "Epoch 36/100\n",
      " - 13s - loss: 3.7476 - acc: 0.1255\n",
      "Epoch 37/100\n",
      " - 13s - loss: 3.6616 - acc: 0.1396\n",
      "Epoch 38/100\n",
      " - 13s - loss: 3.4951 - acc: 0.1538\n",
      "Epoch 39/100\n",
      " - 13s - loss: 3.4053 - acc: 0.1637\n",
      "Epoch 40/100\n",
      " - 13s - loss: 3.3424 - acc: 0.1551\n",
      "Epoch 41/100\n",
      " - 13s - loss: 3.2739 - acc: 0.1670\n",
      "Epoch 42/100\n",
      " - 13s - loss: 3.1463 - acc: 0.1815\n",
      "Epoch 43/100\n",
      " - 13s - loss: 3.1107 - acc: 0.1902\n",
      "Epoch 44/100\n",
      " - 13s - loss: 2.9948 - acc: 0.2093\n",
      "Epoch 45/100\n",
      " - 13s - loss: 2.9033 - acc: 0.2112\n",
      "Epoch 46/100\n",
      " - 13s - loss: 2.8644 - acc: 0.2163\n",
      "Epoch 47/100\n",
      " - 13s - loss: 2.7889 - acc: 0.2476\n",
      "Epoch 48/100\n",
      " - 13s - loss: 2.7266 - acc: 0.2502\n",
      "Epoch 49/100\n",
      " - 13s - loss: 2.6237 - acc: 0.2538\n",
      "Epoch 50/100\n",
      " - 13s - loss: 2.5709 - acc: 0.2588\n",
      "Epoch 51/100\n",
      " - 13s - loss: 2.5023 - acc: 0.2894\n",
      "Epoch 52/100\n",
      " - 13s - loss: 2.4664 - acc: 0.3055\n",
      "Epoch 53/100\n",
      " - 13s - loss: 2.3350 - acc: 0.3270\n",
      "Epoch 54/100\n",
      " - 13s - loss: 2.3491 - acc: 0.3167\n",
      "Epoch 55/100\n",
      " - 13s - loss: 2.2438 - acc: 0.3231\n",
      "Epoch 56/100\n",
      " - 13s - loss: 2.2125 - acc: 0.3361\n",
      "Epoch 57/100\n",
      " - 13s - loss: 2.1464 - acc: 0.3444\n",
      "Epoch 58/100\n",
      " - 13s - loss: 2.0739 - acc: 0.3571\n",
      "Epoch 59/100\n",
      " - 13s - loss: 1.9645 - acc: 0.3802\n",
      "Epoch 60/100\n",
      " - 13s - loss: 2.0085 - acc: 0.3766\n",
      "Epoch 61/100\n",
      " - 13s - loss: 1.9033 - acc: 0.4017\n",
      "Epoch 62/100\n",
      " - 13s - loss: 1.8454 - acc: 0.4072\n",
      "Epoch 63/100\n",
      " - 13s - loss: 1.7896 - acc: 0.4480\n",
      "Epoch 64/100\n",
      " - 13s - loss: 1.7951 - acc: 0.4371\n",
      "Epoch 65/100\n",
      " - 13s - loss: 1.6891 - acc: 0.4782\n",
      "Epoch 66/100\n",
      " - 13s - loss: 1.6306 - acc: 0.4771\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 13s - loss: 1.5551 - acc: 0.5135\n",
      "Epoch 68/100\n",
      " - 13s - loss: 1.4852 - acc: 0.5091\n",
      "Epoch 69/100\n",
      " - 13s - loss: 1.4448 - acc: 0.5205\n",
      "Epoch 70/100\n",
      " - 13s - loss: 1.4794 - acc: 0.5099\n",
      "Epoch 71/100\n",
      " - 13s - loss: 1.4268 - acc: 0.5248\n",
      "Epoch 72/100\n",
      " - 13s - loss: 1.3669 - acc: 0.5154\n",
      "Epoch 73/100\n",
      " - 13s - loss: 1.4045 - acc: 0.5077\n",
      "Epoch 74/100\n",
      " - 13s - loss: 1.4111 - acc: 0.5361\n",
      "Epoch 75/100\n",
      " - 13s - loss: 1.3082 - acc: 0.5458\n",
      "Epoch 76/100\n",
      " - 13s - loss: 1.2121 - acc: 0.5751\n",
      "Epoch 77/100\n",
      " - 13s - loss: 1.2813 - acc: 0.5562\n",
      "Epoch 78/100\n",
      " - 13s - loss: 1.2732 - acc: 0.5799\n",
      "Epoch 79/100\n",
      " - 13s - loss: 1.1708 - acc: 0.5868\n",
      "Epoch 80/100\n",
      " - 13s - loss: 1.1167 - acc: 0.6280\n",
      "Epoch 81/100\n",
      " - 13s - loss: 1.1227 - acc: 0.6375\n",
      "Epoch 82/100\n",
      " - 13s - loss: 1.0466 - acc: 0.6445\n",
      "Epoch 83/100\n",
      " - 13s - loss: 1.0060 - acc: 0.6576\n",
      "Epoch 84/100\n",
      " - 13s - loss: 1.0047 - acc: 0.6504\n",
      "Epoch 85/100\n",
      " - 13s - loss: 0.9889 - acc: 0.6680\n",
      "Epoch 86/100\n",
      " - 13s - loss: 0.9654 - acc: 0.6678\n",
      "Epoch 87/100\n",
      " - 13s - loss: 0.9039 - acc: 0.6860\n",
      "Epoch 88/100\n",
      " - 13s - loss: 0.9345 - acc: 0.6778\n",
      "Epoch 89/100\n",
      " - 13s - loss: 0.9335 - acc: 0.6754\n",
      "Epoch 90/100\n",
      " - 13s - loss: 0.9199 - acc: 0.6896\n",
      "Epoch 91/100\n",
      " - 13s - loss: 0.9341 - acc: 0.6773\n",
      "Epoch 92/100\n",
      " - 13s - loss: 0.8724 - acc: 0.6909\n",
      "Epoch 93/100\n",
      " - 13s - loss: 0.8619 - acc: 0.6886\n",
      "Epoch 94/100\n",
      " - 13s - loss: 0.8456 - acc: 0.7166\n",
      "Epoch 95/100\n",
      " - 13s - loss: 0.7985 - acc: 0.7246\n",
      "Epoch 96/100\n",
      " - 13s - loss: 0.7623 - acc: 0.7288\n",
      "Epoch 97/100\n",
      " - 13s - loss: 0.7585 - acc: 0.7264\n",
      "Epoch 98/100\n",
      " - 13s - loss: 0.7591 - acc: 0.7186\n",
      "Epoch 99/100\n",
      " - 13s - loss: 0.7858 - acc: 0.7230\n",
      "Epoch 100/100\n",
      " - 13s - loss: 0.7602 - acc: 0.7291\n",
      ">3: train=0.093293 test=0.018180\n",
      "          train      test\n",
      "count  3.000000  3.000000\n",
      "mean   0.058336  0.080239\n",
      "std    0.031476  0.054218\n",
      "min    0.032238  0.018180\n",
      "25%    0.040858  0.061151\n",
      "50%    0.049478  0.104123\n",
      "75%    0.071385  0.111268\n",
      "max    0.093293  0.118414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shourya97/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "verbose = 2\n",
    "n_epochs = 100\n",
    "n_photos_per_update = 2\n",
    "n_batches_per_epoch = int(len(train) / n_photos_per_update)\n",
    "n_repeats = 3\n",
    "\n",
    "# run experiment\n",
    "train_results, test_results = list(), list()\n",
    "for i in range(n_repeats):\n",
    "    # define the model\n",
    "    model = define_model(vocab_size, max_length)\n",
    "    # define checkpoint callback\n",
    "#     filepath = 'size_sm_seq_model-model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "#     checkpoint = ModelCheckpoint(filepath, monitor='val_loss',\n",
    "#                                  verbose=1,\n",
    "#                                  save_best_only=True,\n",
    "#                                  mode='min')\n",
    "    # fit model\n",
    "    model.fit_generator(data_generator(train_descriptions, train_features, tokenizer, max_length, n_photos_per_update), \n",
    "                        steps_per_epoch=n_batches_per_epoch, \n",
    "                        epochs=n_epochs,\n",
    "                        verbose=verbose)\n",
    "    # evaluate model on training data\n",
    "    train_score = evaluate_model(model, train_descriptions, train_features, tokenizer, max_length)\n",
    "    test_score = evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)\n",
    "    # store\n",
    "    train_results.append(train_score)\n",
    "    test_results.append(test_score)\n",
    "    print('>%d: train=%f test=%f' % ((i+1), train_score, test_score))\n",
    "# save results to file\n",
    "df = DataFrame()\n",
    "df['train'] = train_results\n",
    "df['test'] = test_results\n",
    "print(df.describe())\n",
    "df.to_csv(model_name+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLUE scores table\n",
    "|      |    train  |    test|\n",
    "|------|-----------|--------|\n",
    "|count | 3.000000  |3.000000|\n",
    "|mean  | 0.058336  |0.080239|\n",
    "|std   | 0.031476  |0.054218|\n",
    "|min   | 0.032238  |0.018180|\n",
    "|25%   | 0.040858  |0.061151|\n",
    "|50%   | 0.049478  |0.104123|\n",
    "|75%   | 0.071385  |0.111268|\n",
    "|max   | 0.093293  |0.118414|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going the other way, we can **double the number of LSTM layers from one to two and see if that makes a dramatic difference**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "    # feature extractor (encoder)\n",
    "    inputs1 = Input(shape=(7, 7, 512))\n",
    "    fe1 = GlobalMaxPooling2D()(inputs1)\n",
    "    fe2 = Dense(128, activation='relu')(fe1)\n",
    "    fe3 = RepeatVector(max_length)(fe2)\n",
    "    # embedding\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    emb2 = Embedding(vocab_size, 50, mask_zero=True)(inputs2)\n",
    "    emb3 = LSTM(256, return_sequences=True)(emb2)\n",
    "    emb4 = LSTM(256, return_sequences=True)(emb3)\n",
    "    emb5 = TimeDistributed(Dense(128, activation='relu'))(emb4)\n",
    "    # merge inputs\n",
    "    merged = concatenate([fe3, emb5])\n",
    "    # language model (decoder)\n",
    "    lm2 = LSTM(500)(merged)\n",
    "    lm3 = Dense(500, activation='relu')(lm2)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(lm3)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_name = 'size_lg_seq_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description Length: 25\n",
      "Epoch 1/100\n",
      " - 33s - loss: 7.0698 - acc: 0.0480\n",
      "Epoch 2/100\n",
      " - 18s - loss: 5.6266 - acc: 0.0581\n",
      "Epoch 3/100\n",
      " - 17s - loss: 5.4509 - acc: 0.0552\n",
      "Epoch 4/100\n",
      " - 17s - loss: 5.3979 - acc: 0.0630\n",
      "Epoch 5/100\n",
      " - 17s - loss: 5.3572 - acc: 0.0630\n",
      "Epoch 6/100\n",
      " - 18s - loss: 5.3378 - acc: 0.0630\n",
      "Epoch 7/100\n",
      " - 17s - loss: 5.3080 - acc: 0.0630\n",
      "Epoch 8/100\n",
      " - 17s - loss: 5.3122 - acc: 0.0630\n",
      "Epoch 9/100\n",
      " - 18s - loss: 5.2613 - acc: 0.0652\n",
      "Epoch 10/100\n",
      " - 17s - loss: 5.2481 - acc: 0.0673\n",
      "Epoch 11/100\n",
      " - 17s - loss: 5.2623 - acc: 0.0736\n",
      "Epoch 12/100\n",
      " - 21s - loss: 5.2081 - acc: 0.0772\n",
      "Epoch 13/100\n",
      " - 19s - loss: 5.1825 - acc: 0.0780\n",
      "Epoch 14/100\n",
      " - 20s - loss: 5.1298 - acc: 0.0741\n",
      "Epoch 15/100\n",
      " - 19s - loss: 5.1119 - acc: 0.0766\n",
      "Epoch 16/100\n",
      " - 20s - loss: 5.1045 - acc: 0.0782\n",
      "Epoch 17/100\n",
      " - 18s - loss: 5.0456 - acc: 0.0800\n",
      "Epoch 18/100\n",
      " - 18s - loss: 5.0348 - acc: 0.0754\n",
      "Epoch 19/100\n",
      " - 18s - loss: 4.9667 - acc: 0.0843\n",
      "Epoch 20/100\n",
      " - 18s - loss: 4.9336 - acc: 0.0796\n",
      "Epoch 21/100\n",
      " - 18s - loss: 4.8564 - acc: 0.0879\n",
      "Epoch 22/100\n",
      " - 18s - loss: 4.8166 - acc: 0.0885\n",
      "Epoch 23/100\n",
      " - 20s - loss: 4.7959 - acc: 0.0887\n",
      "Epoch 24/100\n",
      " - 20s - loss: 4.7523 - acc: 0.0859\n",
      "Epoch 25/100\n",
      " - 18s - loss: 4.6673 - acc: 0.0891\n",
      "Epoch 26/100\n",
      " - 19s - loss: 4.6746 - acc: 0.0896\n",
      "Epoch 27/100\n",
      " - 18s - loss: 4.6075 - acc: 0.0875\n",
      "Epoch 28/100\n",
      " - 18s - loss: 4.6140 - acc: 0.0888\n",
      "Epoch 29/100\n",
      " - 18s - loss: 4.4880 - acc: 0.0906\n",
      "Epoch 30/100\n",
      " - 20s - loss: 4.4805 - acc: 0.0937\n",
      "Epoch 31/100\n",
      " - 20s - loss: 4.4116 - acc: 0.0907\n",
      "Epoch 32/100\n",
      " - 19s - loss: 4.3694 - acc: 0.0979\n",
      "Epoch 33/100\n",
      " - 18s - loss: 4.3666 - acc: 0.0926\n",
      "Epoch 34/100\n",
      " - 17s - loss: 4.3303 - acc: 0.0961\n",
      "Epoch 35/100\n",
      " - 17s - loss: 4.2404 - acc: 0.0977\n",
      "Epoch 36/100\n",
      " - 18s - loss: 4.2126 - acc: 0.1072\n",
      "Epoch 37/100\n",
      " - 17s - loss: 4.1782 - acc: 0.1076\n",
      "Epoch 38/100\n",
      " - 17s - loss: 4.0956 - acc: 0.1052\n",
      "Epoch 39/100\n",
      " - 17s - loss: 4.1003 - acc: 0.0940\n",
      "Epoch 40/100\n",
      " - 18s - loss: 4.0620 - acc: 0.1020\n",
      "Epoch 41/100\n",
      " - 17s - loss: 3.9962 - acc: 0.1087\n",
      "Epoch 42/100\n",
      " - 17s - loss: 3.9824 - acc: 0.1048\n",
      "Epoch 43/100\n",
      " - 18s - loss: 3.8700 - acc: 0.1248\n",
      "Epoch 44/100\n",
      " - 17s - loss: 3.8703 - acc: 0.1126\n",
      "Epoch 45/100\n",
      " - 17s - loss: 3.8261 - acc: 0.1165\n",
      "Epoch 46/100\n",
      " - 17s - loss: 3.7418 - acc: 0.1310\n",
      "Epoch 47/100\n",
      " - 18s - loss: 3.6489 - acc: 0.1446\n",
      "Epoch 48/100\n",
      " - 17s - loss: 3.5748 - acc: 0.1438\n",
      "Epoch 49/100\n",
      " - 17s - loss: 3.4872 - acc: 0.1595\n",
      "Epoch 50/100\n",
      " - 17s - loss: 3.4145 - acc: 0.1518\n",
      "Epoch 51/100\n",
      " - 18s - loss: 3.3478 - acc: 0.1616\n",
      "Epoch 52/100\n",
      " - 17s - loss: 3.2251 - acc: 0.1771\n",
      "Epoch 53/100\n",
      " - 17s - loss: 3.0967 - acc: 0.1935\n",
      "Epoch 54/100\n",
      " - 18s - loss: 3.0015 - acc: 0.1993\n",
      "Epoch 55/100\n",
      " - 17s - loss: 2.9405 - acc: 0.2229\n",
      "Epoch 56/100\n",
      " - 17s - loss: 2.8768 - acc: 0.2356\n",
      "Epoch 57/100\n",
      " - 17s - loss: 2.8286 - acc: 0.2499\n",
      "Epoch 58/100\n",
      " - 18s - loss: 2.8032 - acc: 0.2440\n",
      "Epoch 59/100\n",
      " - 17s - loss: 2.6961 - acc: 0.2540\n",
      "Epoch 60/100\n",
      " - 17s - loss: 2.6093 - acc: 0.2557\n",
      "Epoch 61/100\n",
      " - 17s - loss: 2.5564 - acc: 0.2758\n",
      "Epoch 62/100\n",
      " - 18s - loss: 2.4692 - acc: 0.2843\n",
      "Epoch 63/100\n",
      " - 17s - loss: 2.4168 - acc: 0.3023\n",
      "Epoch 64/100\n",
      " - 20s - loss: 2.3062 - acc: 0.3479\n",
      "Epoch 65/100\n",
      " - 18s - loss: 2.1657 - acc: 0.3640\n",
      "Epoch 66/100\n",
      " - 18s - loss: 2.1185 - acc: 0.3692\n",
      "Epoch 67/100\n",
      " - 17s - loss: 2.0758 - acc: 0.3998\n",
      "Epoch 68/100\n",
      " - 17s - loss: 2.1823 - acc: 0.3460\n",
      "Epoch 69/100\n",
      " - 18s - loss: 2.0531 - acc: 0.3581\n",
      "Epoch 70/100\n",
      " - 17s - loss: 1.9930 - acc: 0.3941\n",
      "Epoch 71/100\n",
      " - 17s - loss: 1.9313 - acc: 0.3764\n",
      "Epoch 72/100\n",
      " - 18s - loss: 1.9004 - acc: 0.4104\n",
      "Epoch 73/100\n",
      " - 17s - loss: 1.8077 - acc: 0.4261\n",
      "Epoch 74/100\n",
      " - 17s - loss: 1.7596 - acc: 0.4239\n",
      "Epoch 75/100\n",
      " - 18s - loss: 1.7291 - acc: 0.4308\n",
      "Epoch 76/100\n",
      " - 18s - loss: 1.6855 - acc: 0.4479\n",
      "Epoch 77/100\n",
      " - 17s - loss: 1.7161 - acc: 0.4536\n",
      "Epoch 78/100\n",
      " - 17s - loss: 1.6675 - acc: 0.4407\n",
      "Epoch 79/100\n",
      " - 18s - loss: 1.5888 - acc: 0.4687\n",
      "Epoch 80/100\n",
      " - 17s - loss: 1.5833 - acc: 0.4794\n",
      "Epoch 81/100\n",
      " - 17s - loss: 1.5218 - acc: 0.4651\n",
      "Epoch 82/100\n",
      " - 17s - loss: 1.4222 - acc: 0.5269\n",
      "Epoch 83/100\n",
      " - 18s - loss: 1.3946 - acc: 0.5196\n",
      "Epoch 84/100\n",
      " - 17s - loss: 1.4307 - acc: 0.5188\n",
      "Epoch 85/100\n",
      " - 17s - loss: 1.4598 - acc: 0.4995\n",
      "Epoch 86/100\n",
      " - 17s - loss: 1.3929 - acc: 0.4985\n",
      "Epoch 87/100\n",
      " - 18s - loss: 1.3593 - acc: 0.5217\n",
      "Epoch 88/100\n",
      " - 17s - loss: 1.3702 - acc: 0.5146\n",
      "Epoch 89/100\n",
      " - 17s - loss: 1.3794 - acc: 0.5229\n",
      "Epoch 90/100\n",
      " - 18s - loss: 1.3353 - acc: 0.5312\n",
      "Epoch 91/100\n",
      " - 17s - loss: 1.3085 - acc: 0.5647\n",
      "Epoch 92/100\n",
      " - 17s - loss: 1.2457 - acc: 0.5784\n",
      "Epoch 93/100\n",
      " - 17s - loss: 1.2276 - acc: 0.5743\n",
      "Epoch 94/100\n",
      " - 18s - loss: 1.2108 - acc: 0.5788\n",
      "Epoch 95/100\n",
      " - 17s - loss: 1.2324 - acc: 0.5566\n",
      "Epoch 96/100\n",
      " - 17s - loss: 1.2068 - acc: 0.5789\n",
      "Epoch 97/100\n",
      " - 18s - loss: 1.1311 - acc: 0.5844\n",
      "Epoch 98/100\n",
      " - 17s - loss: 1.0933 - acc: 0.6026\n",
      "Epoch 99/100\n",
      " - 17s - loss: 1.0597 - acc: 0.6232\n",
      "Epoch 100/100\n",
      " - 17s - loss: 1.1011 - acc: 0.6113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shourya97/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1: train=0.068580 test=0.108476\n",
      "Epoch 1/100\n",
      " - 19s - loss: 7.0957 - acc: 0.0494\n",
      "Epoch 2/100\n",
      " - 17s - loss: 5.5917 - acc: 0.0596\n",
      "Epoch 3/100\n",
      " - 18s - loss: 5.4500 - acc: 0.0643\n",
      "Epoch 4/100\n",
      " - 17s - loss: 5.3851 - acc: 0.0680\n",
      "Epoch 5/100\n",
      " - 17s - loss: 5.3610 - acc: 0.0712\n",
      "Epoch 6/100\n",
      " - 18s - loss: 5.2998 - acc: 0.0711\n",
      "Epoch 7/100\n",
      " - 17s - loss: 5.2425 - acc: 0.0718\n",
      "Epoch 8/100\n",
      " - 17s - loss: 5.1816 - acc: 0.0776\n",
      "Epoch 9/100\n",
      " - 17s - loss: 5.1749 - acc: 0.0778\n",
      "Epoch 10/100\n",
      " - 18s - loss: 5.1315 - acc: 0.0792\n",
      "Epoch 11/100\n",
      " - 17s - loss: 5.0854 - acc: 0.0800\n",
      "Epoch 12/100\n",
      " - 17s - loss: 5.0602 - acc: 0.0813\n",
      "Epoch 13/100\n",
      " - 18s - loss: 5.0944 - acc: 0.0786\n",
      "Epoch 14/100\n",
      " - 17s - loss: 5.0568 - acc: 0.0821\n",
      "Epoch 15/100\n",
      " - 17s - loss: 5.0396 - acc: 0.0809\n",
      "Epoch 16/100\n",
      " - 17s - loss: 5.0027 - acc: 0.0825\n",
      "Epoch 17/100\n",
      " - 18s - loss: 4.9382 - acc: 0.0810\n",
      "Epoch 18/100\n",
      " - 17s - loss: 4.9668 - acc: 0.0763\n",
      "Epoch 19/100\n",
      " - 17s - loss: 4.9577 - acc: 0.0832\n",
      "Epoch 20/100\n",
      " - 17s - loss: 4.9664 - acc: 0.0827\n",
      "Epoch 21/100\n",
      " - 18s - loss: 4.8525 - acc: 0.0822\n",
      "Epoch 22/100\n",
      " - 17s - loss: 4.8954 - acc: 0.0807\n",
      "Epoch 23/100\n",
      " - 17s - loss: 4.8267 - acc: 0.0842\n",
      "Epoch 24/100\n",
      " - 18s - loss: 4.7461 - acc: 0.0843\n",
      "Epoch 25/100\n",
      " - 17s - loss: 4.6858 - acc: 0.0847\n",
      "Epoch 26/100\n",
      " - 17s - loss: 4.7084 - acc: 0.0826\n",
      "Epoch 27/100\n",
      " - 17s - loss: 4.6484 - acc: 0.0825\n",
      "Epoch 28/100\n",
      " - 18s - loss: 4.5517 - acc: 0.0886\n",
      "Epoch 29/100\n",
      " - 17s - loss: 4.5044 - acc: 0.0831\n",
      "Epoch 30/100\n",
      " - 17s - loss: 4.4222 - acc: 0.0877\n",
      "Epoch 31/100\n",
      " - 18s - loss: 4.3806 - acc: 0.0850\n",
      "Epoch 32/100\n",
      " - 17s - loss: 4.3040 - acc: 0.0880\n",
      "Epoch 33/100\n",
      " - 17s - loss: 4.3095 - acc: 0.0886\n",
      "Epoch 34/100\n",
      " - 17s - loss: 4.2176 - acc: 0.0881\n",
      "Epoch 35/100\n",
      " - 18s - loss: 4.2309 - acc: 0.0899\n",
      "Epoch 36/100\n",
      " - 17s - loss: 4.1949 - acc: 0.0794\n",
      "Epoch 37/100\n",
      " - 17s - loss: 4.1117 - acc: 0.0854\n",
      "Epoch 38/100\n",
      " - 17s - loss: 4.0572 - acc: 0.0982\n",
      "Epoch 39/100\n",
      " - 18s - loss: 4.0207 - acc: 0.0931\n",
      "Epoch 40/100\n",
      " - 17s - loss: 4.0102 - acc: 0.0968\n",
      "Epoch 41/100\n",
      " - 17s - loss: 3.9364 - acc: 0.1038\n",
      "Epoch 42/100\n",
      " - 18s - loss: 3.8541 - acc: 0.1075\n",
      "Epoch 43/100\n",
      " - 17s - loss: 3.7883 - acc: 0.1125\n",
      "Epoch 44/100\n",
      " - 17s - loss: 3.7346 - acc: 0.1066\n",
      "Epoch 45/100\n",
      " - 17s - loss: 3.7775 - acc: 0.1113\n",
      "Epoch 46/100\n",
      " - 18s - loss: 3.7122 - acc: 0.1038\n",
      "Epoch 47/100\n",
      " - 17s - loss: 3.6452 - acc: 0.1194\n",
      "Epoch 48/100\n",
      " - 17s - loss: 3.6159 - acc: 0.1175\n",
      "Epoch 49/100\n",
      " - 17s - loss: 3.5997 - acc: 0.1117\n",
      "Epoch 50/100\n",
      " - 18s - loss: 3.5129 - acc: 0.1279\n",
      "Epoch 51/100\n",
      " - 17s - loss: 3.4388 - acc: 0.1289\n",
      "Epoch 52/100\n",
      " - 17s - loss: 3.3979 - acc: 0.1264\n",
      "Epoch 53/100\n",
      " - 18s - loss: 3.3606 - acc: 0.1508\n",
      "Epoch 54/100\n",
      " - 17s - loss: 3.3524 - acc: 0.1490\n",
      "Epoch 55/100\n",
      " - 17s - loss: 3.2454 - acc: 0.1550\n",
      "Epoch 56/100\n",
      " - 17s - loss: 3.1638 - acc: 0.1663\n",
      "Epoch 57/100\n",
      " - 18s - loss: 3.1335 - acc: 0.1789\n",
      "Epoch 58/100\n",
      " - 17s - loss: 3.0711 - acc: 0.1820\n",
      "Epoch 59/100\n",
      " - 17s - loss: 3.0215 - acc: 0.1970\n",
      "Epoch 60/100\n",
      " - 18s - loss: 2.9279 - acc: 0.2152\n",
      "Epoch 61/100\n",
      " - 17s - loss: 2.8992 - acc: 0.2085\n",
      "Epoch 62/100\n",
      " - 17s - loss: 2.8501 - acc: 0.2155\n",
      "Epoch 63/100\n",
      " - 18s - loss: 2.8012 - acc: 0.2331\n",
      "Epoch 64/100\n",
      " - 18s - loss: 2.7441 - acc: 0.2263\n",
      "Epoch 65/100\n",
      " - 17s - loss: 2.7288 - acc: 0.2311\n",
      "Epoch 66/100\n",
      " - 17s - loss: 2.6555 - acc: 0.2349\n",
      "Epoch 67/100\n",
      " - 17s - loss: 2.6069 - acc: 0.2304\n",
      "Epoch 68/100\n",
      " - 18s - loss: 2.5763 - acc: 0.2454\n",
      "Epoch 69/100\n",
      " - 17s - loss: 2.5162 - acc: 0.2551\n",
      "Epoch 70/100\n",
      " - 17s - loss: 2.5289 - acc: 0.2646\n",
      "Epoch 71/100\n",
      " - 18s - loss: 2.4304 - acc: 0.2602\n",
      "Epoch 72/100\n",
      " - 17s - loss: 2.3843 - acc: 0.2768\n",
      "Epoch 73/100\n",
      " - 17s - loss: 2.3433 - acc: 0.2788\n",
      "Epoch 74/100\n",
      " - 17s - loss: 2.4662 - acc: 0.2896\n",
      "Epoch 75/100\n",
      " - 18s - loss: 2.3430 - acc: 0.2905\n",
      "Epoch 76/100\n",
      " - 17s - loss: 2.2920 - acc: 0.3251\n",
      "Epoch 77/100\n",
      " - 17s - loss: 2.2835 - acc: 0.3008\n",
      "Epoch 78/100\n",
      " - 18s - loss: 2.2550 - acc: 0.3183\n",
      "Epoch 79/100\n",
      " - 17s - loss: 2.2299 - acc: 0.3221\n",
      "Epoch 80/100\n",
      " - 17s - loss: 2.2100 - acc: 0.3300\n",
      "Epoch 81/100\n",
      " - 17s - loss: 2.1303 - acc: 0.3509\n",
      "Epoch 82/100\n",
      " - 18s - loss: 2.1676 - acc: 0.3303\n",
      "Epoch 83/100\n",
      " - 18s - loss: 2.1656 - acc: 0.3257\n",
      "Epoch 84/100\n",
      " - 17s - loss: 2.1114 - acc: 0.3517\n",
      "Epoch 85/100\n",
      " - 17s - loss: 2.0748 - acc: 0.3686\n",
      "Epoch 86/100\n",
      " - 18s - loss: 2.0172 - acc: 0.3823\n",
      "Epoch 87/100\n",
      " - 17s - loss: 1.9634 - acc: 0.3954\n",
      "Epoch 88/100\n",
      " - 17s - loss: 1.9328 - acc: 0.3955\n",
      "Epoch 89/100\n",
      " - 18s - loss: 1.9615 - acc: 0.3779\n",
      "Epoch 90/100\n",
      " - 18s - loss: 1.9583 - acc: 0.3861\n",
      "Epoch 91/100\n",
      " - 17s - loss: 1.9207 - acc: 0.3938\n",
      "Epoch 92/100\n",
      " - 17s - loss: 1.8995 - acc: 0.4170\n",
      "Epoch 93/100\n",
      " - 18s - loss: 1.8267 - acc: 0.4314\n",
      "Epoch 94/100\n",
      " - 17s - loss: 1.7594 - acc: 0.4430\n",
      "Epoch 95/100\n",
      " - 17s - loss: 1.7724 - acc: 0.4282\n",
      "Epoch 96/100\n",
      " - 18s - loss: 1.7144 - acc: 0.4502\n",
      "Epoch 97/100\n",
      " - 17s - loss: 1.6709 - acc: 0.4476\n",
      "Epoch 98/100\n",
      " - 17s - loss: 1.6983 - acc: 0.4579\n",
      "Epoch 99/100\n",
      " - 17s - loss: 1.7109 - acc: 0.4584\n",
      "Epoch 100/100\n",
      " - 18s - loss: 1.6580 - acc: 0.4775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shourya97/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2: train=0.005131 test=0.500384\n",
      "Epoch 1/100\n",
      " - 19s - loss: 7.1027 - acc: 0.0446\n",
      "Epoch 2/100\n",
      " - 17s - loss: 5.5707 - acc: 0.0563\n",
      "Epoch 3/100\n",
      " - 17s - loss: 5.4500 - acc: 0.0667\n",
      "Epoch 4/100\n",
      " - 17s - loss: 5.4187 - acc: 0.0592\n",
      "Epoch 5/100\n",
      " - 18s - loss: 5.3593 - acc: 0.0707\n",
      "Epoch 6/100\n",
      " - 17s - loss: 5.3483 - acc: 0.0661\n",
      "Epoch 7/100\n",
      " - 17s - loss: 5.2539 - acc: 0.0730\n",
      "Epoch 8/100\n",
      " - 18s - loss: 5.1955 - acc: 0.0708\n",
      "Epoch 9/100\n",
      " - 17s - loss: 5.1544 - acc: 0.0768\n",
      "Epoch 10/100\n",
      " - 17s - loss: 5.1522 - acc: 0.0763\n",
      "Epoch 11/100\n",
      " - 17s - loss: 5.0756 - acc: 0.0775\n",
      "Epoch 12/100\n",
      " - 18s - loss: 5.0733 - acc: 0.0776\n",
      "Epoch 13/100\n",
      " - 17s - loss: 5.0472 - acc: 0.0784\n",
      "Epoch 14/100\n",
      " - 17s - loss: 5.0225 - acc: 0.0760\n",
      "Epoch 15/100\n",
      " - 17s - loss: 4.9912 - acc: 0.0760\n",
      "Epoch 16/100\n",
      " - 18s - loss: 5.0016 - acc: 0.0676\n",
      "Epoch 17/100\n",
      " - 17s - loss: 4.9460 - acc: 0.0646\n",
      "Epoch 18/100\n",
      " - 18s - loss: 4.9544 - acc: 0.0695\n",
      "Epoch 19/100\n",
      " - 18s - loss: 4.9290 - acc: 0.0824\n",
      "Epoch 20/100\n",
      " - 17s - loss: 4.8704 - acc: 0.0814\n",
      "Epoch 21/100\n",
      " - 17s - loss: 4.8289 - acc: 0.0815\n",
      "Epoch 22/100\n",
      " - 17s - loss: 4.8166 - acc: 0.0788\n",
      "Epoch 23/100\n",
      " - 18s - loss: 4.7504 - acc: 0.0822\n",
      "Epoch 24/100\n",
      " - 17s - loss: 4.6829 - acc: 0.0823\n",
      "Epoch 25/100\n",
      " - 18s - loss: 4.6500 - acc: 0.0846\n",
      "Epoch 26/100\n",
      " - 18s - loss: 4.6015 - acc: 0.0936\n",
      "Epoch 27/100\n",
      " - 17s - loss: 4.5860 - acc: 0.0877\n",
      "Epoch 28/100\n",
      " - 17s - loss: 4.4711 - acc: 0.0981\n",
      "Epoch 29/100\n",
      " - 17s - loss: 4.3845 - acc: 0.0994\n",
      "Epoch 30/100\n",
      " - 18s - loss: 4.3443 - acc: 0.0980\n",
      "Epoch 31/100\n",
      " - 17s - loss: 4.2330 - acc: 0.0998\n",
      "Epoch 32/100\n",
      " - 17s - loss: 4.1782 - acc: 0.0987\n",
      "Epoch 33/100\n",
      " - 17s - loss: 4.1010 - acc: 0.1066\n",
      "Epoch 34/100\n",
      " - 18s - loss: 3.9998 - acc: 0.1105\n",
      "Epoch 35/100\n",
      " - 17s - loss: 4.0290 - acc: 0.1031\n",
      "Epoch 36/100\n",
      " - 17s - loss: 3.8764 - acc: 0.1251\n",
      "Epoch 37/100\n",
      " - 18s - loss: 3.8708 - acc: 0.1216\n",
      "Epoch 38/100\n",
      " - 17s - loss: 3.8199 - acc: 0.1236\n",
      "Epoch 39/100\n",
      " - 17s - loss: 3.7899 - acc: 0.1147\n",
      "Epoch 40/100\n",
      " - 17s - loss: 3.6737 - acc: 0.1320\n",
      "Epoch 41/100\n",
      " - 18s - loss: 3.6113 - acc: 0.1318\n",
      "Epoch 42/100\n",
      " - 17s - loss: 3.5294 - acc: 0.1348\n",
      "Epoch 43/100\n",
      " - 17s - loss: 3.4458 - acc: 0.1530\n",
      "Epoch 44/100\n",
      " - 18s - loss: 3.3674 - acc: 0.1684\n",
      "Epoch 45/100\n",
      " - 18s - loss: 3.2528 - acc: 0.1640\n",
      "Epoch 46/100\n",
      " - 17s - loss: 3.1514 - acc: 0.1745\n",
      "Epoch 47/100\n",
      " - 17s - loss: 3.0623 - acc: 0.1848\n",
      "Epoch 48/100\n",
      " - 18s - loss: 2.9967 - acc: 0.2035\n",
      "Epoch 49/100\n",
      " - 17s - loss: 2.8877 - acc: 0.2022\n",
      "Epoch 50/100\n",
      " - 17s - loss: 2.8619 - acc: 0.2068\n",
      "Epoch 51/100\n",
      " - 17s - loss: 2.7368 - acc: 0.2399\n",
      "Epoch 52/100\n",
      " - 18s - loss: 2.6720 - acc: 0.2404\n",
      "Epoch 53/100\n",
      " - 17s - loss: 2.5829 - acc: 0.2500\n",
      "Epoch 54/100\n",
      " - 17s - loss: 2.5262 - acc: 0.2666\n",
      "Epoch 55/100\n",
      " - 18s - loss: 2.5676 - acc: 0.2590\n",
      "Epoch 56/100\n",
      " - 17s - loss: 2.5248 - acc: 0.2567\n",
      "Epoch 57/100\n",
      " - 17s - loss: 2.3882 - acc: 0.2919\n",
      "Epoch 58/100\n",
      " - 17s - loss: 2.2876 - acc: 0.3079\n",
      "Epoch 59/100\n",
      " - 18s - loss: 2.2574 - acc: 0.3113\n",
      "Epoch 60/100\n",
      " - 17s - loss: 2.2498 - acc: 0.3072\n",
      "Epoch 61/100\n",
      " - 17s - loss: 2.1936 - acc: 0.3033\n",
      "Epoch 62/100\n",
      " - 17s - loss: 2.1425 - acc: 0.3532\n",
      "Epoch 63/100\n",
      " - 18s - loss: 2.0506 - acc: 0.3420\n",
      "Epoch 64/100\n",
      " - 17s - loss: 2.0100 - acc: 0.3553\n",
      "Epoch 65/100\n",
      " - 17s - loss: 1.9805 - acc: 0.3673\n",
      "Epoch 66/100\n",
      " - 18s - loss: 1.9678 - acc: 0.3626\n",
      "Epoch 67/100\n",
      " - 17s - loss: 2.0262 - acc: 0.3603\n",
      "Epoch 68/100\n",
      " - 17s - loss: 1.9195 - acc: 0.3839\n",
      "Epoch 69/100\n",
      " - 17s - loss: 1.8322 - acc: 0.4203\n",
      "Epoch 70/100\n",
      " - 18s - loss: 1.8429 - acc: 0.3821\n",
      "Epoch 71/100\n",
      " - 17s - loss: 1.7846 - acc: 0.4243\n",
      "Epoch 72/100\n",
      " - 17s - loss: 1.6829 - acc: 0.4263\n",
      "Epoch 73/100\n",
      " - 18s - loss: 1.6622 - acc: 0.4323\n",
      "Epoch 74/100\n",
      " - 17s - loss: 1.6256 - acc: 0.4476\n",
      "Epoch 75/100\n",
      " - 17s - loss: 1.5840 - acc: 0.4530\n",
      "Epoch 76/100\n",
      " - 17s - loss: 1.5180 - acc: 0.4872\n",
      "Epoch 77/100\n",
      " - 18s - loss: 1.4986 - acc: 0.4880\n",
      "Epoch 78/100\n",
      " - 17s - loss: 1.5101 - acc: 0.4813\n",
      "Epoch 79/100\n",
      " - 17s - loss: 1.5501 - acc: 0.4688\n",
      "Epoch 80/100\n",
      " - 18s - loss: 1.5713 - acc: 0.4697\n",
      "Epoch 81/100\n",
      " - 18s - loss: 1.6410 - acc: 0.4307\n",
      "Epoch 82/100\n",
      " - 17s - loss: 1.7138 - acc: 0.4267\n",
      "Epoch 83/100\n",
      " - 17s - loss: 1.5431 - acc: 0.4482\n",
      "Epoch 84/100\n",
      " - 18s - loss: 1.4762 - acc: 0.4774\n",
      "Epoch 85/100\n",
      " - 17s - loss: 1.4026 - acc: 0.4881\n",
      "Epoch 86/100\n",
      " - 17s - loss: 1.3827 - acc: 0.5043\n",
      "Epoch 87/100\n",
      " - 17s - loss: 1.3481 - acc: 0.5269\n",
      "Epoch 88/100\n",
      " - 18s - loss: 1.3184 - acc: 0.5379\n",
      "Epoch 89/100\n",
      " - 17s - loss: 1.3220 - acc: 0.5415\n",
      "Epoch 90/100\n",
      " - 17s - loss: 1.2969 - acc: 0.5276\n",
      "Epoch 91/100\n",
      " - 18s - loss: 1.2914 - acc: 0.5272\n",
      "Epoch 92/100\n",
      " - 17s - loss: 1.2419 - acc: 0.5491\n",
      "Epoch 93/100\n",
      " - 18s - loss: 1.1933 - acc: 0.5647\n",
      "Epoch 94/100\n",
      " - 17s - loss: 1.2403 - acc: 0.5549\n",
      "Epoch 95/100\n",
      " - 18s - loss: 1.1839 - acc: 0.5720\n",
      "Epoch 96/100\n",
      " - 17s - loss: 1.1861 - acc: 0.5758\n",
      "Epoch 97/100\n",
      " - 17s - loss: 1.1985 - acc: 0.5707\n",
      "Epoch 98/100\n",
      " - 17s - loss: 1.1682 - acc: 0.5914\n",
      "Epoch 99/100\n",
      " - 18s - loss: 1.1293 - acc: 0.6033\n",
      "Epoch 100/100\n",
      " - 17s - loss: 1.1276 - acc: 0.5949\n",
      ">3: train=0.058909 test=0.019423\n",
      "          train      test\n",
      "count  3.000000  3.000000\n",
      "mean   0.044207  0.209427\n",
      "std    0.034184  0.255880\n",
      "min    0.005131  0.019423\n",
      "25%    0.032020  0.063949\n",
      "50%    0.058909  0.108476\n",
      "75%    0.063745  0.304430\n",
      "max    0.068580  0.500384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shourya97/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "verbose = 2\n",
    "n_epochs = 100\n",
    "n_photos_per_update = 2\n",
    "n_batches_per_epoch = int(len(train) / n_photos_per_update)\n",
    "n_repeats = 3\n",
    "\n",
    "# run experiment\n",
    "train_results, test_results = list(), list()\n",
    "for i in range(n_repeats):\n",
    "    # define the model\n",
    "    model = define_model(vocab_size, max_length)\n",
    "    # define checkpoint callback\n",
    "#     filepath = 'size_lg_seq_model-model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "#     checkpoint = ModelCheckpoint(filepath, monitor='val_loss',\n",
    "#                                  verbose=1,\n",
    "#                                  save_best_only=True,\n",
    "#                                  mode='min')\n",
    "    # fit model\n",
    "    model.fit_generator(data_generator(train_descriptions, train_features, tokenizer, max_length, n_photos_per_update), \n",
    "                        steps_per_epoch=n_batches_per_epoch, \n",
    "                        epochs=n_epochs,\n",
    "                        verbose=verbose)\n",
    "    # evaluate model on training data\n",
    "    train_score = evaluate_model(model, train_descriptions, train_features, tokenizer, max_length)\n",
    "    test_score = evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)\n",
    "    # store\n",
    "    train_results.append(train_score)\n",
    "    test_results.append(test_score)\n",
    "    print('>%d: train=%f test=%f' % ((i+1), train_score, test_score))\n",
    "# save results to file\n",
    "df = DataFrame()\n",
    "df['train'] = train_results\n",
    "df['test'] = test_results\n",
    "print(df.describe())\n",
    "df.to_csv(model_name+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU scores table\n",
    "|       |   train  |    test|\n",
    "|-------|----------|--------|\n",
    "|count  |3.000000  |3.000000|\n",
    "|mean   |0.044207  |0.209427|\n",
    "|std    |0.034184  |0.255880|\n",
    "|min    |0.005131  |0.019423|\n",
    "|25%    |0.032020  |0.063949|\n",
    "|50%    |0.058909  |0.108476|\n",
    "|75%    |0.063745  |0.304430|\n",
    "|max    |0.068580  |0.500384|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try to **increase the representational capacity of the word embedding by doubling it from 50-dimensions to 100-dimensions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "    # feature extractor (encoder)\n",
    "    inputs1 = Input(shape=(7, 7, 512))\n",
    "    fe1 = GlobalMaxPooling2D()(inputs1)\n",
    "    fe2 = Dense(128, activation='relu')(fe1)\n",
    "    fe3 = RepeatVector(max_length)(fe2)\n",
    "    # embedding\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    emb2 = Embedding(vocab_size, 100, mask_zero=True)(inputs2)\n",
    "    emb3 = LSTM(256, return_sequences=True)(emb2)\n",
    "    emb4 = TimeDistributed(Dense(128, activation='relu'))(emb3)\n",
    "    # merge inputs\n",
    "    merged = concatenate([fe3, emb4])\n",
    "    # language model (decoder)\n",
    "    lm2 = LSTM(500)(merged)\n",
    "    lm3 = Dense(500, activation='relu')(lm2)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(lm3)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_name = 'size_em_seq_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 16s - loss: 7.0816 - acc: 0.0384\n",
      "Epoch 2/100\n",
      " - 14s - loss: 5.5922 - acc: 0.0587\n",
      "Epoch 3/100\n",
      " - 13s - loss: 5.4991 - acc: 0.0556\n",
      "Epoch 4/100\n",
      " - 13s - loss: 5.4177 - acc: 0.0630\n",
      "Epoch 5/100\n",
      " - 13s - loss: 5.3538 - acc: 0.0694\n",
      "Epoch 6/100\n",
      " - 13s - loss: 5.2890 - acc: 0.0749\n",
      "Epoch 7/100\n",
      " - 14s - loss: 5.2516 - acc: 0.0706\n",
      "Epoch 8/100\n",
      " - 13s - loss: 5.2130 - acc: 0.0756\n",
      "Epoch 9/100\n",
      " - 13s - loss: 5.1021 - acc: 0.0776\n",
      "Epoch 10/100\n",
      " - 13s - loss: 5.0967 - acc: 0.0762\n",
      "Epoch 11/100\n",
      " - 14s - loss: 5.0426 - acc: 0.0777\n",
      "Epoch 12/100\n",
      " - 13s - loss: 5.0445 - acc: 0.0793\n",
      "Epoch 13/100\n",
      " - 13s - loss: 5.0005 - acc: 0.0820\n",
      "Epoch 14/100\n",
      " - 13s - loss: 4.9289 - acc: 0.0752\n",
      "Epoch 15/100\n",
      " - 13s - loss: 4.9374 - acc: 0.0775\n",
      "Epoch 16/100\n",
      " - 13s - loss: 4.8981 - acc: 0.0760\n",
      "Epoch 17/100\n",
      " - 13s - loss: 4.9274 - acc: 0.0740\n",
      "Epoch 18/100\n",
      " - 13s - loss: 4.8305 - acc: 0.0760\n",
      "Epoch 19/100\n",
      " - 13s - loss: 4.7516 - acc: 0.0738\n",
      "Epoch 20/100\n",
      " - 13s - loss: 4.7068 - acc: 0.0813\n",
      "Epoch 21/100\n",
      " - 14s - loss: 4.7060 - acc: 0.0879\n",
      "Epoch 22/100\n",
      " - 13s - loss: 4.6600 - acc: 0.0800\n",
      "Epoch 23/100\n",
      " - 13s - loss: 4.5770 - acc: 0.0934\n",
      "Epoch 24/100\n",
      " - 13s - loss: 4.5516 - acc: 0.0963\n",
      "Epoch 25/100\n",
      " - 13s - loss: 4.4799 - acc: 0.0910\n",
      "Epoch 26/100\n",
      " - 14s - loss: 4.4053 - acc: 0.0881\n",
      "Epoch 27/100\n",
      " - 13s - loss: 4.3858 - acc: 0.0838\n",
      "Epoch 28/100\n",
      " - 13s - loss: 4.3206 - acc: 0.0988\n",
      "Epoch 29/100\n",
      " - 13s - loss: 4.2025 - acc: 0.1017\n",
      "Epoch 30/100\n",
      " - 13s - loss: 4.1722 - acc: 0.1106\n",
      "Epoch 31/100\n",
      " - 13s - loss: 4.1618 - acc: 0.1125\n",
      "Epoch 32/100\n",
      " - 13s - loss: 4.1214 - acc: 0.1082\n",
      "Epoch 33/100\n",
      " - 13s - loss: 3.9380 - acc: 0.1156\n",
      "Epoch 34/100\n",
      " - 13s - loss: 3.8241 - acc: 0.1300\n",
      "Epoch 35/100\n",
      " - 14s - loss: 3.7188 - acc: 0.1323\n",
      "Epoch 36/100\n",
      " - 13s - loss: 3.6539 - acc: 0.1319\n",
      "Epoch 37/100\n",
      " - 13s - loss: 3.5653 - acc: 0.1422\n",
      "Epoch 38/100\n",
      " - 13s - loss: 3.5376 - acc: 0.1449\n",
      "Epoch 39/100\n",
      " - 13s - loss: 3.4644 - acc: 0.1632\n",
      "Epoch 40/100\n",
      " - 14s - loss: 3.4156 - acc: 0.1524\n",
      "Epoch 41/100\n",
      " - 13s - loss: 3.3621 - acc: 0.1622\n",
      "Epoch 42/100\n",
      " - 13s - loss: 3.2513 - acc: 0.1825\n",
      "Epoch 43/100\n",
      " - 13s - loss: 3.2229 - acc: 0.1935\n",
      "Epoch 44/100\n",
      " - 14s - loss: 3.1047 - acc: 0.1938\n",
      "Epoch 45/100\n",
      " - 13s - loss: 3.0089 - acc: 0.2109\n",
      "Epoch 46/100\n",
      " - 13s - loss: 2.9474 - acc: 0.2161\n",
      "Epoch 47/100\n",
      " - 13s - loss: 2.8749 - acc: 0.2346\n",
      "Epoch 48/100\n",
      " - 13s - loss: 2.7832 - acc: 0.2256\n",
      "Epoch 49/100\n",
      " - 14s - loss: 2.7534 - acc: 0.2394\n",
      "Epoch 50/100\n",
      " - 13s - loss: 2.7585 - acc: 0.2555\n",
      "Epoch 51/100\n",
      " - 13s - loss: 2.6103 - acc: 0.2784\n",
      "Epoch 52/100\n",
      " - 13s - loss: 2.4834 - acc: 0.2835\n",
      "Epoch 53/100\n",
      " - 13s - loss: 2.5050 - acc: 0.2826\n",
      "Epoch 54/100\n",
      " - 14s - loss: 2.4577 - acc: 0.2989\n",
      "Epoch 55/100\n",
      " - 13s - loss: 2.3141 - acc: 0.3501\n",
      "Epoch 56/100\n",
      " - 13s - loss: 2.2522 - acc: 0.3463\n",
      "Epoch 57/100\n",
      " - 13s - loss: 2.1533 - acc: 0.3545\n",
      "Epoch 58/100\n",
      " - 13s - loss: 2.1085 - acc: 0.3564\n",
      "Epoch 59/100\n",
      " - 13s - loss: 1.9799 - acc: 0.4245\n",
      "Epoch 60/100\n",
      " - 13s - loss: 1.9363 - acc: 0.4119\n",
      "Epoch 61/100\n",
      " - 13s - loss: 1.9023 - acc: 0.4105\n",
      "Epoch 62/100\n",
      " - 13s - loss: 1.8168 - acc: 0.4431\n",
      "Epoch 63/100\n",
      " - 13s - loss: 1.7836 - acc: 0.4460\n",
      "Epoch 64/100\n",
      " - 13s - loss: 1.7748 - acc: 0.4561\n",
      "Epoch 65/100\n",
      " - 13s - loss: 1.7087 - acc: 0.4617\n",
      "Epoch 66/100\n",
      " - 13s - loss: 1.7137 - acc: 0.4563\n",
      "Epoch 67/100\n",
      " - 13s - loss: 1.7265 - acc: 0.4699\n",
      "Epoch 68/100\n",
      " - 14s - loss: 1.7135 - acc: 0.4577\n",
      "Epoch 69/100\n",
      " - 13s - loss: 1.6953 - acc: 0.4559\n",
      "Epoch 70/100\n",
      " - 13s - loss: 1.5717 - acc: 0.5083\n",
      "Epoch 71/100\n",
      " - 13s - loss: 1.5790 - acc: 0.5128\n",
      "Epoch 72/100\n",
      " - 13s - loss: 1.4848 - acc: 0.5137\n",
      "Epoch 73/100\n",
      " - 13s - loss: 1.4011 - acc: 0.5433\n",
      "Epoch 74/100\n",
      " - 13s - loss: 1.3460 - acc: 0.5433\n",
      "Epoch 75/100\n",
      " - 13s - loss: 1.3677 - acc: 0.5493\n",
      "Epoch 76/100\n",
      " - 13s - loss: 1.4510 - acc: 0.5300\n",
      "Epoch 77/100\n",
      " - 14s - loss: 1.3431 - acc: 0.5679\n",
      "Epoch 78/100\n",
      " - 13s - loss: 1.2818 - acc: 0.5741\n",
      "Epoch 79/100\n",
      " - 13s - loss: 1.1991 - acc: 0.5780\n",
      "Epoch 80/100\n",
      " - 13s - loss: 1.2190 - acc: 0.5999\n",
      "Epoch 81/100\n",
      " - 13s - loss: 1.2278 - acc: 0.5667\n",
      "Epoch 82/100\n",
      " - 14s - loss: 1.1978 - acc: 0.6056\n",
      "Epoch 83/100\n",
      " - 13s - loss: 1.1832 - acc: 0.5975\n",
      "Epoch 84/100\n",
      " - 13s - loss: 1.1460 - acc: 0.6185\n",
      "Epoch 85/100\n",
      " - 13s - loss: 1.1039 - acc: 0.6216\n",
      "Epoch 86/100\n",
      " - 13s - loss: 1.1291 - acc: 0.6238\n",
      "Epoch 87/100\n",
      " - 13s - loss: 1.0445 - acc: 0.6307\n",
      "Epoch 88/100\n",
      " - 13s - loss: 1.0124 - acc: 0.6422\n",
      "Epoch 89/100\n",
      " - 13s - loss: 1.0505 - acc: 0.6355\n",
      "Epoch 90/100\n",
      " - 13s - loss: 1.0160 - acc: 0.6538\n",
      "Epoch 91/100\n",
      " - 13s - loss: 0.9558 - acc: 0.6577\n",
      "Epoch 92/100\n",
      " - 14s - loss: 1.0335 - acc: 0.6349\n",
      "Epoch 93/100\n",
      " - 13s - loss: 1.0005 - acc: 0.6392\n",
      "Epoch 94/100\n",
      " - 13s - loss: 1.0029 - acc: 0.6567\n",
      "Epoch 95/100\n",
      " - 13s - loss: 0.9132 - acc: 0.6805\n",
      "Epoch 96/100\n",
      " - 14s - loss: 0.8665 - acc: 0.6811\n",
      "Epoch 97/100\n",
      " - 13s - loss: 0.8515 - acc: 0.6908\n",
      "Epoch 98/100\n",
      " - 13s - loss: 0.8515 - acc: 0.7113\n",
      "Epoch 99/100\n",
      " - 13s - loss: 0.8408 - acc: 0.6949\n",
      "Epoch 100/100\n",
      " - 13s - loss: 0.8568 - acc: 0.7061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shourya97/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1: train=0.094151 test=0.129000\n",
      "Epoch 1/100\n",
      " - 15s - loss: 7.0672 - acc: 0.0482\n",
      "Epoch 2/100\n",
      " - 13s - loss: 5.6212 - acc: 0.0549\n",
      "Epoch 3/100\n",
      " - 13s - loss: 5.4564 - acc: 0.0551\n",
      "Epoch 4/100\n",
      " - 13s - loss: 5.3928 - acc: 0.0618\n",
      "Epoch 5/100\n",
      " - 13s - loss: 5.3656 - acc: 0.0630\n",
      "Epoch 6/100\n",
      " - 13s - loss: 5.3300 - acc: 0.0630\n",
      "Epoch 7/100\n",
      " - 13s - loss: 5.3128 - acc: 0.0630\n",
      "Epoch 8/100\n",
      " - 13s - loss: 5.3157 - acc: 0.0630\n",
      "Epoch 9/100\n",
      " - 14s - loss: 5.2821 - acc: 0.0704\n",
      "Epoch 10/100\n",
      " - 13s - loss: 5.2230 - acc: 0.0745\n",
      "Epoch 11/100\n",
      " - 13s - loss: 5.1899 - acc: 0.0768\n",
      "Epoch 12/100\n",
      " - 13s - loss: 5.1343 - acc: 0.0769\n",
      "Epoch 13/100\n",
      " - 13s - loss: 5.1114 - acc: 0.0756\n",
      "Epoch 14/100\n",
      " - 13s - loss: 5.0610 - acc: 0.0751\n",
      "Epoch 15/100\n",
      " - 13s - loss: 5.0387 - acc: 0.0795\n",
      "Epoch 16/100\n",
      " - 13s - loss: 5.0103 - acc: 0.0763\n",
      "Epoch 17/100\n",
      " - 13s - loss: 5.0151 - acc: 0.0783\n",
      "Epoch 18/100\n",
      " - 14s - loss: 4.9446 - acc: 0.0765\n",
      "Epoch 19/100\n",
      " - 13s - loss: 4.9442 - acc: 0.0773\n",
      "Epoch 20/100\n",
      " - 13s - loss: 4.8620 - acc: 0.0835\n",
      "Epoch 21/100\n",
      " - 13s - loss: 4.8460 - acc: 0.0835\n",
      "Epoch 22/100\n",
      " - 13s - loss: 4.8379 - acc: 0.0850\n",
      "Epoch 23/100\n",
      " - 13s - loss: 4.7291 - acc: 0.0902\n",
      "Epoch 24/100\n",
      " - 13s - loss: 4.6063 - acc: 0.0906\n",
      "Epoch 25/100\n",
      " - 13s - loss: 4.5207 - acc: 0.0952\n",
      "Epoch 26/100\n",
      " - 13s - loss: 4.4479 - acc: 0.0938\n",
      "Epoch 27/100\n",
      " - 13s - loss: 4.3857 - acc: 0.0929\n",
      "Epoch 28/100\n",
      " - 14s - loss: 4.3355 - acc: 0.0938\n",
      "Epoch 29/100\n",
      " - 13s - loss: 4.3815 - acc: 0.0926\n",
      "Epoch 30/100\n",
      " - 13s - loss: 4.2873 - acc: 0.0940\n",
      "Epoch 31/100\n",
      " - 13s - loss: 4.2135 - acc: 0.0972\n",
      "Epoch 32/100\n",
      " - 14s - loss: 4.2015 - acc: 0.1047\n",
      "Epoch 33/100\n",
      " - 15s - loss: 4.0647 - acc: 0.0990\n",
      "Epoch 34/100\n",
      " - 14s - loss: 3.9896 - acc: 0.0997\n",
      "Epoch 35/100\n",
      " - 14s - loss: 3.9454 - acc: 0.0997\n",
      "Epoch 36/100\n",
      " - 14s - loss: 3.8922 - acc: 0.1055\n",
      "Epoch 37/100\n",
      " - 14s - loss: 3.8239 - acc: 0.1152\n",
      "Epoch 38/100\n",
      " - 13s - loss: 3.7204 - acc: 0.1266\n",
      "Epoch 39/100\n",
      " - 15s - loss: 3.6776 - acc: 0.1367\n",
      "Epoch 40/100\n",
      " - 14s - loss: 3.5751 - acc: 0.1358\n",
      "Epoch 41/100\n",
      " - 14s - loss: 3.5073 - acc: 0.1418\n",
      "Epoch 42/100\n",
      " - 13s - loss: 3.5228 - acc: 0.1491\n",
      "Epoch 43/100\n",
      " - 13s - loss: 3.4305 - acc: 0.1682\n",
      "Epoch 44/100\n",
      " - 13s - loss: 3.3188 - acc: 0.1701\n",
      "Epoch 45/100\n",
      " - 13s - loss: 3.2158 - acc: 0.1807\n",
      "Epoch 46/100\n",
      " - 13s - loss: 3.0835 - acc: 0.1863\n",
      "Epoch 47/100\n",
      " - 13s - loss: 3.0400 - acc: 0.2019\n",
      "Epoch 48/100\n",
      " - 13s - loss: 2.9486 - acc: 0.2106\n",
      "Epoch 49/100\n",
      " - 13s - loss: 2.9111 - acc: 0.2154\n",
      "Epoch 50/100\n",
      " - 13s - loss: 2.8441 - acc: 0.2260\n",
      "Epoch 51/100\n",
      " - 14s - loss: 2.7235 - acc: 0.2386\n",
      "Epoch 52/100\n",
      " - 13s - loss: 2.7010 - acc: 0.2579\n",
      "Epoch 53/100\n",
      " - 13s - loss: 2.7003 - acc: 0.2453\n",
      "Epoch 54/100\n",
      " - 13s - loss: 2.5541 - acc: 0.2601\n",
      "Epoch 55/100\n",
      " - 14s - loss: 2.5019 - acc: 0.2789\n",
      "Epoch 56/100\n",
      " - 13s - loss: 2.4500 - acc: 0.2886\n",
      "Epoch 57/100\n",
      " - 13s - loss: 2.3649 - acc: 0.3126\n",
      "Epoch 58/100\n",
      " - 13s - loss: 2.2697 - acc: 0.3225\n",
      "Epoch 59/100\n",
      " - 13s - loss: 2.1793 - acc: 0.3433\n",
      "Epoch 60/100\n",
      " - 14s - loss: 2.1579 - acc: 0.3434\n",
      "Epoch 61/100\n",
      " - 13s - loss: 2.0651 - acc: 0.3905\n",
      "Epoch 62/100\n",
      " - 13s - loss: 2.0267 - acc: 0.3867\n",
      "Epoch 63/100\n",
      " - 13s - loss: 1.9271 - acc: 0.4032\n",
      "Epoch 64/100\n",
      " - 13s - loss: 1.8896 - acc: 0.4110\n",
      "Epoch 65/100\n",
      " - 14s - loss: 1.8679 - acc: 0.4091\n",
      "Epoch 66/100\n",
      " - 13s - loss: 1.7490 - acc: 0.4421\n",
      "Epoch 67/100\n",
      " - 13s - loss: 1.7101 - acc: 0.4523\n",
      "Epoch 68/100\n",
      " - 13s - loss: 1.6554 - acc: 0.4666\n",
      "Epoch 69/100\n",
      " - 14s - loss: 1.6037 - acc: 0.4748\n",
      "Epoch 70/100\n",
      " - 13s - loss: 1.6006 - acc: 0.4854\n",
      "Epoch 71/100\n",
      " - 13s - loss: 1.5710 - acc: 0.4906\n",
      "Epoch 72/100\n",
      " - 13s - loss: 1.5158 - acc: 0.5060\n",
      "Epoch 73/100\n",
      " - 13s - loss: 1.5341 - acc: 0.5080\n",
      "Epoch 74/100\n",
      " - 13s - loss: 1.5000 - acc: 0.4987\n",
      "Epoch 75/100\n",
      " - 13s - loss: 1.4781 - acc: 0.5113\n",
      "Epoch 76/100\n",
      " - 13s - loss: 1.4629 - acc: 0.5198\n",
      "Epoch 77/100\n",
      " - 13s - loss: 1.3593 - acc: 0.5266\n",
      "Epoch 78/100\n",
      " - 13s - loss: 1.3105 - acc: 0.5476\n",
      "Epoch 79/100\n",
      " - 14s - loss: 1.2475 - acc: 0.5709\n",
      "Epoch 80/100\n",
      " - 13s - loss: 1.1615 - acc: 0.6007\n",
      "Epoch 81/100\n",
      " - 13s - loss: 1.1721 - acc: 0.5975\n",
      "Epoch 82/100\n",
      " - 13s - loss: 1.1930 - acc: 0.5969\n",
      "Epoch 83/100\n",
      " - 13s - loss: 1.1490 - acc: 0.6105\n",
      "Epoch 84/100\n",
      " - 13s - loss: 1.1725 - acc: 0.5932\n",
      "Epoch 85/100\n",
      " - 13s - loss: 1.2936 - acc: 0.5611\n",
      "Epoch 86/100\n",
      " - 13s - loss: 1.2133 - acc: 0.5833\n",
      "Epoch 87/100\n",
      " - 13s - loss: 1.1645 - acc: 0.6119\n",
      "Epoch 88/100\n",
      " - 13s - loss: 1.1277 - acc: 0.5940\n",
      "Epoch 89/100\n",
      " - 13s - loss: 1.0875 - acc: 0.6034\n",
      "Epoch 90/100\n",
      " - 13s - loss: 1.0799 - acc: 0.6305\n",
      "Epoch 91/100\n",
      " - 13s - loss: 1.0336 - acc: 0.6534\n",
      "Epoch 92/100\n",
      " - 13s - loss: 1.0100 - acc: 0.6474\n",
      "Epoch 93/100\n",
      " - 14s - loss: 1.0342 - acc: 0.6484\n",
      "Epoch 94/100\n",
      " - 13s - loss: 1.0089 - acc: 0.6565\n",
      "Epoch 95/100\n",
      " - 13s - loss: 1.0027 - acc: 0.6523\n",
      "Epoch 96/100\n",
      " - 13s - loss: 0.9203 - acc: 0.6652\n",
      "Epoch 97/100\n",
      " - 13s - loss: 0.9041 - acc: 0.6828\n",
      "Epoch 98/100\n",
      " - 13s - loss: 1.0278 - acc: 0.6302\n",
      "Epoch 99/100\n",
      " - 13s - loss: 0.8989 - acc: 0.6770\n",
      "Epoch 100/100\n",
      " - 13s - loss: 0.8641 - acc: 0.6779\n",
      ">2: train=0.022305 test=0.069993\n",
      "Epoch 1/100\n",
      " - 15s - loss: 7.1081 - acc: 0.0411\n",
      "Epoch 2/100\n",
      " - 13s - loss: 5.6121 - acc: 0.0570\n",
      "Epoch 3/100\n",
      " - 13s - loss: 5.4737 - acc: 0.0666\n",
      "Epoch 4/100\n",
      " - 13s - loss: 5.3624 - acc: 0.0634\n",
      "Epoch 5/100\n",
      " - 13s - loss: 5.3399 - acc: 0.0660\n",
      "Epoch 6/100\n",
      " - 13s - loss: 5.2921 - acc: 0.0761\n",
      "Epoch 7/100\n",
      " - 13s - loss: 5.2143 - acc: 0.0801\n",
      "Epoch 8/100\n",
      " - 13s - loss: 5.1682 - acc: 0.0803\n",
      "Epoch 9/100\n",
      " - 13s - loss: 5.1210 - acc: 0.0812\n",
      "Epoch 10/100\n",
      " - 13s - loss: 5.1223 - acc: 0.0804\n",
      "Epoch 11/100\n",
      " - 13s - loss: 5.0388 - acc: 0.0784\n",
      "Epoch 12/100\n",
      " - 13s - loss: 5.0640 - acc: 0.0816\n",
      "Epoch 13/100\n",
      " - 13s - loss: 4.9665 - acc: 0.0775\n",
      "Epoch 14/100\n",
      " - 18s - loss: 4.9507 - acc: 0.0813\n",
      "Epoch 15/100\n",
      " - 14s - loss: 4.8926 - acc: 0.0878\n",
      "Epoch 16/100\n",
      " - 17s - loss: 4.8504 - acc: 0.0941\n",
      "Epoch 17/100\n",
      " - 16s - loss: 4.8082 - acc: 0.0906\n",
      "Epoch 18/100\n",
      " - 15s - loss: 4.7388 - acc: 0.0865\n",
      "Epoch 19/100\n",
      " - 14s - loss: 4.6595 - acc: 0.0894\n",
      "Epoch 20/100\n",
      " - 13s - loss: 4.5714 - acc: 0.0919\n",
      "Epoch 21/100\n",
      " - 13s - loss: 4.5379 - acc: 0.1003\n",
      "Epoch 22/100\n",
      " - 14s - loss: 4.5181 - acc: 0.0939\n",
      "Epoch 23/100\n",
      " - 14s - loss: 4.4217 - acc: 0.0972\n",
      "Epoch 24/100\n",
      " - 13s - loss: 4.4098 - acc: 0.0947\n",
      "Epoch 25/100\n",
      " - 15s - loss: 4.4136 - acc: 0.1005\n",
      "Epoch 26/100\n",
      " - 14s - loss: 4.4120 - acc: 0.0973\n",
      "Epoch 27/100\n",
      " - 14s - loss: 4.3714 - acc: 0.1031\n",
      "Epoch 28/100\n",
      " - 13s - loss: 4.2857 - acc: 0.0982\n",
      "Epoch 29/100\n",
      " - 13s - loss: 4.2465 - acc: 0.1044\n",
      "Epoch 30/100\n",
      " - 14s - loss: 4.1292 - acc: 0.1144\n",
      "Epoch 31/100\n",
      " - 13s - loss: 4.0338 - acc: 0.1208\n",
      "Epoch 32/100\n",
      " - 13s - loss: 3.9667 - acc: 0.1219\n",
      "Epoch 33/100\n",
      " - 13s - loss: 3.9088 - acc: 0.1237\n",
      "Epoch 34/100\n",
      " - 14s - loss: 3.8556 - acc: 0.1199\n",
      "Epoch 35/100\n",
      " - 13s - loss: 3.7922 - acc: 0.1360\n",
      "Epoch 36/100\n",
      " - 13s - loss: 3.7178 - acc: 0.1317\n",
      "Epoch 37/100\n",
      " - 13s - loss: 3.6291 - acc: 0.1416\n",
      "Epoch 38/100\n",
      " - 13s - loss: 3.5786 - acc: 0.1366\n",
      "Epoch 39/100\n",
      " - 14s - loss: 3.5001 - acc: 0.1477\n",
      "Epoch 40/100\n",
      " - 13s - loss: 3.4229 - acc: 0.1541\n",
      "Epoch 41/100\n",
      " - 13s - loss: 3.3895 - acc: 0.1558\n",
      "Epoch 42/100\n",
      " - 13s - loss: 3.3190 - acc: 0.1531\n",
      "Epoch 43/100\n",
      " - 13s - loss: 3.2526 - acc: 0.1520\n",
      "Epoch 44/100\n",
      " - 14s - loss: 3.1677 - acc: 0.1788\n",
      "Epoch 45/100\n",
      " - 13s - loss: 3.1241 - acc: 0.1836\n",
      "Epoch 46/100\n",
      " - 13s - loss: 3.0723 - acc: 0.1751\n",
      "Epoch 47/100\n",
      " - 13s - loss: 2.9931 - acc: 0.1858\n",
      "Epoch 48/100\n",
      " - 15s - loss: 2.9509 - acc: 0.1998\n",
      "Epoch 49/100\n",
      " - 15s - loss: 2.8432 - acc: 0.2330\n",
      "Epoch 50/100\n",
      " - 14s - loss: 2.7387 - acc: 0.2379\n",
      "Epoch 51/100\n",
      " - 14s - loss: 2.6359 - acc: 0.2427\n",
      "Epoch 52/100\n",
      " - 14s - loss: 2.5829 - acc: 0.2380\n",
      "Epoch 53/100\n",
      " - 14s - loss: 2.4914 - acc: 0.2835\n",
      "Epoch 54/100\n",
      " - 14s - loss: 2.4880 - acc: 0.2818\n",
      "Epoch 55/100\n",
      " - 14s - loss: 2.4354 - acc: 0.2819\n",
      "Epoch 56/100\n",
      " - 14s - loss: 2.3203 - acc: 0.3257\n",
      "Epoch 57/100\n",
      " - 14s - loss: 2.2307 - acc: 0.3328\n",
      "Epoch 58/100\n",
      " - 15s - loss: 2.1968 - acc: 0.3591\n",
      "Epoch 59/100\n",
      " - 15s - loss: 2.1426 - acc: 0.3353\n",
      "Epoch 60/100\n",
      " - 14s - loss: 2.0858 - acc: 0.3812\n",
      "Epoch 61/100\n",
      " - 13s - loss: 2.0588 - acc: 0.3704\n",
      "Epoch 62/100\n",
      " - 14s - loss: 2.0074 - acc: 0.3836\n",
      "Epoch 63/100\n",
      " - 13s - loss: 2.0000 - acc: 0.3973\n",
      "Epoch 64/100\n",
      " - 13s - loss: 1.9471 - acc: 0.4037\n",
      "Epoch 65/100\n",
      " - 13s - loss: 1.8644 - acc: 0.4283\n",
      "Epoch 66/100\n",
      " - 14s - loss: 1.7967 - acc: 0.4471\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 14s - loss: 1.7319 - acc: 0.4416\n",
      "Epoch 68/100\n",
      " - 15s - loss: 1.7128 - acc: 0.4675\n",
      "Epoch 69/100\n",
      " - 13s - loss: 1.6908 - acc: 0.4704\n",
      "Epoch 70/100\n",
      " - 13s - loss: 1.6306 - acc: 0.4926\n",
      "Epoch 71/100\n",
      " - 14s - loss: 1.5895 - acc: 0.4740\n",
      "Epoch 72/100\n",
      " - 14s - loss: 1.5373 - acc: 0.5065\n",
      "Epoch 73/100\n",
      " - 14s - loss: 1.4898 - acc: 0.5180\n",
      "Epoch 74/100\n",
      " - 13s - loss: 1.4748 - acc: 0.4932\n",
      "Epoch 75/100\n",
      " - 13s - loss: 1.4967 - acc: 0.4977\n",
      "Epoch 76/100\n",
      " - 13s - loss: 1.5082 - acc: 0.5044\n",
      "Epoch 77/100\n",
      " - 13s - loss: 1.4385 - acc: 0.5249\n",
      "Epoch 78/100\n",
      " - 13s - loss: 1.4902 - acc: 0.5318\n",
      "Epoch 79/100\n",
      " - 13s - loss: 1.4354 - acc: 0.5318\n",
      "Epoch 80/100\n",
      " - 13s - loss: 1.3421 - acc: 0.5634\n",
      "Epoch 81/100\n",
      " - 14s - loss: 1.2671 - acc: 0.5813\n",
      "Epoch 82/100\n",
      " - 13s - loss: 1.2576 - acc: 0.5810\n",
      "Epoch 83/100\n",
      " - 14s - loss: 1.2285 - acc: 0.5763\n",
      "Epoch 84/100\n",
      " - 13s - loss: 1.2303 - acc: 0.5922\n",
      "Epoch 85/100\n",
      " - 13s - loss: 1.2418 - acc: 0.5806\n",
      "Epoch 86/100\n",
      " - 13s - loss: 1.1761 - acc: 0.5886\n",
      "Epoch 87/100\n",
      " - 13s - loss: 1.1425 - acc: 0.6173\n",
      "Epoch 88/100\n",
      " - 13s - loss: 1.1459 - acc: 0.6089\n",
      "Epoch 89/100\n",
      " - 13s - loss: 1.1109 - acc: 0.6305\n",
      "Epoch 90/100\n",
      " - 14s - loss: 1.1058 - acc: 0.6227\n",
      "Epoch 91/100\n",
      " - 13s - loss: 1.0420 - acc: 0.6070\n",
      "Epoch 92/100\n",
      " - 13s - loss: 1.0340 - acc: 0.6485\n",
      "Epoch 93/100\n",
      " - 13s - loss: 1.0772 - acc: 0.6275\n",
      "Epoch 94/100\n",
      " - 13s - loss: 1.0758 - acc: 0.6191\n",
      "Epoch 95/100\n",
      " - 13s - loss: 1.0984 - acc: 0.6269\n",
      "Epoch 96/100\n",
      " - 13s - loss: 1.0108 - acc: 0.6358\n",
      "Epoch 97/100\n",
      " - 14s - loss: 0.9759 - acc: 0.6679\n",
      "Epoch 98/100\n",
      " - 13s - loss: 0.9917 - acc: 0.6605\n",
      "Epoch 99/100\n",
      " - 14s - loss: 0.9921 - acc: 0.6473\n",
      "Epoch 100/100\n",
      " - 13s - loss: 0.9542 - acc: 0.6784\n",
      ">3: train=0.068009 test=0.109073\n",
      "          train      test\n",
      "count  3.000000  3.000000\n",
      "mean   0.061488  0.102689\n",
      "std    0.036364  0.030017\n",
      "min    0.022305  0.069993\n",
      "25%    0.045157  0.089533\n",
      "50%    0.068009  0.109073\n",
      "75%    0.081080  0.119037\n",
      "max    0.094151  0.129000\n"
     ]
    }
   ],
   "source": [
    "verbose = 2\n",
    "n_epochs = 100\n",
    "n_photos_per_update = 2\n",
    "n_batches_per_epoch = int(len(train) / n_photos_per_update)\n",
    "n_repeats = 3\n",
    "\n",
    "# run experiment\n",
    "train_results, test_results = list(), list()\n",
    "for i in range(n_repeats):\n",
    "    # define the model\n",
    "    model = define_model(vocab_size, max_length)\n",
    "    # define checkpoint callback\n",
    "#     filepath = 'size_em_seq_model-model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "#     checkpoint = ModelCheckpoint(filepath, monitor='val_loss',\n",
    "#                                  verbose=1,\n",
    "#                                  save_best_only=True,\n",
    "#                                  mode='min')\n",
    "    # fit model\n",
    "    model.fit_generator(data_generator(train_descriptions, train_features, tokenizer, max_length, n_photos_per_update), \n",
    "                        steps_per_epoch=n_batches_per_epoch, \n",
    "                        epochs=n_epochs,\n",
    "                        verbose=verbose)\n",
    "    # evaluate model on training data\n",
    "    train_score = evaluate_model(model, train_descriptions, train_features, tokenizer, max_length)\n",
    "    test_score = evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)\n",
    "    # store\n",
    "    train_results.append(train_score)\n",
    "    test_results.append(test_score)\n",
    "    print('>%d: train=%f test=%f' % ((i+1), train_score, test_score))\n",
    "# save results to file\n",
    "df = DataFrame()\n",
    "df['train'] = train_results\n",
    "df['test'] = test_results\n",
    "print(df.describe())\n",
    "df.to_csv(model_name+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU scores table\n",
    "|       |   train  |    test|\n",
    "|-------|----------|--------|\n",
    "|count  |3.000000  |3.000000|\n",
    "|mean   |0.061488  |0.102689|\n",
    "|std    |0.036364  |0.030017|\n",
    "|min    |0.022305  |0.069993|\n",
    "|25%    |0.045157  |0.089533|\n",
    "|50%    |0.068009  |0.109073|\n",
    "|75%    |0.081080  |0.119037|\n",
    "|max    |0.094151  |0.129000|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of Language Model\n",
    "We can refer to the model that learns from the concatenated sequence and photo feature input as the language model. It is responsible for generating words.\n",
    "\n",
    "First, we can look at the impact on model skill by **descresing the LSTM and dense layers from 500 to 256 neurons**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "    # feature extractor (encoder)\n",
    "    inputs1 = Input(shape=(7, 7, 512))\n",
    "    fe1 = GlobalMaxPooling2D()(inputs1)\n",
    "    fe2 = Dense(128, activation='relu')(fe1)\n",
    "    fe3 = RepeatVector(max_length)(fe2)\n",
    "    # embedding\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    emb2 = Embedding(vocab_size, 50, mask_zero=True)(inputs2)\n",
    "    emb3 = LSTM(256, return_sequences=True)(emb2)\n",
    "    emb4 = TimeDistributed(Dense(128, activation='relu'))(emb3)\n",
    "    # merge inputs\n",
    "    merged = concatenate([fe3, emb4])\n",
    "    # language model (decoder)\n",
    "    lm2 = LSTM(256)(merged)\n",
    "    lm3 = Dense(256, activation='relu')(lm2)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(lm3)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_name = 'size_sm_lang_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 11s - loss: 7.2896 - acc: 0.0504\n",
      "Epoch 2/100\n",
      " - 9s - loss: 5.5627 - acc: 0.0586\n",
      "Epoch 3/100\n",
      " - 9s - loss: 5.4075 - acc: 0.0630\n",
      "Epoch 4/100\n",
      " - 9s - loss: 5.3386 - acc: 0.0630\n",
      "Epoch 5/100\n",
      " - 9s - loss: 5.3084 - acc: 0.0648\n",
      "Epoch 6/100\n",
      " - 9s - loss: 5.2449 - acc: 0.0739\n",
      "Epoch 7/100\n",
      " - 9s - loss: 5.1872 - acc: 0.0723\n",
      "Epoch 8/100\n",
      " - 9s - loss: 5.1436 - acc: 0.0755\n",
      "Epoch 9/100\n",
      " - 9s - loss: 5.1011 - acc: 0.0768\n",
      "Epoch 10/100\n",
      " - 9s - loss: 5.0744 - acc: 0.0702\n",
      "Epoch 11/100\n",
      " - 9s - loss: 5.0357 - acc: 0.0748\n",
      "Epoch 12/100\n",
      " - 9s - loss: 5.0087 - acc: 0.0792\n",
      "Epoch 13/100\n",
      " - 9s - loss: 4.9605 - acc: 0.0792\n",
      "Epoch 14/100\n",
      " - 9s - loss: 4.9315 - acc: 0.0817\n",
      "Epoch 15/100\n",
      " - 9s - loss: 4.9110 - acc: 0.0769\n",
      "Epoch 16/100\n",
      " - 9s - loss: 4.8957 - acc: 0.0825\n",
      "Epoch 17/100\n",
      " - 9s - loss: 4.8959 - acc: 0.0827\n",
      "Epoch 18/100\n",
      " - 9s - loss: 4.8502 - acc: 0.0849\n",
      "Epoch 19/100\n",
      " - 9s - loss: 4.8272 - acc: 0.0755\n",
      "Epoch 20/100\n",
      " - 9s - loss: 4.7982 - acc: 0.0833\n",
      "Epoch 21/100\n",
      " - 9s - loss: 4.8002 - acc: 0.0805\n",
      "Epoch 22/100\n",
      " - 9s - loss: 4.8053 - acc: 0.0878\n",
      "Epoch 23/100\n",
      " - 9s - loss: 4.7424 - acc: 0.0857\n",
      "Epoch 24/100\n",
      " - 9s - loss: 4.6803 - acc: 0.0856\n",
      "Epoch 25/100\n",
      " - 9s - loss: 4.6275 - acc: 0.0839\n",
      "Epoch 26/100\n",
      " - 9s - loss: 4.5430 - acc: 0.0917\n",
      "Epoch 27/100\n",
      " - 9s - loss: 4.4824 - acc: 0.0904\n",
      "Epoch 28/100\n",
      " - 9s - loss: 4.4148 - acc: 0.0882\n",
      "Epoch 29/100\n",
      " - 9s - loss: 4.3886 - acc: 0.0881\n",
      "Epoch 30/100\n",
      " - 9s - loss: 4.3262 - acc: 0.0993\n",
      "Epoch 31/100\n",
      " - 9s - loss: 4.2542 - acc: 0.0935\n",
      "Epoch 32/100\n",
      " - 9s - loss: 4.2036 - acc: 0.1010\n",
      "Epoch 33/100\n",
      " - 9s - loss: 4.1700 - acc: 0.1028\n",
      "Epoch 34/100\n",
      " - 9s - loss: 4.1277 - acc: 0.1030\n",
      "Epoch 35/100\n",
      " - 9s - loss: 4.0933 - acc: 0.1015\n",
      "Epoch 36/100\n",
      " - 9s - loss: 4.0525 - acc: 0.1016\n",
      "Epoch 37/100\n",
      " - 9s - loss: 3.9752 - acc: 0.1134\n",
      "Epoch 38/100\n",
      " - 9s - loss: 3.9615 - acc: 0.1075\n",
      "Epoch 39/100\n",
      " - 9s - loss: 4.0146 - acc: 0.1021\n",
      "Epoch 40/100\n",
      " - 9s - loss: 3.9122 - acc: 0.1091\n",
      "Epoch 41/100\n",
      " - 9s - loss: 3.8261 - acc: 0.1160\n",
      "Epoch 42/100\n",
      " - 9s - loss: 3.8083 - acc: 0.1074\n",
      "Epoch 43/100\n",
      " - 9s - loss: 3.7900 - acc: 0.1062\n",
      "Epoch 44/100\n",
      " - 9s - loss: 3.6999 - acc: 0.1289\n",
      "Epoch 45/100\n",
      " - 9s - loss: 3.7033 - acc: 0.1269\n",
      "Epoch 46/100\n",
      " - 9s - loss: 3.6269 - acc: 0.1222\n",
      "Epoch 47/100\n",
      " - 9s - loss: 3.5721 - acc: 0.1288\n",
      "Epoch 48/100\n",
      " - 9s - loss: 3.4922 - acc: 0.1438\n",
      "Epoch 49/100\n",
      " - 9s - loss: 3.4941 - acc: 0.1578\n",
      "Epoch 50/100\n",
      " - 9s - loss: 3.4452 - acc: 0.1416\n",
      "Epoch 51/100\n",
      " - 10s - loss: 3.3783 - acc: 0.1583\n",
      "Epoch 52/100\n",
      " - 11s - loss: 3.3670 - acc: 0.1507\n",
      "Epoch 53/100\n",
      " - 9s - loss: 3.3839 - acc: 0.1436\n",
      "Epoch 54/100\n",
      " - 9s - loss: 3.3246 - acc: 0.1580\n",
      "Epoch 55/100\n",
      " - 9s - loss: 3.3124 - acc: 0.1599\n",
      "Epoch 56/100\n",
      " - 9s - loss: 3.2592 - acc: 0.1627\n",
      "Epoch 57/100\n",
      " - 9s - loss: 3.0908 - acc: 0.1869\n",
      "Epoch 58/100\n",
      " - 9s - loss: 3.1087 - acc: 0.1816\n",
      "Epoch 59/100\n",
      " - 9s - loss: 3.0163 - acc: 0.1974\n",
      "Epoch 60/100\n",
      " - 9s - loss: 2.9329 - acc: 0.2004\n",
      "Epoch 61/100\n",
      " - 9s - loss: 2.9899 - acc: 0.2130\n",
      "Epoch 62/100\n",
      " - 9s - loss: 2.8259 - acc: 0.2466\n",
      "Epoch 63/100\n",
      " - 9s - loss: 2.7862 - acc: 0.2321\n",
      "Epoch 64/100\n",
      " - 9s - loss: 2.7506 - acc: 0.2300\n",
      "Epoch 65/100\n",
      " - 9s - loss: 2.7136 - acc: 0.2429\n",
      "Epoch 66/100\n",
      " - 9s - loss: 2.7410 - acc: 0.2283\n",
      "Epoch 67/100\n",
      " - 9s - loss: 2.6579 - acc: 0.2429\n",
      "Epoch 68/100\n",
      " - 9s - loss: 2.6182 - acc: 0.2499\n",
      "Epoch 69/100\n",
      " - 9s - loss: 2.4893 - acc: 0.2925\n",
      "Epoch 70/100\n",
      " - 9s - loss: 2.5110 - acc: 0.2838\n",
      "Epoch 71/100\n",
      " - 9s - loss: 2.5011 - acc: 0.2998\n",
      "Epoch 72/100\n",
      " - 9s - loss: 2.4323 - acc: 0.2860\n",
      "Epoch 73/100\n",
      " - 9s - loss: 2.3385 - acc: 0.3123\n",
      "Epoch 74/100\n",
      " - 9s - loss: 2.3632 - acc: 0.3052\n",
      "Epoch 75/100\n",
      " - 9s - loss: 2.2623 - acc: 0.3556\n",
      "Epoch 76/100\n",
      " - 9s - loss: 2.2101 - acc: 0.3419\n",
      "Epoch 77/100\n",
      " - 9s - loss: 2.1550 - acc: 0.3447\n",
      "Epoch 78/100\n",
      " - 9s - loss: 2.1772 - acc: 0.3413\n",
      "Epoch 79/100\n",
      " - 9s - loss: 2.0948 - acc: 0.3564\n",
      "Epoch 80/100\n",
      " - 9s - loss: 2.0836 - acc: 0.3517\n",
      "Epoch 81/100\n",
      " - 9s - loss: 2.0375 - acc: 0.3597\n",
      "Epoch 82/100\n",
      " - 9s - loss: 2.0301 - acc: 0.3562\n",
      "Epoch 83/100\n",
      " - 9s - loss: 1.9856 - acc: 0.3824\n",
      "Epoch 84/100\n",
      " - 9s - loss: 1.9705 - acc: 0.3628\n",
      "Epoch 85/100\n",
      " - 9s - loss: 2.0241 - acc: 0.3813\n",
      "Epoch 86/100\n",
      " - 9s - loss: 1.9975 - acc: 0.3560\n",
      "Epoch 87/100\n",
      " - 9s - loss: 1.8848 - acc: 0.3960\n",
      "Epoch 88/100\n",
      " - 9s - loss: 1.7544 - acc: 0.4275\n",
      "Epoch 89/100\n",
      " - 9s - loss: 1.7661 - acc: 0.4266\n",
      "Epoch 90/100\n",
      " - 9s - loss: 1.7620 - acc: 0.4232\n",
      "Epoch 91/100\n",
      " - 9s - loss: 1.8096 - acc: 0.4091\n",
      "Epoch 92/100\n",
      " - 9s - loss: 1.7330 - acc: 0.4375\n",
      "Epoch 93/100\n",
      " - 9s - loss: 1.6492 - acc: 0.4592\n",
      "Epoch 94/100\n",
      " - 9s - loss: 1.6412 - acc: 0.4512\n",
      "Epoch 95/100\n",
      " - 9s - loss: 1.5940 - acc: 0.4657\n",
      "Epoch 96/100\n",
      " - 9s - loss: 1.6141 - acc: 0.4710\n",
      "Epoch 97/100\n",
      " - 9s - loss: 1.6563 - acc: 0.4729\n",
      "Epoch 98/100\n",
      " - 9s - loss: 1.6938 - acc: 0.4392\n",
      "Epoch 99/100\n",
      " - 9s - loss: 1.7383 - acc: 0.4496\n",
      "Epoch 100/100\n",
      " - 9s - loss: 1.5716 - acc: 0.4742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shourya97/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1: train=0.034838 test=0.015892\n",
      "Epoch 1/100\n",
      " - 11s - loss: 7.3247 - acc: 0.0535\n",
      "Epoch 2/100\n",
      " - 9s - loss: 5.5615 - acc: 0.0542\n",
      "Epoch 3/100\n",
      " - 9s - loss: 5.4369 - acc: 0.0592\n",
      "Epoch 4/100\n",
      " - 9s - loss: 5.3769 - acc: 0.0630\n",
      "Epoch 5/100\n",
      " - 9s - loss: 5.3543 - acc: 0.0630\n",
      "Epoch 6/100\n",
      " - 9s - loss: 5.3297 - acc: 0.0630\n",
      "Epoch 7/100\n",
      " - 9s - loss: 5.3250 - acc: 0.0630\n",
      "Epoch 8/100\n",
      " - 9s - loss: 5.3107 - acc: 0.0630\n",
      "Epoch 9/100\n",
      " - 9s - loss: 5.2981 - acc: 0.0630\n",
      "Epoch 10/100\n",
      " - 9s - loss: 5.2916 - acc: 0.0630\n",
      "Epoch 11/100\n",
      " - 9s - loss: 5.2889 - acc: 0.0630\n",
      "Epoch 12/100\n",
      " - 9s - loss: 5.2871 - acc: 0.0630\n",
      "Epoch 13/100\n",
      " - 9s - loss: 5.3003 - acc: 0.0630\n",
      "Epoch 14/100\n",
      " - 9s - loss: 5.2936 - acc: 0.0630\n",
      "Epoch 15/100\n",
      " - 9s - loss: 5.2521 - acc: 0.0695\n",
      "Epoch 16/100\n",
      " - 9s - loss: 5.2202 - acc: 0.0657\n",
      "Epoch 17/100\n",
      " - 9s - loss: 5.2148 - acc: 0.0675\n",
      "Epoch 18/100\n",
      " - 9s - loss: 5.1589 - acc: 0.0695\n",
      "Epoch 19/100\n",
      " - 9s - loss: 5.1410 - acc: 0.0747\n",
      "Epoch 20/100\n",
      " - 9s - loss: 5.0970 - acc: 0.0750\n",
      "Epoch 21/100\n",
      " - 9s - loss: 5.0850 - acc: 0.0722\n",
      "Epoch 22/100\n",
      " - 9s - loss: 5.0385 - acc: 0.0741\n",
      "Epoch 23/100\n",
      " - 9s - loss: 5.0278 - acc: 0.0759\n",
      "Epoch 24/100\n",
      " - 9s - loss: 4.9801 - acc: 0.0782\n",
      "Epoch 25/100\n",
      " - 9s - loss: 4.9733 - acc: 0.0766\n",
      "Epoch 26/100\n",
      " - 9s - loss: 4.9383 - acc: 0.0812\n",
      "Epoch 27/100\n",
      " - 9s - loss: 4.9267 - acc: 0.0835\n",
      "Epoch 28/100\n",
      " - 9s - loss: 4.9267 - acc: 0.0819\n",
      "Epoch 29/100\n",
      " - 9s - loss: 4.9163 - acc: 0.0787\n",
      "Epoch 30/100\n",
      " - 9s - loss: 4.8826 - acc: 0.0819\n",
      "Epoch 31/100\n",
      " - 9s - loss: 4.9239 - acc: 0.0762\n",
      "Epoch 32/100\n",
      " - 9s - loss: 4.8806 - acc: 0.0839\n",
      "Epoch 33/100\n",
      " - 9s - loss: 4.8444 - acc: 0.0821\n",
      "Epoch 34/100\n",
      " - 9s - loss: 4.8237 - acc: 0.0832\n",
      "Epoch 35/100\n",
      " - 9s - loss: 4.8259 - acc: 0.0840\n",
      "Epoch 36/100\n",
      " - 9s - loss: 4.7865 - acc: 0.0819\n",
      "Epoch 37/100\n",
      " - 9s - loss: 4.7252 - acc: 0.0821\n",
      "Epoch 38/100\n",
      " - 9s - loss: 4.7104 - acc: 0.0808\n",
      "Epoch 39/100\n",
      " - 9s - loss: 4.6787 - acc: 0.0899\n",
      "Epoch 40/100\n",
      " - 9s - loss: 4.6353 - acc: 0.0853\n",
      "Epoch 41/100\n",
      " - 9s - loss: 4.6192 - acc: 0.0927\n",
      "Epoch 42/100\n",
      " - 9s - loss: 4.5932 - acc: 0.0883\n",
      "Epoch 43/100\n",
      " - 9s - loss: 4.5860 - acc: 0.0879\n",
      "Epoch 44/100\n",
      " - 9s - loss: 4.5675 - acc: 0.0990\n",
      "Epoch 45/100\n",
      " - 9s - loss: 4.5232 - acc: 0.0976\n",
      "Epoch 46/100\n",
      " - 9s - loss: 4.4919 - acc: 0.0937\n",
      "Epoch 47/100\n",
      " - 9s - loss: 4.4966 - acc: 0.0915\n",
      "Epoch 48/100\n",
      " - 9s - loss: 4.4068 - acc: 0.0987\n",
      "Epoch 49/100\n",
      " - 9s - loss: 4.4238 - acc: 0.0973\n",
      "Epoch 50/100\n",
      " - 9s - loss: 4.3941 - acc: 0.0935\n",
      "Epoch 51/100\n",
      " - 9s - loss: 4.3092 - acc: 0.1013\n",
      "Epoch 52/100\n",
      " - 9s - loss: 4.2675 - acc: 0.0921\n",
      "Epoch 53/100\n",
      " - 9s - loss: 4.2119 - acc: 0.1064\n",
      "Epoch 54/100\n",
      " - 9s - loss: 4.1820 - acc: 0.0973\n",
      "Epoch 55/100\n",
      " - 9s - loss: 4.2153 - acc: 0.0928\n",
      "Epoch 56/100\n",
      " - 9s - loss: 4.1523 - acc: 0.1043\n",
      "Epoch 57/100\n",
      " - 9s - loss: 4.1103 - acc: 0.1024\n",
      "Epoch 58/100\n",
      " - 9s - loss: 4.0684 - acc: 0.1116\n",
      "Epoch 59/100\n",
      " - 9s - loss: 4.0329 - acc: 0.1029\n",
      "Epoch 60/100\n",
      " - 9s - loss: 3.9995 - acc: 0.1103\n",
      "Epoch 61/100\n",
      " - 9s - loss: 3.9943 - acc: 0.1010\n",
      "Epoch 62/100\n",
      " - 9s - loss: 3.9666 - acc: 0.1102\n",
      "Epoch 63/100\n",
      " - 9s - loss: 3.9399 - acc: 0.1092\n",
      "Epoch 64/100\n",
      " - 9s - loss: 3.9269 - acc: 0.1001\n",
      "Epoch 65/100\n",
      " - 9s - loss: 3.8269 - acc: 0.1117\n",
      "Epoch 66/100\n",
      " - 9s - loss: 3.8124 - acc: 0.1104\n",
      "Epoch 67/100\n",
      " - 9s - loss: 3.7916 - acc: 0.1127\n",
      "Epoch 68/100\n",
      " - 9s - loss: 3.7371 - acc: 0.1162\n",
      "Epoch 69/100\n",
      " - 9s - loss: 3.7108 - acc: 0.1183\n",
      "Epoch 70/100\n",
      " - 9s - loss: 3.6679 - acc: 0.1246\n",
      "Epoch 71/100\n",
      " - 9s - loss: 3.5763 - acc: 0.1249\n",
      "Epoch 72/100\n",
      " - 9s - loss: 3.5649 - acc: 0.1313\n",
      "Epoch 73/100\n",
      " - 9s - loss: 3.5143 - acc: 0.1387\n",
      "Epoch 74/100\n",
      " - 9s - loss: 3.4932 - acc: 0.1300\n",
      "Epoch 75/100\n",
      " - 9s - loss: 3.4478 - acc: 0.1331\n",
      "Epoch 76/100\n",
      " - 9s - loss: 3.4243 - acc: 0.1503\n",
      "Epoch 77/100\n",
      " - 9s - loss: 3.3817 - acc: 0.1482\n",
      "Epoch 78/100\n",
      " - 9s - loss: 3.3155 - acc: 0.1640\n",
      "Epoch 79/100\n",
      " - 9s - loss: 3.3329 - acc: 0.1509\n",
      "Epoch 80/100\n",
      " - 9s - loss: 3.2552 - acc: 0.1626\n",
      "Epoch 81/100\n",
      " - 9s - loss: 3.2046 - acc: 0.1629\n",
      "Epoch 82/100\n",
      " - 9s - loss: 3.1795 - acc: 0.1672\n",
      "Epoch 83/100\n",
      " - 9s - loss: 3.1706 - acc: 0.1726\n",
      "Epoch 84/100\n",
      " - 9s - loss: 3.1588 - acc: 0.1742\n",
      "Epoch 85/100\n",
      " - 9s - loss: 3.1464 - acc: 0.1724\n",
      "Epoch 86/100\n",
      " - 9s - loss: 3.0314 - acc: 0.2026\n",
      "Epoch 87/100\n",
      " - 9s - loss: 2.9967 - acc: 0.1935\n",
      "Epoch 88/100\n",
      " - 9s - loss: 2.9653 - acc: 0.2196\n",
      "Epoch 89/100\n",
      " - 9s - loss: 2.9058 - acc: 0.2054\n",
      "Epoch 90/100\n",
      " - 9s - loss: 2.8461 - acc: 0.2017\n",
      "Epoch 91/100\n",
      " - 9s - loss: 2.7971 - acc: 0.2422\n",
      "Epoch 92/100\n",
      " - 9s - loss: 2.7641 - acc: 0.2450\n",
      "Epoch 93/100\n",
      " - 9s - loss: 2.6980 - acc: 0.2498\n",
      "Epoch 94/100\n",
      " - 9s - loss: 2.6865 - acc: 0.2559\n",
      "Epoch 95/100\n",
      " - 9s - loss: 2.6720 - acc: 0.2418\n",
      "Epoch 96/100\n",
      " - 9s - loss: 2.6878 - acc: 0.2419\n",
      "Epoch 97/100\n",
      " - 9s - loss: 2.5944 - acc: 0.2798\n",
      "Epoch 98/100\n",
      " - 9s - loss: 2.5672 - acc: 0.2968\n",
      "Epoch 99/100\n",
      " - 9s - loss: 2.5191 - acc: 0.2898\n",
      "Epoch 100/100\n",
      " - 9s - loss: 2.4798 - acc: 0.2976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shourya97/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2: train=0.005278 test=0.087785\n",
      "Epoch 1/100\n",
      " - 11s - loss: 7.2838 - acc: 0.0392\n",
      "Epoch 2/100\n",
      " - 9s - loss: 5.5564 - acc: 0.0641\n",
      "Epoch 3/100\n",
      " - 9s - loss: 5.4067 - acc: 0.0630\n",
      "Epoch 4/100\n",
      " - 9s - loss: 5.3500 - acc: 0.0630\n",
      "Epoch 5/100\n",
      " - 9s - loss: 5.3488 - acc: 0.0630\n",
      "Epoch 6/100\n",
      " - 9s - loss: 5.3072 - acc: 0.0668\n",
      "Epoch 7/100\n",
      " - 9s - loss: 5.2760 - acc: 0.0750\n",
      "Epoch 8/100\n",
      " - 9s - loss: 5.2203 - acc: 0.0700\n",
      "Epoch 9/100\n",
      " - 9s - loss: 5.1830 - acc: 0.0685\n",
      "Epoch 10/100\n",
      " - 9s - loss: 5.1416 - acc: 0.0714\n",
      "Epoch 11/100\n",
      " - 9s - loss: 5.1057 - acc: 0.0745\n",
      "Epoch 12/100\n",
      " - 9s - loss: 5.0907 - acc: 0.0749\n",
      "Epoch 13/100\n",
      " - 9s - loss: 5.0824 - acc: 0.0730\n",
      "Epoch 14/100\n",
      " - 9s - loss: 5.0615 - acc: 0.0765\n",
      "Epoch 15/100\n",
      " - 9s - loss: 5.0633 - acc: 0.0719\n",
      "Epoch 16/100\n",
      " - 9s - loss: 5.0166 - acc: 0.0791\n",
      "Epoch 17/100\n",
      " - 9s - loss: 5.0219 - acc: 0.0779\n",
      "Epoch 18/100\n",
      " - 9s - loss: 5.0444 - acc: 0.0758\n",
      "Epoch 19/100\n",
      " - 9s - loss: 4.9990 - acc: 0.0763\n",
      "Epoch 20/100\n",
      " - 9s - loss: 4.9692 - acc: 0.0749\n",
      "Epoch 21/100\n",
      " - 9s - loss: 4.9456 - acc: 0.0764\n",
      "Epoch 22/100\n",
      " - 9s - loss: 4.9194 - acc: 0.0813\n",
      "Epoch 23/100\n",
      " - 9s - loss: 4.9315 - acc: 0.0793\n",
      "Epoch 24/100\n",
      " - 9s - loss: 4.9271 - acc: 0.0818\n",
      "Epoch 25/100\n",
      " - 9s - loss: 4.9151 - acc: 0.0804\n",
      "Epoch 26/100\n",
      " - 9s - loss: 4.8812 - acc: 0.0834\n",
      "Epoch 27/100\n",
      " - 9s - loss: 4.8524 - acc: 0.0875\n",
      "Epoch 28/100\n",
      " - 9s - loss: 4.8189 - acc: 0.0858\n",
      "Epoch 29/100\n",
      " - 9s - loss: 4.8016 - acc: 0.0866\n",
      "Epoch 30/100\n",
      " - 9s - loss: 4.7718 - acc: 0.0874\n",
      "Epoch 31/100\n",
      " - 9s - loss: 4.7513 - acc: 0.0864\n",
      "Epoch 32/100\n",
      " - 9s - loss: 4.7429 - acc: 0.0840\n",
      "Epoch 33/100\n",
      " - 9s - loss: 4.7314 - acc: 0.0838\n",
      "Epoch 34/100\n",
      " - 9s - loss: 4.6466 - acc: 0.0870\n",
      "Epoch 35/100\n",
      " - 9s - loss: 4.6265 - acc: 0.0891\n",
      "Epoch 36/100\n",
      " - 9s - loss: 4.6103 - acc: 0.0846\n",
      "Epoch 37/100\n",
      " - 9s - loss: 4.5208 - acc: 0.0935\n",
      "Epoch 38/100\n",
      " - 9s - loss: 4.4991 - acc: 0.0943\n",
      "Epoch 39/100\n",
      " - 9s - loss: 4.5434 - acc: 0.0858\n",
      "Epoch 40/100\n",
      " - 9s - loss: 4.5392 - acc: 0.0866\n",
      "Epoch 41/100\n",
      " - 9s - loss: 4.4551 - acc: 0.0893\n",
      "Epoch 42/100\n",
      " - 9s - loss: 4.4341 - acc: 0.0962\n",
      "Epoch 43/100\n",
      " - 9s - loss: 4.4351 - acc: 0.0987\n",
      "Epoch 44/100\n",
      " - 9s - loss: 4.4038 - acc: 0.0964\n",
      "Epoch 45/100\n",
      " - 9s - loss: 4.3611 - acc: 0.0908\n",
      "Epoch 46/100\n",
      " - 9s - loss: 4.3239 - acc: 0.1030\n",
      "Epoch 47/100\n",
      " - 9s - loss: 4.2745 - acc: 0.0946\n",
      "Epoch 48/100\n",
      " - 9s - loss: 4.2351 - acc: 0.0944\n",
      "Epoch 49/100\n",
      " - 9s - loss: 4.2835 - acc: 0.0975\n",
      "Epoch 50/100\n",
      " - 9s - loss: 4.1918 - acc: 0.1072\n",
      "Epoch 51/100\n",
      " - 9s - loss: 4.1998 - acc: 0.1053\n",
      "Epoch 52/100\n",
      " - 9s - loss: 4.1679 - acc: 0.1110\n",
      "Epoch 53/100\n",
      " - 9s - loss: 4.1102 - acc: 0.1105\n",
      "Epoch 54/100\n",
      " - 9s - loss: 4.0937 - acc: 0.1026\n",
      "Epoch 55/100\n",
      " - 9s - loss: 4.0408 - acc: 0.1076\n",
      "Epoch 56/100\n",
      " - 9s - loss: 4.0507 - acc: 0.1065\n",
      "Epoch 57/100\n",
      " - 9s - loss: 4.0264 - acc: 0.1113\n",
      "Epoch 58/100\n",
      " - 9s - loss: 3.9721 - acc: 0.1125\n",
      "Epoch 59/100\n",
      " - 9s - loss: 4.0092 - acc: 0.1022\n",
      "Epoch 60/100\n",
      " - 9s - loss: 3.9545 - acc: 0.1044\n",
      "Epoch 61/100\n",
      " - 9s - loss: 3.9177 - acc: 0.1114\n",
      "Epoch 62/100\n",
      " - 9s - loss: 3.9524 - acc: 0.1185\n",
      "Epoch 63/100\n",
      " - 9s - loss: 3.9217 - acc: 0.1207\n",
      "Epoch 64/100\n",
      " - 9s - loss: 3.8715 - acc: 0.1037\n",
      "Epoch 65/100\n",
      " - 9s - loss: 3.8456 - acc: 0.1137\n",
      "Epoch 66/100\n",
      " - 9s - loss: 3.7861 - acc: 0.1115\n",
      "Epoch 67/100\n",
      " - 9s - loss: 3.7859 - acc: 0.1086\n",
      "Epoch 68/100\n",
      " - 9s - loss: 3.8358 - acc: 0.1102\n",
      "Epoch 69/100\n",
      " - 9s - loss: 3.7304 - acc: 0.1137\n",
      "Epoch 70/100\n",
      " - 9s - loss: 3.7457 - acc: 0.1068\n",
      "Epoch 71/100\n",
      " - 9s - loss: 3.7355 - acc: 0.1171\n",
      "Epoch 72/100\n",
      " - 9s - loss: 3.7612 - acc: 0.1031\n",
      "Epoch 73/100\n",
      " - 9s - loss: 3.6724 - acc: 0.1086\n",
      "Epoch 74/100\n",
      " - 9s - loss: 3.6089 - acc: 0.1147\n",
      "Epoch 75/100\n",
      " - 9s - loss: 3.5682 - acc: 0.1282\n",
      "Epoch 76/100\n",
      " - 9s - loss: 3.5170 - acc: 0.1300\n",
      "Epoch 77/100\n",
      " - 9s - loss: 3.4923 - acc: 0.1276\n",
      "Epoch 78/100\n",
      " - 9s - loss: 3.5810 - acc: 0.1168\n",
      "Epoch 79/100\n",
      " - 9s - loss: 3.5063 - acc: 0.1201\n",
      "Epoch 80/100\n",
      " - 9s - loss: 3.4546 - acc: 0.1419\n",
      "Epoch 81/100\n",
      " - 9s - loss: 3.4352 - acc: 0.1329\n",
      "Epoch 82/100\n",
      " - 9s - loss: 3.3816 - acc: 0.1314\n",
      "Epoch 83/100\n",
      " - 9s - loss: 3.3468 - acc: 0.1349\n",
      "Epoch 84/100\n",
      " - 9s - loss: 3.3233 - acc: 0.1485\n",
      "Epoch 85/100\n",
      " - 9s - loss: 3.2756 - acc: 0.1494\n",
      "Epoch 86/100\n",
      " - 9s - loss: 3.2249 - acc: 0.1440\n",
      "Epoch 87/100\n",
      " - 9s - loss: 3.2000 - acc: 0.1550\n",
      "Epoch 88/100\n",
      " - 9s - loss: 3.1904 - acc: 0.1484\n",
      "Epoch 89/100\n",
      " - 9s - loss: 3.1835 - acc: 0.1572\n",
      "Epoch 90/100\n",
      " - 9s - loss: 3.1438 - acc: 0.1754\n",
      "Epoch 91/100\n",
      " - 9s - loss: 3.0911 - acc: 0.1754\n",
      "Epoch 92/100\n",
      " - 9s - loss: 3.0298 - acc: 0.1954\n",
      "Epoch 93/100\n",
      " - 9s - loss: 2.9507 - acc: 0.1833\n",
      "Epoch 94/100\n",
      " - 9s - loss: 3.0249 - acc: 0.1786\n",
      "Epoch 95/100\n",
      " - 9s - loss: 3.1036 - acc: 0.1768\n",
      "Epoch 96/100\n",
      " - 9s - loss: 3.0120 - acc: 0.1943\n",
      "Epoch 97/100\n",
      " - 9s - loss: 3.0444 - acc: 0.1781\n",
      "Epoch 98/100\n",
      " - 9s - loss: 3.0538 - acc: 0.1755\n",
      "Epoch 99/100\n",
      " - 9s - loss: 2.9653 - acc: 0.1868\n",
      "Epoch 100/100\n",
      " - 9s - loss: 2.9236 - acc: 0.2116\n",
      ">3: train=0.057480 test=0.096642\n",
      "          train      test\n",
      "count  3.000000  3.000000\n",
      "mean   0.032532  0.066773\n",
      "std    0.026177  0.044286\n",
      "min    0.005278  0.015892\n",
      "25%    0.020058  0.051838\n",
      "50%    0.034838  0.087785\n",
      "75%    0.046159  0.092213\n",
      "max    0.057480  0.096642\n"
     ]
    }
   ],
   "source": [
    "verbose = 2\n",
    "n_epochs = 100\n",
    "n_photos_per_update = 2\n",
    "n_batches_per_epoch = int(len(train) / n_photos_per_update)\n",
    "n_repeats = 3\n",
    "\n",
    "# run experiment\n",
    "train_results, test_results = list(), list()\n",
    "for i in range(n_repeats):\n",
    "    # define the model\n",
    "    model = define_model(vocab_size, max_length)\n",
    "    # define checkpoint callback\n",
    "#     filepath = 'size_sm_lang_model-model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "#     checkpoint = ModelCheckpoint(filepath, monitor='val_loss',\n",
    "#                                  verbose=1,\n",
    "#                                  save_best_only=True,\n",
    "#                                  mode='min')\n",
    "    # fit model\n",
    "    model.fit_generator(data_generator(train_descriptions, train_features, tokenizer, max_length, n_photos_per_update), \n",
    "                        steps_per_epoch=n_batches_per_epoch, \n",
    "                        epochs=n_epochs,\n",
    "                        verbose=verbose)\n",
    "    # evaluate model on training data\n",
    "    train_score = evaluate_model(model, train_descriptions, train_features, tokenizer, max_length)\n",
    "    test_score = evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)\n",
    "    # store\n",
    "    train_results.append(train_score)\n",
    "    test_results.append(test_score)\n",
    "    print('>%d: train=%f test=%f' % ((i+1), train_score, test_score))\n",
    "# save results to file\n",
    "df = DataFrame()\n",
    "df['train'] = train_results\n",
    "df['test'] = test_results\n",
    "print(df.describe())\n",
    "df.to_csv(model_name+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU scores table\n",
    "|       |  train   |   test  |\n",
    "|-------|----------|--------|\n",
    "|count  |3.000000  |3.000000|\n",
    "|mean   |0.032532  |0.066773|\n",
    "|std    |0.026177  |0.044286|\n",
    "|min    |0.005278  |0.015892|\n",
    "|25%    |0.020058  |0.051838|\n",
    "|50%    |0.034838  |0.087785|\n",
    "|75%    |0.046159  |0.092213|\n",
    "|max    |0.057480  |0.096642|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the impact of **doubling the capacity of the language model by adding a second LSTM layer of the same size.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "    # feature extractor (encoder)\n",
    "    inputs1 = Input(shape=(7, 7, 512))\n",
    "    fe1 = GlobalMaxPooling2D()(inputs1)\n",
    "    fe2 = Dense(128, activation='relu')(fe1)\n",
    "    fe3 = RepeatVector(max_length)(fe2)\n",
    "    # embedding\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    emb2 = Embedding(vocab_size, 50, mask_zero=True)(inputs2)\n",
    "    emb3 = LSTM(256, return_sequences=True)(emb2)\n",
    "    emb4 = TimeDistributed(Dense(128, activation='relu'))(emb3)\n",
    "    # merge inputs\n",
    "    merged = concatenate([fe3, emb4])\n",
    "    # language model (decoder)\n",
    "    lm2 = LSTM(500, return_sequences=True)(merged)\n",
    "    lm3 = LSTM(500)(lm2)\n",
    "    lm4 = Dense(500, activation='relu')(lm3)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(lm4)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_name = 'size_lg_lang_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 27s - loss: 7.1036 - acc: 0.0523\n",
      "Epoch 2/100\n",
      " - 22s - loss: 5.5664 - acc: 0.0594\n",
      "Epoch 3/100\n",
      " - 22s - loss: 5.3466 - acc: 0.0596\n",
      "Epoch 4/100\n",
      " - 22s - loss: 5.2196 - acc: 0.0585\n",
      "Epoch 5/100\n",
      " - 22s - loss: 5.1373 - acc: 0.0563\n",
      "Epoch 6/100\n",
      " - 22s - loss: 5.0831 - acc: 0.0625\n",
      "Epoch 7/100\n",
      " - 22s - loss: 5.0200 - acc: 0.0576\n",
      "Epoch 8/100\n",
      " - 22s - loss: 4.9783 - acc: 0.0607\n",
      "Epoch 9/100\n",
      " - 22s - loss: 4.9392 - acc: 0.0607\n",
      "Epoch 10/100\n",
      " - 22s - loss: 4.9088 - acc: 0.0592\n",
      "Epoch 11/100\n",
      " - 22s - loss: 4.8464 - acc: 0.0587\n",
      "Epoch 12/100\n",
      " - 22s - loss: 4.8244 - acc: 0.0587\n",
      "Epoch 13/100\n",
      " - 22s - loss: 4.7956 - acc: 0.0568\n",
      "Epoch 14/100\n",
      " - 22s - loss: 4.7892 - acc: 0.0604\n",
      "Epoch 15/100\n",
      " - 22s - loss: 4.7626 - acc: 0.0569\n",
      "Epoch 16/100\n",
      " - 22s - loss: 4.7221 - acc: 0.0642\n",
      "Epoch 17/100\n",
      " - 22s - loss: 4.7056 - acc: 0.0639\n",
      "Epoch 18/100\n",
      " - 22s - loss: 4.6511 - acc: 0.0656\n",
      "Epoch 19/100\n",
      " - 22s - loss: 4.6395 - acc: 0.0692\n",
      "Epoch 20/100\n",
      " - 22s - loss: 4.6144 - acc: 0.0670\n",
      "Epoch 21/100\n",
      " - 22s - loss: 4.5829 - acc: 0.0679\n",
      "Epoch 22/100\n",
      " - 22s - loss: 4.5577 - acc: 0.0681\n",
      "Epoch 23/100\n",
      " - 22s - loss: 4.5279 - acc: 0.0689\n",
      "Epoch 24/100\n",
      " - 22s - loss: 4.5112 - acc: 0.0639\n",
      "Epoch 25/100\n",
      " - 22s - loss: 4.4915 - acc: 0.0691\n",
      "Epoch 26/100\n",
      " - 22s - loss: 4.4598 - acc: 0.0694\n",
      "Epoch 27/100\n",
      " - 22s - loss: 4.4318 - acc: 0.0678\n",
      "Epoch 28/100\n",
      " - 22s - loss: 4.4072 - acc: 0.0641\n",
      "Epoch 29/100\n",
      " - 22s - loss: 4.3854 - acc: 0.0625\n",
      "Epoch 30/100\n",
      " - 22s - loss: 4.3670 - acc: 0.0614\n",
      "Epoch 31/100\n",
      " - 22s - loss: 4.3520 - acc: 0.0650\n",
      "Epoch 32/100\n",
      " - 22s - loss: 4.3390 - acc: 0.0643\n",
      "Epoch 33/100\n",
      " - 22s - loss: 4.3178 - acc: 0.0658\n",
      "Epoch 34/100\n",
      " - 22s - loss: 4.3283 - acc: 0.0619\n",
      "Epoch 35/100\n",
      " - 22s - loss: 4.3025 - acc: 0.0605\n",
      "Epoch 36/100\n",
      " - 22s - loss: 4.2806 - acc: 0.0628\n",
      "Epoch 37/100\n",
      " - 22s - loss: 4.2680 - acc: 0.0627\n",
      "Epoch 38/100\n",
      " - 22s - loss: 4.2575 - acc: 0.0615\n",
      "Epoch 39/100\n",
      " - 22s - loss: 4.2461 - acc: 0.0624\n",
      "Epoch 40/100\n",
      " - 22s - loss: 4.2356 - acc: 0.0624\n",
      "Epoch 41/100\n",
      " - 22s - loss: 4.2257 - acc: 0.0625\n",
      "Epoch 42/100\n",
      " - 22s - loss: 4.2159 - acc: 0.0625\n",
      "Epoch 43/100\n",
      " - 22s - loss: 4.2057 - acc: 0.0633\n",
      "Epoch 44/100\n",
      " - 22s - loss: 4.1963 - acc: 0.0650\n",
      "Epoch 45/100\n",
      " - 22s - loss: 4.1865 - acc: 0.0640\n",
      "Epoch 46/100\n",
      " - 22s - loss: 4.1756 - acc: 0.0640\n",
      "Epoch 47/100\n",
      " - 22s - loss: 4.1663 - acc: 0.0640\n",
      "Epoch 48/100\n",
      " - 22s - loss: 4.1581 - acc: 0.0630\n",
      "Epoch 49/100\n",
      " - 22s - loss: 4.1482 - acc: 0.0622\n",
      "Epoch 50/100\n",
      " - 22s - loss: 4.1399 - acc: 0.0620\n",
      "Epoch 51/100\n",
      " - 22s - loss: 4.1326 - acc: 0.0636\n",
      "Epoch 52/100\n",
      " - 22s - loss: 4.1254 - acc: 0.0639\n",
      "Epoch 53/100\n",
      " - 22s - loss: 4.1196 - acc: 0.0657\n",
      "Epoch 54/100\n",
      " - 22s - loss: 4.1144 - acc: 0.0629\n",
      "Epoch 55/100\n",
      " - 22s - loss: 4.1067 - acc: 0.0649\n",
      "Epoch 56/100\n",
      " - 22s - loss: 4.1028 - acc: 0.0649\n",
      "Epoch 57/100\n",
      " - 22s - loss: 4.0960 - acc: 0.0618\n",
      "Epoch 58/100\n",
      " - 22s - loss: 4.0912 - acc: 0.0602\n",
      "Epoch 59/100\n",
      " - 22s - loss: 4.0881 - acc: 0.0609\n",
      "Epoch 60/100\n",
      " - 22s - loss: 4.0806 - acc: 0.0611\n",
      "Epoch 61/100\n",
      " - 22s - loss: 4.0748 - acc: 0.0644\n",
      "Epoch 62/100\n",
      " - 22s - loss: 4.0706 - acc: 0.0636\n",
      "Epoch 63/100\n",
      " - 22s - loss: 4.0659 - acc: 0.0646\n",
      "Epoch 64/100\n",
      " - 22s - loss: 4.0615 - acc: 0.0644\n",
      "Epoch 65/100\n",
      " - 22s - loss: 4.0575 - acc: 0.0633\n",
      "Epoch 66/100\n",
      " - 22s - loss: 4.0543 - acc: 0.0652\n",
      "Epoch 67/100\n",
      " - 22s - loss: 4.0489 - acc: 0.0692\n",
      "Epoch 68/100\n",
      " - 22s - loss: 4.0438 - acc: 0.0703\n",
      "Epoch 69/100\n",
      " - 22s - loss: 4.0410 - acc: 0.0657\n",
      "Epoch 70/100\n",
      " - 23s - loss: 4.0355 - acc: 0.0676\n",
      "Epoch 71/100\n",
      " - 23s - loss: 4.0376 - acc: 0.0663\n",
      "Epoch 72/100\n",
      " - 23s - loss: 4.0296 - acc: 0.0659\n",
      "Epoch 73/100\n",
      " - 22s - loss: 4.0224 - acc: 0.0682\n",
      "Epoch 74/100\n",
      " - 22s - loss: 4.0172 - acc: 0.0659\n",
      "Epoch 75/100\n",
      " - 22s - loss: 4.0266 - acc: 0.0669\n",
      "Epoch 76/100\n",
      " - 22s - loss: 4.0344 - acc: 0.0640\n",
      "Epoch 77/100\n",
      " - 22s - loss: 4.0124 - acc: 0.0639\n",
      "Epoch 78/100\n",
      " - 22s - loss: 4.0029 - acc: 0.0698\n",
      "Epoch 79/100\n",
      " - 22s - loss: 3.9966 - acc: 0.0710\n",
      "Epoch 80/100\n",
      " - 22s - loss: 3.9959 - acc: 0.0684\n",
      "Epoch 81/100\n",
      " - 22s - loss: 3.9949 - acc: 0.0673\n",
      "Epoch 82/100\n",
      " - 23s - loss: 3.9906 - acc: 0.0635\n",
      "Epoch 83/100\n",
      " - 22s - loss: 3.9911 - acc: 0.0670\n",
      "Epoch 84/100\n",
      " - 22s - loss: 4.0362 - acc: 0.0706\n",
      "Epoch 85/100\n",
      " - 22s - loss: 4.0050 - acc: 0.0723\n",
      "Epoch 86/100\n",
      " - 22s - loss: 3.9876 - acc: 0.0679\n",
      "Epoch 87/100\n",
      " - 22s - loss: 3.9770 - acc: 0.0727\n",
      "Epoch 88/100\n",
      " - 23s - loss: 3.9813 - acc: 0.0661\n",
      "Epoch 89/100\n",
      " - 22s - loss: 3.9788 - acc: 0.0719\n",
      "Epoch 90/100\n",
      " - 22s - loss: 3.9626 - acc: 0.0698\n",
      "Epoch 91/100\n",
      " - 22s - loss: 3.9521 - acc: 0.0717\n",
      "Epoch 92/100\n",
      " - 22s - loss: 3.9458 - acc: 0.0666\n",
      "Epoch 93/100\n",
      " - 22s - loss: 3.9404 - acc: 0.0654\n",
      "Epoch 94/100\n",
      " - 22s - loss: 3.9398 - acc: 0.0708\n",
      "Epoch 95/100\n",
      " - 23s - loss: 3.9256 - acc: 0.0680\n",
      "Epoch 96/100\n",
      " - 22s - loss: 3.9213 - acc: 0.0713\n",
      "Epoch 97/100\n",
      " - 22s - loss: 3.9121 - acc: 0.0669\n",
      "Epoch 98/100\n",
      " - 22s - loss: 3.9036 - acc: 0.0689\n",
      "Epoch 99/100\n",
      " - 22s - loss: 3.8940 - acc: 0.0725\n",
      "Epoch 100/100\n",
      " - 22s - loss: 3.8919 - acc: 0.0764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shourya97/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1: train=0.006395 test=0.082561\n",
      "Epoch 1/100\n",
      " - 25s - loss: 7.0994 - acc: 0.0507\n",
      "Epoch 2/100\n",
      " - 22s - loss: 5.5491 - acc: 0.0592\n",
      "Epoch 3/100\n",
      " - 22s - loss: 5.3447 - acc: 0.0613\n",
      "Epoch 4/100\n",
      " - 23s - loss: 5.2161 - acc: 0.0633\n",
      "Epoch 5/100\n",
      " - 24s - loss: 5.1559 - acc: 0.0643\n",
      "Epoch 6/100\n",
      " - 23s - loss: 5.1039 - acc: 0.0634\n",
      "Epoch 7/100\n",
      " - 23s - loss: 5.0431 - acc: 0.0569\n",
      "Epoch 8/100\n",
      " - 23s - loss: 4.9871 - acc: 0.0601\n",
      "Epoch 9/100\n",
      " - 23s - loss: 4.9638 - acc: 0.0496\n",
      "Epoch 10/100\n",
      " - 23s - loss: 4.9094 - acc: 0.0534\n",
      "Epoch 11/100\n",
      " - 22s - loss: 4.8791 - acc: 0.0647\n",
      "Epoch 12/100\n",
      " - 22s - loss: 4.8507 - acc: 0.0686\n",
      "Epoch 13/100\n",
      " - 23s - loss: 4.8157 - acc: 0.0658\n",
      "Epoch 14/100\n",
      " - 22s - loss: 4.7694 - acc: 0.0687\n",
      "Epoch 15/100\n",
      " - 22s - loss: 4.7507 - acc: 0.0678\n",
      "Epoch 16/100\n",
      " - 22s - loss: 4.7132 - acc: 0.0684\n",
      "Epoch 17/100\n",
      " - 22s - loss: 4.6336 - acc: 0.0725\n",
      "Epoch 18/100\n",
      " - 22s - loss: 4.6166 - acc: 0.0767\n",
      "Epoch 19/100\n",
      " - 22s - loss: 4.5998 - acc: 0.0766\n",
      "Epoch 20/100\n",
      " - 22s - loss: 4.5367 - acc: 0.0836\n",
      "Epoch 21/100\n",
      " - 22s - loss: 4.5003 - acc: 0.0842\n",
      "Epoch 22/100\n",
      " - 22s - loss: 4.5015 - acc: 0.0766\n",
      "Epoch 23/100\n",
      " - 22s - loss: 4.4454 - acc: 0.0942\n",
      "Epoch 24/100\n",
      " - 22s - loss: 4.3964 - acc: 0.0906\n",
      "Epoch 25/100\n",
      " - 22s - loss: 4.4363 - acc: 0.0864\n",
      "Epoch 26/100\n",
      " - 22s - loss: 4.3309 - acc: 0.0812\n",
      "Epoch 27/100\n",
      " - 22s - loss: 4.2854 - acc: 0.0900\n",
      "Epoch 28/100\n",
      " - 22s - loss: 4.2444 - acc: 0.0945\n",
      "Epoch 29/100\n",
      " - 22s - loss: 4.2004 - acc: 0.0926\n",
      "Epoch 30/100\n",
      " - 22s - loss: 4.1814 - acc: 0.0873\n",
      "Epoch 31/100\n",
      " - 22s - loss: 4.1316 - acc: 0.0896\n",
      "Epoch 32/100\n",
      " - 22s - loss: 4.1150 - acc: 0.0860\n",
      "Epoch 33/100\n",
      " - 22s - loss: 4.0727 - acc: 0.0824\n",
      "Epoch 34/100\n",
      " - 22s - loss: 4.0128 - acc: 0.0881\n",
      "Epoch 35/100\n",
      " - 22s - loss: 3.9832 - acc: 0.0784\n",
      "Epoch 36/100\n",
      " - 22s - loss: 3.9281 - acc: 0.0894\n",
      "Epoch 37/100\n",
      " - 22s - loss: 3.8889 - acc: 0.0860\n",
      "Epoch 38/100\n",
      " - 22s - loss: 3.8739 - acc: 0.0887\n",
      "Epoch 39/100\n",
      " - 22s - loss: 3.8282 - acc: 0.0941\n",
      "Epoch 40/100\n",
      " - 22s - loss: 3.7933 - acc: 0.0921\n",
      "Epoch 41/100\n",
      " - 22s - loss: 3.7984 - acc: 0.0781\n",
      "Epoch 42/100\n",
      " - 22s - loss: 3.8599 - acc: 0.0879\n",
      "Epoch 43/100\n",
      " - 22s - loss: 3.7989 - acc: 0.0956\n",
      "Epoch 44/100\n",
      " - 22s - loss: 3.7502 - acc: 0.1003\n",
      "Epoch 45/100\n",
      " - 22s - loss: 3.7353 - acc: 0.0871\n",
      "Epoch 46/100\n",
      " - 22s - loss: 3.6778 - acc: 0.0814\n",
      "Epoch 47/100\n",
      " - 22s - loss: 3.6357 - acc: 0.0936\n",
      "Epoch 48/100\n",
      " - 22s - loss: 3.6778 - acc: 0.0895\n",
      "Epoch 49/100\n",
      " - 22s - loss: 3.6146 - acc: 0.0954\n",
      "Epoch 50/100\n",
      " - 22s - loss: 3.5543 - acc: 0.1014\n",
      "Epoch 51/100\n",
      " - 22s - loss: 3.5410 - acc: 0.1017\n",
      "Epoch 52/100\n",
      " - 22s - loss: 3.6447 - acc: 0.0954\n",
      "Epoch 53/100\n",
      " - 22s - loss: 3.5765 - acc: 0.0987\n",
      "Epoch 54/100\n",
      " - 22s - loss: 3.5559 - acc: 0.0990\n",
      "Epoch 55/100\n",
      " - 22s - loss: 3.5234 - acc: 0.0914\n",
      "Epoch 56/100\n",
      " - 23s - loss: 3.4956 - acc: 0.1010\n",
      "Epoch 57/100\n",
      " - 22s - loss: 3.4245 - acc: 0.1043\n",
      "Epoch 58/100\n",
      " - 22s - loss: 3.3931 - acc: 0.1023\n",
      "Epoch 59/100\n",
      " - 22s - loss: 3.3614 - acc: 0.1078\n",
      "Epoch 60/100\n",
      " - 22s - loss: 3.3394 - acc: 0.1139\n",
      "Epoch 61/100\n",
      " - 22s - loss: 3.3216 - acc: 0.1109\n",
      "Epoch 62/100\n",
      " - 22s - loss: 3.2905 - acc: 0.1114\n",
      "Epoch 63/100\n",
      " - 22s - loss: 3.2693 - acc: 0.1041\n",
      "Epoch 64/100\n",
      " - 22s - loss: 3.2847 - acc: 0.1042\n",
      "Epoch 65/100\n",
      " - 22s - loss: 3.2981 - acc: 0.1021\n",
      "Epoch 66/100\n",
      " - 22s - loss: 3.2457 - acc: 0.1165\n",
      "Epoch 67/100\n",
      " - 22s - loss: 3.2004 - acc: 0.1136\n",
      "Epoch 68/100\n",
      " - 22s - loss: 3.2115 - acc: 0.1139\n",
      "Epoch 69/100\n",
      " - 22s - loss: 3.2045 - acc: 0.1059\n",
      "Epoch 70/100\n",
      " - 22s - loss: 3.2181 - acc: 0.0987\n",
      "Epoch 71/100\n",
      " - 22s - loss: 3.2467 - acc: 0.1044\n",
      "Epoch 72/100\n",
      " - 22s - loss: 3.2858 - acc: 0.1080\n",
      "Epoch 73/100\n",
      " - 22s - loss: 3.2691 - acc: 0.0988\n",
      "Epoch 74/100\n",
      " - 22s - loss: 3.1863 - acc: 0.1158\n",
      "Epoch 75/100\n",
      " - 22s - loss: 3.1384 - acc: 0.1165\n",
      "Epoch 76/100\n",
      " - 22s - loss: 3.1104 - acc: 0.1237\n",
      "Epoch 77/100\n",
      " - 22s - loss: 3.2943 - acc: 0.1033\n",
      "Epoch 78/100\n",
      " - 22s - loss: 3.1665 - acc: 0.1111\n",
      "Epoch 79/100\n",
      " - 22s - loss: 3.1678 - acc: 0.1090\n",
      "Epoch 80/100\n",
      " - 22s - loss: 3.0533 - acc: 0.1172\n",
      "Epoch 81/100\n",
      " - 22s - loss: 3.0525 - acc: 0.1205\n",
      "Epoch 82/100\n",
      " - 23s - loss: 3.0668 - acc: 0.1231\n",
      "Epoch 83/100\n",
      " - 22s - loss: 3.0548 - acc: 0.1162\n",
      "Epoch 84/100\n",
      " - 22s - loss: 3.0398 - acc: 0.1199\n",
      "Epoch 85/100\n",
      " - 22s - loss: 3.0348 - acc: 0.1290\n",
      "Epoch 86/100\n",
      " - 22s - loss: 3.0373 - acc: 0.1138\n",
      "Epoch 87/100\n",
      " - 22s - loss: 3.1240 - acc: 0.1194\n",
      "Epoch 88/100\n",
      " - 22s - loss: 3.0377 - acc: 0.1152\n",
      "Epoch 89/100\n",
      " - 22s - loss: 2.9874 - acc: 0.1293\n",
      "Epoch 90/100\n",
      " - 22s - loss: 2.9574 - acc: 0.1161\n",
      "Epoch 91/100\n",
      " - 22s - loss: 2.9560 - acc: 0.1156\n",
      "Epoch 92/100\n",
      " - 22s - loss: 2.9054 - acc: 0.1378\n",
      "Epoch 93/100\n",
      " - 22s - loss: 2.9365 - acc: 0.1147\n",
      "Epoch 94/100\n",
      " - 22s - loss: 2.9065 - acc: 0.1281\n",
      "Epoch 95/100\n",
      " - 22s - loss: 2.9739 - acc: 0.1208\n",
      "Epoch 96/100\n",
      " - 22s - loss: 2.9191 - acc: 0.1236\n",
      "Epoch 97/100\n",
      " - 22s - loss: 2.9906 - acc: 0.1190\n",
      "Epoch 98/100\n",
      " - 22s - loss: 2.8935 - acc: 0.1173\n",
      "Epoch 99/100\n",
      " - 22s - loss: 2.8589 - acc: 0.1315\n",
      "Epoch 100/100\n",
      " - 22s - loss: 2.8652 - acc: 0.1314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shourya97/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2: train=0.008303 test=0.020878\n",
      "Epoch 1/100\n",
      " - 26s - loss: 7.0904 - acc: 0.0469\n",
      "Epoch 2/100\n",
      " - 22s - loss: 5.5449 - acc: 0.0604\n",
      "Epoch 3/100\n",
      " - 22s - loss: 5.3419 - acc: 0.0635\n",
      "Epoch 4/100\n",
      " - 22s - loss: 5.2271 - acc: 0.0604\n",
      "Epoch 5/100\n",
      " - 22s - loss: 5.1848 - acc: 0.0645\n",
      "Epoch 6/100\n",
      " - 22s - loss: 5.1194 - acc: 0.0617\n",
      "Epoch 7/100\n",
      " - 23s - loss: 5.0406 - acc: 0.0700\n",
      "Epoch 8/100\n",
      " - 22s - loss: 4.9832 - acc: 0.0700\n",
      "Epoch 9/100\n",
      " - 23s - loss: 4.9128 - acc: 0.0702\n",
      "Epoch 10/100\n",
      " - 22s - loss: 4.8740 - acc: 0.0705\n",
      "Epoch 11/100\n",
      " - 22s - loss: 4.8118 - acc: 0.0760\n",
      "Epoch 12/100\n",
      " - 22s - loss: 4.7653 - acc: 0.0763\n",
      "Epoch 13/100\n",
      " - 22s - loss: 4.7224 - acc: 0.0812\n",
      "Epoch 14/100\n",
      " - 22s - loss: 4.6553 - acc: 0.0872\n",
      "Epoch 15/100\n",
      " - 22s - loss: 4.5917 - acc: 0.0874\n",
      "Epoch 16/100\n",
      " - 22s - loss: 4.5311 - acc: 0.0827\n",
      "Epoch 17/100\n",
      " - 22s - loss: 4.4942 - acc: 0.0833\n",
      "Epoch 18/100\n",
      " - 22s - loss: 4.4564 - acc: 0.0789\n",
      "Epoch 19/100\n",
      " - 22s - loss: 4.4317 - acc: 0.0804\n",
      "Epoch 20/100\n",
      " - 22s - loss: 4.5526 - acc: 0.0664\n",
      "Epoch 21/100\n",
      " - 22s - loss: 4.5466 - acc: 0.0832\n",
      "Epoch 22/100\n",
      " - 22s - loss: 4.6220 - acc: 0.0821\n",
      "Epoch 23/100\n",
      " - 23s - loss: 4.6701 - acc: 0.0846\n",
      "Epoch 24/100\n",
      " - 22s - loss: 4.6933 - acc: 0.0874\n",
      "Epoch 25/100\n",
      " - 22s - loss: 4.5323 - acc: 0.0891\n",
      "Epoch 26/100\n",
      " - 22s - loss: 4.4307 - acc: 0.0888\n",
      "Epoch 27/100\n",
      " - 22s - loss: 4.3333 - acc: 0.0948\n",
      "Epoch 28/100\n",
      " - 22s - loss: 4.3200 - acc: 0.0890\n",
      "Epoch 29/100\n",
      " - 22s - loss: 4.2599 - acc: 0.0913\n",
      "Epoch 30/100\n",
      " - 22s - loss: 4.1919 - acc: 0.0930\n",
      "Epoch 31/100\n",
      " - 23s - loss: 4.2352 - acc: 0.0894\n",
      "Epoch 32/100\n",
      " - 22s - loss: 4.1743 - acc: 0.0859\n",
      "Epoch 33/100\n",
      " - 22s - loss: 4.0881 - acc: 0.0898\n",
      "Epoch 34/100\n",
      " - 22s - loss: 4.0306 - acc: 0.0977\n",
      "Epoch 35/100\n",
      " - 22s - loss: 4.0423 - acc: 0.0849\n",
      "Epoch 36/100\n",
      " - 22s - loss: 3.9922 - acc: 0.0870\n",
      "Epoch 37/100\n",
      " - 23s - loss: 3.9620 - acc: 0.0864\n",
      "Epoch 38/100\n",
      " - 22s - loss: 3.9166 - acc: 0.0904\n",
      "Epoch 39/100\n",
      " - 22s - loss: 3.8878 - acc: 0.0862\n",
      "Epoch 40/100\n",
      " - 22s - loss: 3.8524 - acc: 0.0850\n",
      "Epoch 41/100\n",
      " - 22s - loss: 3.8527 - acc: 0.0838\n",
      "Epoch 42/100\n",
      " - 22s - loss: 3.8587 - acc: 0.0943\n",
      "Epoch 43/100\n",
      " - 22s - loss: 3.8703 - acc: 0.0987\n",
      "Epoch 44/100\n",
      " - 22s - loss: 3.9085 - acc: 0.0918\n",
      "Epoch 45/100\n",
      " - 22s - loss: 3.9077 - acc: 0.0976\n",
      "Epoch 46/100\n",
      " - 22s - loss: 3.7952 - acc: 0.0902\n",
      "Epoch 47/100\n",
      " - 22s - loss: 3.7627 - acc: 0.0907\n",
      "Epoch 48/100\n",
      " - 23s - loss: 3.7204 - acc: 0.0923\n",
      "Epoch 49/100\n",
      " - 22s - loss: 3.7054 - acc: 0.0921\n",
      "Epoch 50/100\n",
      " - 22s - loss: 3.6735 - acc: 0.0937\n",
      "Epoch 51/100\n",
      " - 22s - loss: 3.6623 - acc: 0.0900\n",
      "Epoch 52/100\n",
      " - 22s - loss: 3.6211 - acc: 0.0963\n",
      "Epoch 53/100\n",
      " - 22s - loss: 3.5994 - acc: 0.0949\n",
      "Epoch 54/100\n",
      " - 22s - loss: 3.5724 - acc: 0.0967\n",
      "Epoch 55/100\n",
      " - 22s - loss: 3.5685 - acc: 0.0944\n",
      "Epoch 56/100\n",
      " - 22s - loss: 3.6413 - acc: 0.0869\n",
      "Epoch 57/100\n",
      " - 22s - loss: 3.5682 - acc: 0.0935\n",
      "Epoch 58/100\n",
      " - 22s - loss: 3.5322 - acc: 0.1012\n",
      "Epoch 59/100\n",
      " - 22s - loss: 3.4886 - acc: 0.0975\n",
      "Epoch 60/100\n",
      " - 22s - loss: 3.4579 - acc: 0.0982\n",
      "Epoch 61/100\n",
      " - 22s - loss: 3.4626 - acc: 0.0932\n",
      "Epoch 62/100\n",
      " - 22s - loss: 3.4615 - acc: 0.0936\n",
      "Epoch 63/100\n",
      " - 22s - loss: 3.4042 - acc: 0.0935\n",
      "Epoch 64/100\n",
      " - 22s - loss: 3.3857 - acc: 0.0986\n",
      "Epoch 65/100\n",
      " - 22s - loss: 3.4042 - acc: 0.1001\n",
      "Epoch 66/100\n",
      " - 22s - loss: 3.3706 - acc: 0.1042\n",
      "Epoch 67/100\n",
      " - 22s - loss: 3.3474 - acc: 0.1081\n",
      "Epoch 68/100\n",
      " - 22s - loss: 3.3820 - acc: 0.0980\n",
      "Epoch 69/100\n",
      " - 22s - loss: 3.3804 - acc: 0.1110\n",
      "Epoch 70/100\n",
      " - 22s - loss: 3.3613 - acc: 0.1042\n",
      "Epoch 71/100\n",
      " - 22s - loss: 3.3661 - acc: 0.0953\n",
      "Epoch 72/100\n",
      " - 22s - loss: 3.3084 - acc: 0.1021\n",
      "Epoch 73/100\n",
      " - 22s - loss: 3.2364 - acc: 0.1128\n",
      "Epoch 74/100\n",
      " - 22s - loss: 3.2275 - acc: 0.1096\n",
      "Epoch 75/100\n",
      " - 22s - loss: 3.2291 - acc: 0.1132\n",
      "Epoch 76/100\n",
      " - 23s - loss: 3.1878 - acc: 0.1220\n",
      "Epoch 77/100\n",
      " - 22s - loss: 3.1705 - acc: 0.1157\n",
      "Epoch 78/100\n",
      " - 22s - loss: 3.2915 - acc: 0.0967\n",
      "Epoch 79/100\n",
      " - 22s - loss: 3.2542 - acc: 0.1074\n",
      "Epoch 80/100\n",
      " - 22s - loss: 3.3477 - acc: 0.1034\n",
      "Epoch 81/100\n",
      " - 24s - loss: 3.2130 - acc: 0.1088\n",
      "Epoch 82/100\n",
      " - 22s - loss: 3.2254 - acc: 0.1013\n",
      "Epoch 83/100\n",
      " - 22s - loss: 3.2485 - acc: 0.0980\n",
      "Epoch 84/100\n",
      " - 22s - loss: 3.2444 - acc: 0.0971\n",
      "Epoch 85/100\n",
      " - 23s - loss: 3.2414 - acc: 0.1002\n",
      "Epoch 86/100\n",
      " - 22s - loss: 3.2228 - acc: 0.0901\n",
      "Epoch 87/100\n",
      " - 22s - loss: 3.1669 - acc: 0.1056\n",
      "Epoch 88/100\n",
      " - 22s - loss: 3.1643 - acc: 0.1068\n",
      "Epoch 89/100\n",
      " - 22s - loss: 3.2982 - acc: 0.0936\n",
      "Epoch 90/100\n",
      " - 22s - loss: 3.3371 - acc: 0.0920\n",
      "Epoch 91/100\n",
      " - 22s - loss: 3.2929 - acc: 0.1059\n",
      "Epoch 92/100\n",
      " - 22s - loss: 3.2128 - acc: 0.1153\n",
      "Epoch 93/100\n",
      " - 23s - loss: 3.0993 - acc: 0.1243\n",
      "Epoch 94/100\n",
      " - 22s - loss: 3.0289 - acc: 0.1283\n",
      "Epoch 95/100\n",
      " - 22s - loss: 3.0155 - acc: 0.1259\n",
      "Epoch 96/100\n",
      " - 23s - loss: 3.0302 - acc: 0.1150\n",
      "Epoch 97/100\n",
      " - 22s - loss: 3.0186 - acc: 0.1204\n",
      "Epoch 98/100\n",
      " - 22s - loss: 3.0317 - acc: 0.1081\n",
      "Epoch 99/100\n",
      " - 23s - loss: 2.9369 - acc: 0.1208\n",
      "Epoch 100/100\n",
      " - 22s - loss: 2.9224 - acc: 0.1182\n",
      ">3: train=0.038773 test=0.003929\n",
      "          train      test\n",
      "count  3.000000  3.000000\n",
      "mean   0.017823  0.035789\n",
      "std    0.018168  0.041382\n",
      "min    0.006395  0.003929\n",
      "25%    0.007349  0.012403\n",
      "50%    0.008303  0.020878\n",
      "75%    0.023538  0.051719\n",
      "max    0.038773  0.082561\n"
     ]
    }
   ],
   "source": [
    "verbose = 2\n",
    "n_epochs = 100\n",
    "n_photos_per_update = 2\n",
    "n_batches_per_epoch = int(len(train) / n_photos_per_update)\n",
    "n_repeats = 3\n",
    "\n",
    "# run experiment\n",
    "train_results, test_results = list(), list()\n",
    "for i in range(n_repeats):\n",
    "    # define the model\n",
    "    model = define_model(vocab_size, max_length)\n",
    "    # define checkpoint callback\n",
    "#     filepath = 'size_lg_lang_model-model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "#     checkpoint = ModelCheckpoint(filepath, monitor='val_loss',\n",
    "#                                  verbose=1,\n",
    "#                                  save_best_only=True,\n",
    "#                                  mode='min')\n",
    "    # fit model\n",
    "    model.fit_generator(data_generator(train_descriptions, train_features, tokenizer, max_length, n_photos_per_update), \n",
    "                        steps_per_epoch=n_batches_per_epoch, \n",
    "                        epochs=n_epochs,\n",
    "                        verbose=verbose)\n",
    "    # evaluate model on training data\n",
    "    train_score = evaluate_model(model, train_descriptions, train_features, tokenizer, max_length)\n",
    "    test_score = evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)\n",
    "    # store\n",
    "    train_results.append(train_score)\n",
    "    test_results.append(test_score)\n",
    "    print('>%d: train=%f test=%f' % ((i+1), train_score, test_score))\n",
    "# save results to file\n",
    "df = DataFrame()\n",
    "df['train'] = train_results\n",
    "df['test'] = test_results\n",
    "print(df.describe())\n",
    "df.to_csv(model_name+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU scores table\n",
    "|       |   train  |    test|\n",
    "|-------|----------|--------|\n",
    "|count  |3.000000  |3.000000|\n",
    "|mean   |0.017823  |0.035789|\n",
    "|std    |0.018168  |0.041382|\n",
    "|min    |0.006395  |0.003929|\n",
    "|25%    |0.007349  |0.012403|\n",
    "|50%    |0.008303  |0.020878|\n",
    "|75%    |0.023538  |0.051719|\n",
    "|max    |0.038773  |0.082561|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the Feature Extraction Model\n",
    "The use of the pre-trained VGG16 model provides some additional points of configuration.\n",
    "\n",
    "The baseline model removed the top from the VGG model, including a global max pooling layer, which then feeds into an encoding of the features to a 128 element vector.\n",
    "\n",
    "In this section, we will look at the following modifications to the baseline model:\n",
    "\n",
    "1. Using a global average pooling layer after the VGG model.\n",
    "2. Not using any global pooling.\n",
    "\n",
    "### Global Average Pooling\n",
    "\n",
    "We can replace the GlobalMaxPooling2D layer with a GlobalAveragePooling2D to achieve average pooling.\n",
    "\n",
    "Global average pooling was developed to reduce overfitting for image classification problems, but may offer some benefit in interpreting the features extracted from the image.\n",
    "\n",
    "Global Average Pooling, see the paper: [Network In Network, 2013.](https://arxiv.org/abs/1312.4400)\n",
    "\n",
    "The updated *define_model()* function and experiment name are listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "    # feature extractor (encoder)\n",
    "    inputs1 = Input(shape=(7, 7, 512))\n",
    "    fe1 = GlobalAveragePooling2D()(inputs1)\n",
    "    fe2 = Dense(128, activation='relu')(fe1)\n",
    "    fe3 = RepeatVector(max_length)(fe2)\n",
    "    # embedding\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    emb2 = Embedding(vocab_size, 50, mask_zero=True)(inputs2)\n",
    "    emb3 = LSTM(256, return_sequences=True)(emb2)\n",
    "    emb4 = TimeDistributed(Dense(128, activation='relu'))(emb3)\n",
    "    # merge inputs\n",
    "    merged = concatenate([fe3, emb4])\n",
    "    # language model (decoder)\n",
    "    lm2 = LSTM(500)(merged)\n",
    "    lm3 = Dense(500, activation='relu')(lm2)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(lm3)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_name = 'fe_avg_pool'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 15s - loss: 7.0292 - acc: 0.0578\n",
      "Epoch 2/100\n",
      " - 14s - loss: 5.4995 - acc: 0.0763\n",
      "Epoch 3/100\n",
      " - 14s - loss: 5.3079 - acc: 0.0783\n",
      "Epoch 4/100\n",
      " - 15s - loss: 5.1376 - acc: 0.0791\n",
      "Epoch 5/100\n",
      " - 13s - loss: 4.9547 - acc: 0.0810\n",
      "Epoch 6/100\n",
      " - 14s - loss: 4.7919 - acc: 0.0924\n",
      "Epoch 7/100\n",
      " - 15s - loss: 4.6226 - acc: 0.0990\n",
      "Epoch 8/100\n",
      " - 13s - loss: 4.4890 - acc: 0.1001\n",
      "Epoch 9/100\n",
      " - 13s - loss: 4.3706 - acc: 0.1055\n",
      "Epoch 10/100\n",
      " - 13s - loss: 4.0196 - acc: 0.1492\n",
      "Epoch 11/100\n",
      " - 13s - loss: 3.8028 - acc: 0.1534\n",
      "Epoch 12/100\n",
      " - 13s - loss: 3.5568 - acc: 0.1675\n",
      "Epoch 13/100\n",
      " - 13s - loss: 3.2250 - acc: 0.1841\n",
      "Epoch 14/100\n",
      " - 13s - loss: 2.8723 - acc: 0.2225\n",
      "Epoch 15/100\n",
      " - 13s - loss: 2.7229 - acc: 0.2788\n",
      "Epoch 16/100\n",
      " - 13s - loss: 2.4083 - acc: 0.3154\n",
      "Epoch 17/100\n",
      " - 13s - loss: 2.0623 - acc: 0.3623\n",
      "Epoch 18/100\n",
      " - 13s - loss: 1.8391 - acc: 0.4178\n",
      "Epoch 19/100\n",
      " - 13s - loss: 1.7743 - acc: 0.4191\n",
      "Epoch 20/100\n",
      " - 13s - loss: 1.6464 - acc: 0.4600\n",
      "Epoch 21/100\n",
      " - 13s - loss: 1.6170 - acc: 0.4571\n",
      "Epoch 22/100\n",
      " - 13s - loss: 1.5287 - acc: 0.4914\n",
      "Epoch 23/100\n",
      " - 13s - loss: 1.3826 - acc: 0.5316\n",
      "Epoch 24/100\n",
      " - 13s - loss: 1.2071 - acc: 0.5849\n",
      "Epoch 25/100\n",
      " - 13s - loss: 1.1318 - acc: 0.6027\n",
      "Epoch 26/100\n",
      " - 13s - loss: 1.0486 - acc: 0.6361\n",
      "Epoch 27/100\n",
      " - 13s - loss: 0.9065 - acc: 0.6786\n",
      "Epoch 28/100\n",
      " - 13s - loss: 0.8208 - acc: 0.7088\n",
      "Epoch 29/100\n",
      " - 13s - loss: 0.7720 - acc: 0.7358\n",
      "Epoch 30/100\n",
      " - 13s - loss: 0.6824 - acc: 0.7650\n",
      "Epoch 31/100\n",
      " - 13s - loss: 0.5536 - acc: 0.8205\n",
      "Epoch 32/100\n",
      " - 13s - loss: 0.4735 - acc: 0.8327\n",
      "Epoch 33/100\n",
      " - 13s - loss: 0.4195 - acc: 0.8681\n",
      "Epoch 34/100\n",
      " - 13s - loss: 0.3612 - acc: 0.8808\n",
      "Epoch 35/100\n",
      " - 13s - loss: 0.3077 - acc: 0.9056\n",
      "Epoch 36/100\n",
      " - 13s - loss: 0.2581 - acc: 0.9300\n",
      "Epoch 37/100\n",
      " - 13s - loss: 0.2167 - acc: 0.9377\n",
      "Epoch 38/100\n",
      " - 13s - loss: 0.1810 - acc: 0.9479\n",
      "Epoch 39/100\n",
      " - 13s - loss: 0.1565 - acc: 0.9538\n",
      "Epoch 40/100\n",
      " - 13s - loss: 0.1320 - acc: 0.9668\n",
      "Epoch 41/100\n",
      " - 13s - loss: 0.1138 - acc: 0.9680\n",
      "Epoch 42/100\n",
      " - 13s - loss: 0.1050 - acc: 0.9665\n",
      "Epoch 43/100\n",
      " - 13s - loss: 0.0943 - acc: 0.9731\n",
      "Epoch 44/100\n",
      " - 13s - loss: 0.0807 - acc: 0.9792\n",
      "Epoch 45/100\n",
      " - 13s - loss: 0.0683 - acc: 0.9797\n",
      "Epoch 46/100\n",
      " - 13s - loss: 0.0727 - acc: 0.9811\n",
      "Epoch 47/100\n",
      " - 13s - loss: 0.0743 - acc: 0.9814\n",
      "Epoch 48/100\n",
      " - 13s - loss: 0.0571 - acc: 0.9887\n",
      "Epoch 49/100\n",
      " - 13s - loss: 0.0383 - acc: 0.9882\n",
      "Epoch 50/100\n",
      " - 13s - loss: 0.0313 - acc: 0.9920\n",
      "Epoch 51/100\n",
      " - 13s - loss: 0.0360 - acc: 0.9890\n",
      "Epoch 52/100\n",
      " - 13s - loss: 0.0439 - acc: 0.9923\n",
      "Epoch 53/100\n",
      " - 13s - loss: 0.0576 - acc: 0.9898\n",
      "Epoch 54/100\n",
      " - 13s - loss: 0.0729 - acc: 0.9869\n",
      "Epoch 55/100\n",
      " - 13s - loss: 0.0697 - acc: 0.9820\n",
      "Epoch 56/100\n",
      " - 13s - loss: 0.0701 - acc: 0.9835\n",
      "Epoch 57/100\n",
      " - 13s - loss: 0.0931 - acc: 0.9791\n",
      "Epoch 58/100\n",
      " - 13s - loss: 0.1307 - acc: 0.9623\n",
      "Epoch 59/100\n",
      " - 13s - loss: 0.1737 - acc: 0.9642\n",
      "Epoch 60/100\n",
      " - 13s - loss: 0.3411 - acc: 0.9137\n",
      "Epoch 61/100\n",
      " - 13s - loss: 0.4870 - acc: 0.8551\n",
      "Epoch 62/100\n",
      " - 13s - loss: 0.7242 - acc: 0.8079\n",
      "Epoch 63/100\n",
      " - 13s - loss: 0.5710 - acc: 0.8252\n",
      "Epoch 64/100\n",
      " - 13s - loss: 0.4161 - acc: 0.8880\n",
      "Epoch 65/100\n",
      " - 13s - loss: 0.2372 - acc: 0.9252\n",
      "Epoch 66/100\n",
      " - 13s - loss: 0.2061 - acc: 0.9377\n",
      "Epoch 67/100\n",
      " - 13s - loss: 0.1361 - acc: 0.9574\n",
      "Epoch 68/100\n",
      " - 13s - loss: 0.0666 - acc: 0.9822\n",
      "Epoch 69/100\n",
      " - 13s - loss: 0.0281 - acc: 0.9915\n",
      "Epoch 70/100\n",
      " - 13s - loss: 0.0152 - acc: 0.9976\n",
      "Epoch 71/100\n",
      " - 13s - loss: 0.0117 - acc: 0.9980\n",
      "Epoch 72/100\n",
      " - 13s - loss: 0.0134 - acc: 0.9972\n",
      "Epoch 73/100\n",
      " - 13s - loss: 0.0090 - acc: 0.9987\n",
      "Epoch 74/100\n",
      " - 13s - loss: 0.0071 - acc: 0.9987\n",
      "Epoch 75/100\n",
      " - 13s - loss: 0.0056 - acc: 0.9993\n",
      "Epoch 76/100\n",
      " - 13s - loss: 0.0048 - acc: 0.9993\n",
      "Epoch 77/100\n",
      " - 13s - loss: 0.0044 - acc: 0.9993\n",
      "Epoch 78/100\n",
      " - 13s - loss: 0.0038 - acc: 0.9993\n",
      "Epoch 79/100\n",
      " - 13s - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 80/100\n",
      " - 13s - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 81/100\n",
      " - 13s - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 82/100\n",
      " - 13s - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 83/100\n",
      " - 13s - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 84/100\n",
      " - 13s - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 85/100\n",
      " - 13s - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 86/100\n",
      " - 13s - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 87/100\n",
      " - 13s - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 88/100\n",
      " - 13s - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 89/100\n",
      " - 13s - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 90/100\n",
      " - 13s - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 91/100\n",
      " - 13s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 92/100\n",
      " - 13s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 93/100\n",
      " - 13s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 94/100\n",
      " - 13s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 95/100\n",
      " - 13s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 96/100\n",
      " - 13s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 97/100\n",
      " - 13s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 98/100\n",
      " - 13s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 99/100\n",
      " - 13s - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 100/100\n",
      " - 13s - loss: 9.7994e-04 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shourya97/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1: train=0.213250 test=0.025153\n",
      "Epoch 1/100\n",
      " - 14s - loss: 7.0233 - acc: 0.0525\n",
      "Epoch 2/100\n",
      " - 13s - loss: 5.5478 - acc: 0.0755\n",
      "Epoch 3/100\n",
      " - 13s - loss: 5.3485 - acc: 0.0765\n",
      "Epoch 4/100\n",
      " - 13s - loss: 5.2146 - acc: 0.0853\n",
      "Epoch 5/100\n",
      " - 13s - loss: 5.0448 - acc: 0.0829\n",
      "Epoch 6/100\n",
      " - 13s - loss: 4.8911 - acc: 0.0968\n",
      "Epoch 7/100\n",
      " - 13s - loss: 4.7111 - acc: 0.0947\n",
      "Epoch 8/100\n",
      " - 13s - loss: 4.6150 - acc: 0.1039\n",
      "Epoch 9/100\n",
      " - 13s - loss: 4.4388 - acc: 0.1020\n",
      "Epoch 10/100\n",
      " - 13s - loss: 4.2577 - acc: 0.1162\n",
      "Epoch 11/100\n",
      " - 13s - loss: 4.0087 - acc: 0.1425\n",
      "Epoch 12/100\n",
      " - 13s - loss: 3.7514 - acc: 0.1521\n",
      "Epoch 13/100\n",
      " - 13s - loss: 3.4851 - acc: 0.1917\n",
      "Epoch 14/100\n",
      " - 13s - loss: 3.2419 - acc: 0.1921\n",
      "Epoch 15/100\n",
      " - 13s - loss: 3.0040 - acc: 0.2247\n",
      "Epoch 16/100\n",
      " - 13s - loss: 2.6966 - acc: 0.2621\n",
      "Epoch 17/100\n",
      " - 13s - loss: 2.3588 - acc: 0.3176\n",
      "Epoch 18/100\n",
      " - 13s - loss: 2.1452 - acc: 0.3755\n",
      "Epoch 19/100\n",
      " - 13s - loss: 2.0108 - acc: 0.3708\n",
      "Epoch 20/100\n",
      " - 13s - loss: 1.9524 - acc: 0.3924\n",
      "Epoch 21/100\n",
      " - 13s - loss: 1.9580 - acc: 0.4053\n",
      "Epoch 22/100\n",
      " - 13s - loss: 1.7264 - acc: 0.4340\n",
      "Epoch 23/100\n",
      " - 13s - loss: 1.5332 - acc: 0.4758\n",
      "Epoch 24/100\n",
      " - 13s - loss: 1.4391 - acc: 0.5005\n",
      "Epoch 25/100\n",
      " - 13s - loss: 1.2729 - acc: 0.5400\n",
      "Epoch 26/100\n",
      " - 13s - loss: 1.2177 - acc: 0.5660\n",
      "Epoch 27/100\n",
      " - 13s - loss: 1.1241 - acc: 0.6057\n",
      "Epoch 28/100\n",
      " - 13s - loss: 1.0376 - acc: 0.6342\n",
      "Epoch 29/100\n",
      " - 13s - loss: 0.9728 - acc: 0.6238\n",
      "Epoch 30/100\n",
      " - 13s - loss: 1.0181 - acc: 0.6497\n",
      "Epoch 31/100\n",
      " - 13s - loss: 0.9754 - acc: 0.6470\n",
      "Epoch 32/100\n",
      " - 13s - loss: 0.9606 - acc: 0.6528\n",
      "Epoch 33/100\n",
      " - 13s - loss: 0.8522 - acc: 0.6996\n",
      "Epoch 34/100\n",
      " - 13s - loss: 0.7834 - acc: 0.7108\n",
      "Epoch 35/100\n",
      " - 13s - loss: 0.7283 - acc: 0.7351\n",
      "Epoch 36/100\n",
      " - 13s - loss: 0.6612 - acc: 0.7507\n",
      "Epoch 37/100\n",
      " - 13s - loss: 0.5955 - acc: 0.7938\n",
      "Epoch 38/100\n",
      " - 13s - loss: 0.5386 - acc: 0.8088\n",
      "Epoch 39/100\n",
      " - 13s - loss: 0.4912 - acc: 0.8299\n",
      "Epoch 40/100\n",
      " - 13s - loss: 0.4715 - acc: 0.8420\n",
      "Epoch 41/100\n",
      " - 13s - loss: 0.4148 - acc: 0.8540\n",
      "Epoch 42/100\n",
      " - 13s - loss: 0.3757 - acc: 0.8813\n",
      "Epoch 43/100\n",
      " - 13s - loss: 0.3141 - acc: 0.8995\n",
      "Epoch 44/100\n",
      " - 13s - loss: 0.2765 - acc: 0.9171\n",
      "Epoch 45/100\n",
      " - 13s - loss: 0.2504 - acc: 0.9197\n",
      "Epoch 46/100\n",
      " - 13s - loss: 0.2333 - acc: 0.9260\n",
      "Epoch 47/100\n",
      " - 13s - loss: 0.2133 - acc: 0.9287\n",
      "Epoch 48/100\n",
      " - 13s - loss: 0.1756 - acc: 0.9434\n",
      "Epoch 49/100\n",
      " - 13s - loss: 0.1488 - acc: 0.9497\n",
      "Epoch 50/100\n",
      " - 13s - loss: 0.1233 - acc: 0.9599\n",
      "Epoch 51/100\n",
      " - 13s - loss: 0.1154 - acc: 0.9631\n",
      "Epoch 52/100\n",
      " - 13s - loss: 0.0970 - acc: 0.9682\n",
      "Epoch 53/100\n",
      " - 13s - loss: 0.0782 - acc: 0.9796\n",
      "Epoch 54/100\n",
      " - 13s - loss: 0.0818 - acc: 0.9797\n",
      "Epoch 55/100\n",
      " - 13s - loss: 0.0735 - acc: 0.9777\n",
      "Epoch 56/100\n",
      " - 13s - loss: 0.0815 - acc: 0.9774\n",
      "Epoch 57/100\n",
      " - 13s - loss: 0.0742 - acc: 0.9823\n",
      "Epoch 58/100\n",
      " - 13s - loss: 0.0851 - acc: 0.9707\n",
      "Epoch 59/100\n",
      " - 13s - loss: 0.0976 - acc: 0.9718\n",
      "Epoch 60/100\n",
      " - 13s - loss: 0.1084 - acc: 0.9701\n",
      "Epoch 61/100\n",
      " - 13s - loss: 0.1094 - acc: 0.9641\n",
      "Epoch 62/100\n",
      " - 13s - loss: 0.1387 - acc: 0.9545\n",
      "Epoch 63/100\n",
      " - 13s - loss: 0.2199 - acc: 0.9285\n",
      "Epoch 64/100\n",
      " - 13s - loss: 0.3075 - acc: 0.9141\n",
      "Epoch 65/100\n",
      " - 13s - loss: 0.3779 - acc: 0.8971\n",
      "Epoch 66/100\n",
      " - 13s - loss: 0.3610 - acc: 0.8916\n",
      "Epoch 67/100\n",
      " - 13s - loss: 0.4388 - acc: 0.8688\n",
      "Epoch 68/100\n",
      " - 13s - loss: 0.5249 - acc: 0.8499\n",
      "Epoch 69/100\n",
      " - 13s - loss: 0.3794 - acc: 0.8751\n",
      "Epoch 70/100\n",
      " - 13s - loss: 0.2856 - acc: 0.9078\n",
      "Epoch 71/100\n",
      " - 14s - loss: 0.2512 - acc: 0.9231\n",
      "Epoch 72/100\n",
      " - 15s - loss: 0.1362 - acc: 0.9580\n",
      "Epoch 73/100\n",
      " - 13s - loss: 0.0933 - acc: 0.9760\n",
      "Epoch 74/100\n",
      " - 13s - loss: 0.0545 - acc: 0.9815\n",
      "Epoch 75/100\n",
      " - 13s - loss: 0.0340 - acc: 0.9897\n",
      "Epoch 76/100\n",
      " - 13s - loss: 0.0311 - acc: 0.9918\n",
      "Epoch 77/100\n",
      " - 13s - loss: 0.0284 - acc: 0.9930\n",
      "Epoch 78/100\n",
      " - 13s - loss: 0.0197 - acc: 0.9936\n",
      "Epoch 79/100\n",
      " - 13s - loss: 0.0189 - acc: 0.9940\n",
      "Epoch 80/100\n",
      " - 13s - loss: 0.0159 - acc: 0.9963\n",
      "Epoch 81/100\n",
      " - 13s - loss: 0.0130 - acc: 0.9957\n",
      "Epoch 82/100\n",
      " - 13s - loss: 0.0104 - acc: 0.9979\n",
      "Epoch 83/100\n",
      " - 13s - loss: 0.0083 - acc: 0.9979\n",
      "Epoch 84/100\n",
      " - 13s - loss: 0.0074 - acc: 0.9979\n",
      "Epoch 85/100\n",
      " - 13s - loss: 0.0074 - acc: 0.9987\n",
      "Epoch 86/100\n",
      " - 13s - loss: 0.0072 - acc: 0.9987\n",
      "Epoch 87/100\n",
      " - 13s - loss: 0.0064 - acc: 0.9987\n",
      "Epoch 88/100\n",
      " - 14s - loss: 0.0065 - acc: 0.9987\n",
      "Epoch 89/100\n",
      " - 13s - loss: 0.0057 - acc: 0.9987\n",
      "Epoch 90/100\n",
      " - 13s - loss: 0.0052 - acc: 0.9987\n",
      "Epoch 91/100\n",
      " - 13s - loss: 0.0049 - acc: 0.9987\n",
      "Epoch 92/100\n",
      " - 13s - loss: 0.0047 - acc: 0.9987\n",
      "Epoch 93/100\n",
      " - 13s - loss: 0.0043 - acc: 0.9987\n",
      "Epoch 94/100\n",
      " - 13s - loss: 0.0040 - acc: 0.9993\n",
      "Epoch 95/100\n",
      " - 13s - loss: 0.0039 - acc: 0.9993\n",
      "Epoch 96/100\n",
      " - 13s - loss: 0.0037 - acc: 0.9993\n",
      "Epoch 97/100\n",
      " - 13s - loss: 0.0034 - acc: 0.9993\n",
      "Epoch 98/100\n",
      " - 13s - loss: 0.0034 - acc: 0.9993\n",
      "Epoch 99/100\n",
      " - 13s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 100/100\n",
      " - 13s - loss: 0.0031 - acc: 0.9993\n",
      ">2: train=0.281187 test=0.021756\n",
      "Epoch 1/100\n",
      " - 15s - loss: 7.0436 - acc: 0.0510\n",
      "Epoch 2/100\n",
      " - 14s - loss: 5.5380 - acc: 0.0686\n",
      "Epoch 3/100\n",
      " - 14s - loss: 5.3827 - acc: 0.0816\n",
      "Epoch 4/100\n",
      " - 14s - loss: 5.1901 - acc: 0.0755\n",
      "Epoch 5/100\n",
      " - 14s - loss: 5.0572 - acc: 0.0857\n",
      "Epoch 6/100\n",
      " - 14s - loss: 4.9149 - acc: 0.0885\n",
      "Epoch 7/100\n",
      " - 14s - loss: 4.7676 - acc: 0.0954\n",
      "Epoch 8/100\n",
      " - 14s - loss: 4.6132 - acc: 0.1094\n",
      "Epoch 9/100\n",
      " - 14s - loss: 4.4479 - acc: 0.1143\n",
      "Epoch 10/100\n",
      " - 13s - loss: 4.2485 - acc: 0.1182\n",
      "Epoch 11/100\n",
      " - 13s - loss: 4.0483 - acc: 0.1482\n",
      "Epoch 12/100\n",
      " - 13s - loss: 3.8187 - acc: 0.1634\n",
      "Epoch 13/100\n",
      " - 13s - loss: 3.4990 - acc: 0.1914\n",
      "Epoch 14/100\n",
      " - 13s - loss: 3.1683 - acc: 0.2391\n",
      "Epoch 15/100\n",
      " - 13s - loss: 2.8950 - acc: 0.2646\n",
      "Epoch 16/100\n",
      " - 13s - loss: 2.6684 - acc: 0.2980\n",
      "Epoch 17/100\n",
      " - 13s - loss: 2.4182 - acc: 0.3333\n",
      "Epoch 18/100\n",
      " - 13s - loss: 2.2967 - acc: 0.3474\n",
      "Epoch 19/100\n",
      " - 13s - loss: 2.0298 - acc: 0.3852\n",
      "Epoch 20/100\n",
      " - 13s - loss: 1.7077 - acc: 0.4715\n",
      "Epoch 21/100\n",
      " - 13s - loss: 1.5252 - acc: 0.5227\n",
      "Epoch 22/100\n",
      " - 13s - loss: 1.3416 - acc: 0.5454\n",
      "Epoch 23/100\n",
      " - 13s - loss: 1.2742 - acc: 0.5764\n",
      "Epoch 24/100\n",
      " - 13s - loss: 1.0732 - acc: 0.6439\n",
      "Epoch 25/100\n",
      " - 13s - loss: 0.9498 - acc: 0.6940\n",
      "Epoch 26/100\n",
      " - 13s - loss: 0.8874 - acc: 0.7120\n",
      "Epoch 27/100\n",
      " - 13s - loss: 0.8723 - acc: 0.7208\n",
      "Epoch 28/100\n",
      " - 13s - loss: 0.8327 - acc: 0.7196\n",
      "Epoch 29/100\n",
      " - 13s - loss: 0.7617 - acc: 0.7609\n",
      "Epoch 30/100\n",
      " - 13s - loss: 0.7198 - acc: 0.7597\n",
      "Epoch 31/100\n",
      " - 14s - loss: 0.6277 - acc: 0.8040\n",
      "Epoch 32/100\n",
      " - 13s - loss: 0.6321 - acc: 0.7957\n",
      "Epoch 33/100\n",
      " - 13s - loss: 0.6429 - acc: 0.7736\n",
      "Epoch 34/100\n",
      " - 13s - loss: 0.5790 - acc: 0.7985\n",
      "Epoch 35/100\n",
      " - 13s - loss: 0.5055 - acc: 0.8224\n",
      "Epoch 36/100\n",
      " - 13s - loss: 0.3999 - acc: 0.8706\n",
      "Epoch 37/100\n",
      " - 13s - loss: 0.3103 - acc: 0.8915\n",
      "Epoch 38/100\n",
      " - 13s - loss: 0.2686 - acc: 0.9137\n",
      "Epoch 39/100\n",
      " - 13s - loss: 0.2191 - acc: 0.9351\n",
      "Epoch 40/100\n",
      " - 13s - loss: 0.1826 - acc: 0.9475\n",
      "Epoch 41/100\n",
      " - 13s - loss: 0.1356 - acc: 0.9619\n",
      "Epoch 42/100\n",
      " - 13s - loss: 0.1037 - acc: 0.9715\n",
      "Epoch 43/100\n",
      " - 13s - loss: 0.0918 - acc: 0.9767\n",
      "Epoch 44/100\n",
      " - 13s - loss: 0.0710 - acc: 0.9816\n",
      "Epoch 45/100\n",
      " - 13s - loss: 0.0591 - acc: 0.9809\n",
      "Epoch 46/100\n",
      " - 13s - loss: 0.0525 - acc: 0.9866\n",
      "Epoch 47/100\n",
      " - 13s - loss: 0.0473 - acc: 0.9854\n",
      "Epoch 48/100\n",
      " - 13s - loss: 0.0422 - acc: 0.9881\n",
      "Epoch 49/100\n",
      " - 13s - loss: 0.0377 - acc: 0.9889\n",
      "Epoch 50/100\n",
      " - 13s - loss: 0.0379 - acc: 0.9905\n",
      "Epoch 51/100\n",
      " - 13s - loss: 0.0344 - acc: 0.9898\n",
      "Epoch 52/100\n",
      " - 13s - loss: 0.0320 - acc: 0.9920\n",
      "Epoch 53/100\n",
      " - 13s - loss: 0.0249 - acc: 0.9941\n",
      "Epoch 54/100\n",
      " - 13s - loss: 0.0202 - acc: 0.9962\n",
      "Epoch 55/100\n",
      " - 13s - loss: 0.0187 - acc: 0.9947\n",
      "Epoch 56/100\n",
      " - 13s - loss: 0.0196 - acc: 0.9955\n",
      "Epoch 57/100\n",
      " - 13s - loss: 0.0182 - acc: 0.9955\n",
      "Epoch 58/100\n",
      " - 13s - loss: 0.0168 - acc: 0.9967\n",
      "Epoch 59/100\n",
      " - 13s - loss: 0.0147 - acc: 0.9973\n",
      "Epoch 60/100\n",
      " - 13s - loss: 0.0147 - acc: 0.9967\n",
      "Epoch 61/100\n",
      " - 13s - loss: 0.0138 - acc: 0.9967\n",
      "Epoch 62/100\n",
      " - 13s - loss: 0.0114 - acc: 0.9973\n",
      "Epoch 63/100\n",
      " - 13s - loss: 0.0119 - acc: 0.9967\n",
      "Epoch 64/100\n",
      " - 13s - loss: 0.0125 - acc: 0.9973\n",
      "Epoch 65/100\n",
      " - 13s - loss: 0.0126 - acc: 0.9967\n",
      "Epoch 66/100\n",
      " - 13s - loss: 0.0100 - acc: 0.9973\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 13s - loss: 0.0102 - acc: 0.9973\n",
      "Epoch 68/100\n",
      " - 13s - loss: 0.0095 - acc: 0.9973\n",
      "Epoch 69/100\n",
      " - 13s - loss: 0.0083 - acc: 0.9973\n",
      "Epoch 70/100\n",
      " - 13s - loss: 0.0073 - acc: 0.9983\n",
      "Epoch 71/100\n",
      " - 13s - loss: 0.0074 - acc: 0.9983\n",
      "Epoch 72/100\n",
      " - 13s - loss: 0.0060 - acc: 0.9983\n",
      "Epoch 73/100\n",
      " - 13s - loss: 0.0045 - acc: 0.9983\n",
      "Epoch 74/100\n",
      " - 13s - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 75/100\n",
      " - 13s - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 76/100\n",
      " - 13s - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 77/100\n",
      " - 13s - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 78/100\n",
      " - 13s - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 79/100\n",
      " - 13s - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 80/100\n",
      " - 13s - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 81/100\n",
      " - 13s - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 82/100\n",
      " - 13s - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 83/100\n",
      " - 13s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 84/100\n",
      " - 13s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 85/100\n",
      " - 13s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 86/100\n",
      " - 13s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 87/100\n",
      " - 13s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 88/100\n",
      " - 13s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 89/100\n",
      " - 13s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 90/100\n",
      " - 13s - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 91/100\n",
      " - 13s - loss: 9.7788e-04 - acc: 1.0000\n",
      "Epoch 92/100\n",
      " - 13s - loss: 9.3488e-04 - acc: 1.0000\n",
      "Epoch 93/100\n",
      " - 13s - loss: 8.9607e-04 - acc: 1.0000\n",
      "Epoch 94/100\n",
      " - 13s - loss: 8.5775e-04 - acc: 1.0000\n",
      "Epoch 95/100\n",
      " - 13s - loss: 8.2266e-04 - acc: 1.0000\n",
      "Epoch 96/100\n",
      " - 13s - loss: 7.8847e-04 - acc: 1.0000\n",
      "Epoch 97/100\n",
      " - 13s - loss: 7.5758e-04 - acc: 1.0000\n",
      "Epoch 98/100\n",
      " - 13s - loss: 7.2666e-04 - acc: 1.0000\n",
      "Epoch 99/100\n",
      " - 13s - loss: 6.9773e-04 - acc: 1.0000\n",
      "Epoch 100/100\n",
      " - 13s - loss: 6.7037e-04 - acc: 1.0000\n",
      ">3: train=0.231541 test=0.025265\n",
      "          train      test\n",
      "count  3.000000  3.000000\n",
      "mean   0.241993  0.024058\n",
      "std    0.035154  0.001994\n",
      "min    0.213250  0.021756\n",
      "25%    0.222396  0.023454\n",
      "50%    0.231541  0.025153\n",
      "75%    0.256364  0.025209\n",
      "max    0.281187  0.025265\n"
     ]
    }
   ],
   "source": [
    "verbose = 2\n",
    "n_epochs = 100\n",
    "n_photos_per_update = 2\n",
    "n_batches_per_epoch = int(len(train) / n_photos_per_update)\n",
    "n_repeats = 3\n",
    "\n",
    "# run experiment\n",
    "train_results, test_results = list(), list()\n",
    "for i in range(n_repeats):\n",
    "    # define the model\n",
    "    model = define_model(vocab_size, max_length)\n",
    "    # define checkpoint callback\n",
    "#     filepath = 'fe_avg_pool-model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "#     checkpoint = ModelCheckpoint(filepath, monitor='val_loss',\n",
    "#                                  verbose=1,\n",
    "#                                  save_best_only=True,\n",
    "#                                  mode='min')\n",
    "    # fit model\n",
    "    model.fit_generator(data_generator(train_descriptions, train_features, tokenizer, max_length, n_photos_per_update), \n",
    "                        steps_per_epoch=n_batches_per_epoch, \n",
    "                        epochs=n_epochs,\n",
    "                        verbose=verbose)\n",
    "    # evaluate model on training data\n",
    "    train_score = evaluate_model(model, train_descriptions, train_features, tokenizer, max_length)\n",
    "    test_score = evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)\n",
    "    # store\n",
    "    train_results.append(train_score)\n",
    "    test_results.append(test_score)\n",
    "    print('>%d: train=%f test=%f' % ((i+1), train_score, test_score))\n",
    "# save results to file\n",
    "df = DataFrame()\n",
    "df['train'] = train_results\n",
    "df['test'] = test_results\n",
    "print(df.describe())\n",
    "df.to_csv(model_name+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU scores table\n",
    "|       |   train  |    test|\n",
    "|-------|----------|--------|\n",
    "|count  |3.000000  |3.000000|\n",
    "|mean   |0.241993  |0.024058|\n",
    "|std    |0.035154  |0.001994|\n",
    "|min    |0.213250  |0.021756|\n",
    "|25%    |0.222396  |0.023454|\n",
    "|50%    |0.231541  |0.025153|\n",
    "|75%    |0.256364  |0.025209|\n",
    "|max    |0.281187  |0.025265|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Pooling\n",
    "We can **remove the GlobalMaxPooling2D and flatten the 3D photo feature and feed it directly into a Dense layer**.\n",
    "\n",
    "I would not expect this to be a good model design, but it is worth testing this assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "    # feature extractor (encoder)\n",
    "    inputs1 = Input(shape=(7, 7, 512))\n",
    "    fe1 = Flatten()(inputs1)\n",
    "    fe2 = Dense(128, activation='relu')(fe1)\n",
    "    fe3 = RepeatVector(max_length)(fe2)\n",
    "    # embedding\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    emb2 = Embedding(vocab_size, 50, mask_zero=True)(inputs2)\n",
    "    emb3 = LSTM(256, return_sequences=True)(emb2)\n",
    "    emb4 = TimeDistributed(Dense(128, activation='relu'))(emb3)\n",
    "    # merge inputs\n",
    "    merged = concatenate([fe3, emb4])\n",
    "    # language model (decoder)\n",
    "    lm2 = LSTM(500)(merged)\n",
    "    lm3 = Dense(500, activation='relu')(lm2)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(lm3)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_name = 'fe_flat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 17s - loss: 7.0724 - acc: 0.0502\n",
      "Epoch 2/100\n",
      " - 15s - loss: 5.5723 - acc: 0.0690\n",
      "Epoch 3/100\n",
      " - 15s - loss: 5.4107 - acc: 0.0717\n",
      "Epoch 4/100\n",
      " - 15s - loss: 5.2668 - acc: 0.0761\n",
      "Epoch 5/100\n",
      " - 15s - loss: 5.2143 - acc: 0.0707\n",
      "Epoch 6/100\n",
      " - 15s - loss: 5.0935 - acc: 0.0768\n",
      "Epoch 7/100\n",
      " - 16s - loss: 4.9703 - acc: 0.0801\n",
      "Epoch 8/100\n",
      " - 15s - loss: 4.8923 - acc: 0.0802\n",
      "Epoch 9/100\n",
      " - 15s - loss: 4.8030 - acc: 0.0793\n",
      "Epoch 10/100\n",
      " - 15s - loss: 4.6878 - acc: 0.0777\n",
      "Epoch 11/100\n",
      " - 15s - loss: 4.5286 - acc: 0.0892\n",
      "Epoch 12/100\n",
      " - 15s - loss: 4.4169 - acc: 0.0934\n",
      "Epoch 13/100\n",
      " - 15s - loss: 4.2775 - acc: 0.0864\n",
      "Epoch 14/100\n",
      " - 15s - loss: 4.0841 - acc: 0.1083\n",
      "Epoch 15/100\n",
      " - 15s - loss: 3.9508 - acc: 0.1088\n",
      "Epoch 16/100\n",
      " - 15s - loss: 3.7477 - acc: 0.1117\n",
      "Epoch 17/100\n",
      " - 15s - loss: 3.5519 - acc: 0.1151\n",
      "Epoch 18/100\n",
      " - 15s - loss: 3.4535 - acc: 0.1181\n",
      "Epoch 19/100\n",
      " - 15s - loss: 3.3317 - acc: 0.1255\n",
      "Epoch 20/100\n",
      " - 15s - loss: 3.2259 - acc: 0.1374\n",
      "Epoch 21/100\n",
      " - 15s - loss: 3.0658 - acc: 0.1335\n",
      "Epoch 22/100\n",
      " - 15s - loss: 2.9743 - acc: 0.1336\n",
      "Epoch 23/100\n",
      " - 15s - loss: 2.9087 - acc: 0.1403\n",
      "Epoch 24/100\n",
      " - 15s - loss: 2.8528 - acc: 0.1422\n",
      "Epoch 25/100\n",
      " - 15s - loss: 2.9336 - acc: 0.1346\n",
      "Epoch 26/100\n",
      " - 15s - loss: 2.8347 - acc: 0.1402\n",
      "Epoch 27/100\n",
      " - 15s - loss: 2.8016 - acc: 0.1328\n",
      "Epoch 28/100\n",
      " - 15s - loss: 2.7941 - acc: 0.1453\n",
      "Epoch 29/100\n",
      " - 15s - loss: 2.7270 - acc: 0.1494\n",
      "Epoch 30/100\n",
      " - 15s - loss: 2.6055 - acc: 0.1516\n",
      "Epoch 31/100\n",
      " - 15s - loss: 2.5854 - acc: 0.1616\n",
      "Epoch 32/100\n",
      " - 15s - loss: 2.5966 - acc: 0.1517\n",
      "Epoch 33/100\n",
      " - 15s - loss: 2.5451 - acc: 0.1554\n",
      "Epoch 34/100\n",
      " - 15s - loss: 2.5272 - acc: 0.1539\n",
      "Epoch 35/100\n",
      " - 15s - loss: 2.5726 - acc: 0.1566\n",
      "Epoch 36/100\n",
      " - 15s - loss: 2.5339 - acc: 0.1512\n",
      "Epoch 37/100\n",
      " - 15s - loss: 2.5104 - acc: 0.1589\n",
      "Epoch 38/100\n",
      " - 15s - loss: 2.5176 - acc: 0.1552\n",
      "Epoch 39/100\n",
      " - 15s - loss: 2.5462 - acc: 0.1612\n",
      "Epoch 40/100\n",
      " - 15s - loss: 2.5557 - acc: 0.1496\n",
      "Epoch 41/100\n",
      " - 15s - loss: 2.5884 - acc: 0.1574\n",
      "Epoch 42/100\n",
      " - 15s - loss: 2.5269 - acc: 0.1645\n",
      "Epoch 43/100\n",
      " - 15s - loss: 2.4905 - acc: 0.1483\n",
      "Epoch 44/100\n",
      " - 15s - loss: 2.5079 - acc: 0.1519\n",
      "Epoch 45/100\n",
      " - 15s - loss: 2.4848 - acc: 0.1695\n",
      "Epoch 46/100\n",
      " - 15s - loss: 2.4192 - acc: 0.1698\n",
      "Epoch 47/100\n",
      " - 15s - loss: 2.4421 - acc: 0.1626\n",
      "Epoch 48/100\n",
      " - 15s - loss: 2.4014 - acc: 0.1728\n",
      "Epoch 49/100\n",
      " - 15s - loss: 2.3611 - acc: 0.1745\n",
      "Epoch 50/100\n",
      " - 15s - loss: 2.3396 - acc: 0.1812\n",
      "Epoch 51/100\n",
      " - 15s - loss: 2.3624 - acc: 0.1753\n",
      "Epoch 52/100\n",
      " - 15s - loss: 2.3848 - acc: 0.1832\n",
      "Epoch 53/100\n",
      " - 15s - loss: 2.3806 - acc: 0.1799\n",
      "Epoch 54/100\n",
      " - 15s - loss: 2.3585 - acc: 0.1759\n",
      "Epoch 55/100\n",
      " - 15s - loss: 2.3600 - acc: 0.1842\n",
      "Epoch 56/100\n",
      " - 15s - loss: 2.2816 - acc: 0.1887\n",
      "Epoch 57/100\n",
      " - 15s - loss: 2.2997 - acc: 0.1894\n",
      "Epoch 58/100\n",
      " - 15s - loss: 2.3307 - acc: 0.1857\n",
      "Epoch 59/100\n",
      " - 15s - loss: 2.3340 - acc: 0.1777\n",
      "Epoch 60/100\n",
      " - 15s - loss: 2.2951 - acc: 0.1921\n",
      "Epoch 61/100\n",
      " - 15s - loss: 2.3092 - acc: 0.1873\n",
      "Epoch 62/100\n",
      " - 15s - loss: 2.2910 - acc: 0.1865\n",
      "Epoch 63/100\n",
      " - 15s - loss: 2.2679 - acc: 0.1850\n",
      "Epoch 64/100\n",
      " - 15s - loss: 2.2676 - acc: 0.1989\n",
      "Epoch 65/100\n",
      " - 15s - loss: 2.2747 - acc: 0.1920\n",
      "Epoch 66/100\n",
      " - 15s - loss: 2.3000 - acc: 0.1968\n",
      "Epoch 67/100\n",
      " - 15s - loss: 2.2714 - acc: 0.1983\n",
      "Epoch 68/100\n",
      " - 15s - loss: 2.2782 - acc: 0.1968\n",
      "Epoch 69/100\n",
      " - 15s - loss: 2.2528 - acc: 0.1960\n",
      "Epoch 70/100\n",
      " - 15s - loss: 2.2701 - acc: 0.1956\n",
      "Epoch 71/100\n",
      " - 15s - loss: 2.2895 - acc: 0.1885\n",
      "Epoch 72/100\n",
      " - 15s - loss: 2.2854 - acc: 0.1976\n",
      "Epoch 73/100\n",
      " - 15s - loss: 2.2688 - acc: 0.2098\n",
      "Epoch 74/100\n",
      " - 15s - loss: 2.2211 - acc: 0.2106\n",
      "Epoch 75/100\n",
      " - 15s - loss: 2.2759 - acc: 0.1923\n",
      "Epoch 76/100\n",
      " - 15s - loss: 2.2939 - acc: 0.1965\n",
      "Epoch 77/100\n",
      " - 15s - loss: 2.1915 - acc: 0.2065\n",
      "Epoch 78/100\n",
      " - 15s - loss: 2.1956 - acc: 0.2090\n",
      "Epoch 79/100\n",
      " - 15s - loss: 2.2485 - acc: 0.2277\n",
      "Epoch 80/100\n",
      " - 15s - loss: 2.2183 - acc: 0.2013\n",
      "Epoch 81/100\n",
      " - 15s - loss: 2.2409 - acc: 0.2015\n",
      "Epoch 82/100\n",
      " - 15s - loss: 2.2536 - acc: 0.2019\n",
      "Epoch 83/100\n",
      " - 15s - loss: 2.2295 - acc: 0.2069\n",
      "Epoch 84/100\n",
      " - 15s - loss: 2.2414 - acc: 0.1857\n",
      "Epoch 85/100\n",
      " - 15s - loss: 2.2595 - acc: 0.2056\n",
      "Epoch 86/100\n",
      " - 15s - loss: 2.2412 - acc: 0.2053\n",
      "Epoch 87/100\n",
      " - 15s - loss: 2.2262 - acc: 0.2206\n",
      "Epoch 88/100\n",
      " - 15s - loss: 2.2189 - acc: 0.2118\n",
      "Epoch 89/100\n",
      " - 15s - loss: 2.2033 - acc: 0.2156\n",
      "Epoch 90/100\n",
      " - 15s - loss: 2.1745 - acc: 0.2194\n",
      "Epoch 91/100\n",
      " - 15s - loss: 2.1507 - acc: 0.2306\n",
      "Epoch 92/100\n",
      " - 15s - loss: 2.1301 - acc: 0.2265\n",
      "Epoch 93/100\n",
      " - 15s - loss: 2.1246 - acc: 0.2280\n",
      "Epoch 94/100\n",
      " - 15s - loss: 2.1285 - acc: 0.2357\n",
      "Epoch 95/100\n",
      " - 15s - loss: 2.1444 - acc: 0.2211\n",
      "Epoch 96/100\n",
      " - 15s - loss: 2.1395 - acc: 0.2230\n",
      "Epoch 97/100\n",
      " - 15s - loss: 2.0880 - acc: 0.2275\n",
      "Epoch 98/100\n",
      " - 15s - loss: 2.0832 - acc: 0.2388\n",
      "Epoch 99/100\n",
      " - 15s - loss: 2.1120 - acc: 0.2393\n",
      "Epoch 100/100\n",
      " - 15s - loss: 2.1370 - acc: 0.2327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shourya97/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/home/shourya97/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1: train=0.025289 test=0.085539\n",
      "Epoch 1/100\n",
      " - 16s - loss: 7.0533 - acc: 0.0486\n",
      "Epoch 2/100\n",
      " - 15s - loss: 5.5888 - acc: 0.0615\n",
      "Epoch 3/100\n",
      " - 15s - loss: 5.3859 - acc: 0.0638\n",
      "Epoch 4/100\n",
      " - 15s - loss: 5.2363 - acc: 0.0729\n",
      "Epoch 5/100\n",
      " - 15s - loss: 5.1034 - acc: 0.0740\n",
      "Epoch 6/100\n",
      " - 15s - loss: 5.0222 - acc: 0.0741\n",
      "Epoch 7/100\n",
      " - 15s - loss: 4.8782 - acc: 0.0842\n",
      "Epoch 8/100\n",
      " - 15s - loss: 4.7867 - acc: 0.0827\n",
      "Epoch 9/100\n",
      " - 15s - loss: 4.6357 - acc: 0.0923\n",
      "Epoch 10/100\n",
      " - 15s - loss: 4.4789 - acc: 0.0954\n",
      "Epoch 11/100\n",
      " - 15s - loss: 4.2470 - acc: 0.0945\n",
      "Epoch 12/100\n",
      " - 15s - loss: 4.0394 - acc: 0.0961\n",
      "Epoch 13/100\n",
      " - 15s - loss: 3.8882 - acc: 0.0983\n",
      "Epoch 14/100\n",
      " - 15s - loss: 3.7122 - acc: 0.1114\n",
      "Epoch 15/100\n",
      " - 15s - loss: 3.5197 - acc: 0.1138\n",
      "Epoch 16/100\n",
      " - 15s - loss: 3.3190 - acc: 0.1225\n",
      "Epoch 17/100\n",
      " - 15s - loss: 3.1657 - acc: 0.1254\n",
      "Epoch 18/100\n",
      " - 15s - loss: 2.9955 - acc: 0.1285\n",
      "Epoch 19/100\n",
      " - 15s - loss: 2.9459 - acc: 0.1331\n",
      "Epoch 20/100\n",
      " - 15s - loss: 2.8593 - acc: 0.1364\n",
      "Epoch 21/100\n",
      " - 15s - loss: 2.8113 - acc: 0.1413\n",
      "Epoch 22/100\n",
      " - 15s - loss: 2.7110 - acc: 0.1443\n",
      "Epoch 23/100\n",
      " - 15s - loss: 2.6781 - acc: 0.1403\n",
      "Epoch 24/100\n",
      " - 15s - loss: 2.6883 - acc: 0.1419\n",
      "Epoch 25/100\n",
      " - 15s - loss: 2.6503 - acc: 0.1535\n",
      "Epoch 26/100\n",
      " - 15s - loss: 2.6176 - acc: 0.1404\n",
      "Epoch 27/100\n",
      " - 15s - loss: 2.5408 - acc: 0.1453\n",
      "Epoch 28/100\n",
      " - 15s - loss: 2.5326 - acc: 0.1468\n",
      "Epoch 29/100\n",
      " - 15s - loss: 2.5538 - acc: 0.1554\n",
      "Epoch 30/100\n",
      " - 15s - loss: 2.5567 - acc: 0.1509\n",
      "Epoch 31/100\n",
      " - 15s - loss: 2.5137 - acc: 0.1434\n",
      "Epoch 32/100\n",
      " - 15s - loss: 2.4774 - acc: 0.1526\n",
      "Epoch 33/100\n",
      " - 15s - loss: 2.5469 - acc: 0.1476\n",
      "Epoch 34/100\n",
      " - 15s - loss: 2.5464 - acc: 0.1533\n",
      "Epoch 35/100\n",
      " - 15s - loss: 2.4552 - acc: 0.1655\n",
      "Epoch 36/100\n",
      " - 15s - loss: 2.4615 - acc: 0.1570\n",
      "Epoch 37/100\n",
      " - 15s - loss: 2.4483 - acc: 0.1520\n",
      "Epoch 38/100\n",
      " - 15s - loss: 2.4199 - acc: 0.1579\n",
      "Epoch 39/100\n",
      " - 15s - loss: 2.3997 - acc: 0.1560\n",
      "Epoch 40/100\n",
      " - 15s - loss: 2.3803 - acc: 0.1598\n",
      "Epoch 41/100\n",
      " - 15s - loss: 2.3936 - acc: 0.1555\n",
      "Epoch 42/100\n",
      " - 15s - loss: 2.3899 - acc: 0.1578\n",
      "Epoch 43/100\n",
      " - 15s - loss: 2.4030 - acc: 0.1464\n",
      "Epoch 44/100\n",
      " - 15s - loss: 2.4050 - acc: 0.1695\n",
      "Epoch 45/100\n",
      " - 15s - loss: 2.4100 - acc: 0.1537\n",
      "Epoch 46/100\n",
      " - 15s - loss: 2.4166 - acc: 0.1645\n",
      "Epoch 47/100\n",
      " - 15s - loss: 2.3807 - acc: 0.1608\n",
      "Epoch 48/100\n",
      " - 15s - loss: 2.3660 - acc: 0.1667\n",
      "Epoch 49/100\n",
      " - 15s - loss: 2.3606 - acc: 0.1609\n",
      "Epoch 50/100\n",
      " - 15s - loss: 2.3500 - acc: 0.1538\n",
      "Epoch 51/100\n",
      " - 15s - loss: 2.3911 - acc: 0.1653\n",
      "Epoch 52/100\n",
      " - 15s - loss: 2.3832 - acc: 0.1574\n",
      "Epoch 53/100\n",
      " - 15s - loss: 2.3755 - acc: 0.1673\n",
      "Epoch 54/100\n",
      " - 15s - loss: 2.4070 - acc: 0.1618\n",
      "Epoch 55/100\n",
      " - 15s - loss: 2.3832 - acc: 0.1581\n",
      "Epoch 56/100\n",
      " - 15s - loss: 2.3426 - acc: 0.1633\n",
      "Epoch 57/100\n",
      " - 15s - loss: 2.3353 - acc: 0.1831\n",
      "Epoch 58/100\n",
      " - 15s - loss: 2.3301 - acc: 0.1710\n",
      "Epoch 59/100\n",
      " - 15s - loss: 2.3225 - acc: 0.1728\n",
      "Epoch 60/100\n",
      " - 15s - loss: 2.3266 - acc: 0.1732\n",
      "Epoch 61/100\n",
      " - 15s - loss: 2.2644 - acc: 0.1750\n",
      "Epoch 62/100\n",
      " - 15s - loss: 2.2395 - acc: 0.1722\n",
      "Epoch 63/100\n",
      " - 15s - loss: 2.2617 - acc: 0.1985\n",
      "Epoch 64/100\n",
      " - 15s - loss: 2.2622 - acc: 0.1881\n",
      "Epoch 65/100\n",
      " - 15s - loss: 2.2890 - acc: 0.1894\n",
      "Epoch 66/100\n",
      " - 15s - loss: 2.3357 - acc: 0.1865\n",
      "Epoch 67/100\n",
      " - 15s - loss: 2.3358 - acc: 0.1847\n",
      "Epoch 68/100\n",
      " - 15s - loss: 2.3139 - acc: 0.1748\n",
      "Epoch 69/100\n",
      " - 15s - loss: 2.2996 - acc: 0.1887\n",
      "Epoch 70/100\n",
      " - 15s - loss: 2.3518 - acc: 0.1869\n",
      "Epoch 71/100\n",
      " - 15s - loss: 2.3330 - acc: 0.1795\n",
      "Epoch 72/100\n",
      " - 15s - loss: 2.3110 - acc: 0.1952\n",
      "Epoch 73/100\n",
      " - 15s - loss: 2.2987 - acc: 0.2042\n",
      "Epoch 74/100\n",
      " - 15s - loss: 2.3195 - acc: 0.1869\n",
      "Epoch 75/100\n",
      " - 15s - loss: 2.3062 - acc: 0.1920\n",
      "Epoch 76/100\n",
      " - 15s - loss: 2.3016 - acc: 0.1961\n",
      "Epoch 77/100\n",
      " - 15s - loss: 2.3262 - acc: 0.1830\n",
      "Epoch 78/100\n",
      " - 15s - loss: 2.2745 - acc: 0.2073\n",
      "Epoch 79/100\n",
      " - 15s - loss: 2.2608 - acc: 0.1908\n",
      "Epoch 80/100\n",
      " - 15s - loss: 2.2183 - acc: 0.2086\n",
      "Epoch 81/100\n",
      " - 15s - loss: 2.2326 - acc: 0.1971\n",
      "Epoch 82/100\n",
      " - 15s - loss: 2.2537 - acc: 0.2118\n",
      "Epoch 83/100\n",
      " - 15s - loss: 2.2003 - acc: 0.1956\n",
      "Epoch 84/100\n",
      " - 15s - loss: 2.1960 - acc: 0.2247\n",
      "Epoch 85/100\n",
      " - 15s - loss: 2.2223 - acc: 0.2200\n",
      "Epoch 86/100\n",
      " - 15s - loss: 2.2037 - acc: 0.2068\n",
      "Epoch 87/100\n",
      " - 15s - loss: 2.2105 - acc: 0.1952\n",
      "Epoch 88/100\n",
      " - 15s - loss: 2.1693 - acc: 0.2074\n",
      "Epoch 89/100\n",
      " - 15s - loss: 2.1154 - acc: 0.2249\n",
      "Epoch 90/100\n",
      " - 15s - loss: 2.1229 - acc: 0.2296\n",
      "Epoch 91/100\n",
      " - 15s - loss: 2.1778 - acc: 0.2259\n",
      "Epoch 92/100\n",
      " - 15s - loss: 2.1474 - acc: 0.2326\n",
      "Epoch 93/100\n",
      " - 15s - loss: 2.1419 - acc: 0.2396\n",
      "Epoch 94/100\n",
      " - 15s - loss: 2.0911 - acc: 0.2524\n",
      "Epoch 95/100\n",
      " - 15s - loss: 2.0570 - acc: 0.2573\n",
      "Epoch 96/100\n",
      " - 15s - loss: 2.0810 - acc: 0.2550\n",
      "Epoch 97/100\n",
      " - 15s - loss: 2.0482 - acc: 0.2584\n",
      "Epoch 98/100\n",
      " - 15s - loss: 2.0733 - acc: 0.2485\n",
      "Epoch 99/100\n",
      " - 15s - loss: 2.0344 - acc: 0.2692\n",
      "Epoch 100/100\n",
      " - 15s - loss: 2.0194 - acc: 0.2860\n",
      ">2: train=0.010474 test=0.102160\n",
      "Epoch 1/100\n",
      " - 16s - loss: 7.0495 - acc: 0.0448\n",
      "Epoch 2/100\n",
      " - 15s - loss: 5.6132 - acc: 0.0627\n",
      "Epoch 3/100\n",
      " - 15s - loss: 5.4138 - acc: 0.0672\n",
      "Epoch 4/100\n",
      " - 15s - loss: 5.2991 - acc: 0.0677\n",
      "Epoch 5/100\n",
      " - 15s - loss: 5.2140 - acc: 0.0742\n",
      "Epoch 6/100\n",
      " - 15s - loss: 5.0960 - acc: 0.0756\n",
      "Epoch 7/100\n",
      " - 15s - loss: 4.9549 - acc: 0.0774\n",
      "Epoch 8/100\n",
      " - 15s - loss: 4.8412 - acc: 0.0806\n",
      "Epoch 9/100\n",
      " - 16s - loss: 4.7407 - acc: 0.0869\n",
      "Epoch 10/100\n",
      " - 15s - loss: 4.5917 - acc: 0.0870\n",
      "Epoch 11/100\n",
      " - 15s - loss: 4.3995 - acc: 0.0887\n",
      "Epoch 12/100\n",
      " - 15s - loss: 4.1803 - acc: 0.0912\n",
      "Epoch 13/100\n",
      " - 15s - loss: 4.0734 - acc: 0.0956\n",
      "Epoch 14/100\n",
      " - 15s - loss: 3.9097 - acc: 0.1079\n",
      "Epoch 15/100\n",
      " - 15s - loss: 3.7451 - acc: 0.1107\n",
      "Epoch 16/100\n",
      " - 15s - loss: 3.6253 - acc: 0.1087\n",
      "Epoch 17/100\n",
      " - 15s - loss: 3.4724 - acc: 0.1242\n",
      "Epoch 18/100\n",
      " - 15s - loss: 3.3147 - acc: 0.1241\n",
      "Epoch 19/100\n",
      " - 15s - loss: 3.1304 - acc: 0.1423\n",
      "Epoch 20/100\n",
      " - 15s - loss: 3.1046 - acc: 0.1350\n",
      "Epoch 21/100\n",
      " - 15s - loss: 3.0284 - acc: 0.1230\n",
      "Epoch 22/100\n",
      " - 15s - loss: 2.9517 - acc: 0.1452\n",
      "Epoch 23/100\n",
      " - 15s - loss: 2.8823 - acc: 0.1318\n",
      "Epoch 24/100\n",
      " - 15s - loss: 2.8485 - acc: 0.1423\n",
      "Epoch 25/100\n",
      " - 15s - loss: 2.8205 - acc: 0.1412\n",
      "Epoch 26/100\n",
      " - 15s - loss: 2.7908 - acc: 0.1398\n",
      "Epoch 27/100\n",
      " - 15s - loss: 2.7577 - acc: 0.1393\n",
      "Epoch 28/100\n",
      " - 15s - loss: 2.7256 - acc: 0.1370\n",
      "Epoch 29/100\n",
      " - 15s - loss: 2.6942 - acc: 0.1511\n",
      "Epoch 30/100\n",
      " - 15s - loss: 2.6249 - acc: 0.1434\n",
      "Epoch 31/100\n",
      " - 15s - loss: 2.6331 - acc: 0.1500\n",
      "Epoch 32/100\n",
      " - 15s - loss: 2.5490 - acc: 0.1463\n",
      "Epoch 33/100\n",
      " - 15s - loss: 2.5724 - acc: 0.1447\n",
      "Epoch 34/100\n",
      " - 15s - loss: 2.4994 - acc: 0.1442\n",
      "Epoch 35/100\n",
      " - 15s - loss: 2.4758 - acc: 0.1489\n",
      "Epoch 36/100\n",
      " - 15s - loss: 2.5238 - acc: 0.1460\n",
      "Epoch 37/100\n",
      " - 15s - loss: 2.5214 - acc: 0.1478\n",
      "Epoch 38/100\n",
      " - 15s - loss: 2.4431 - acc: 0.1624\n",
      "Epoch 39/100\n",
      " - 15s - loss: 2.4987 - acc: 0.1438\n",
      "Epoch 40/100\n",
      " - 15s - loss: 2.4743 - acc: 0.1602\n",
      "Epoch 41/100\n",
      " - 15s - loss: 2.4911 - acc: 0.1507\n",
      "Epoch 42/100\n",
      " - 15s - loss: 2.4670 - acc: 0.1557\n",
      "Epoch 43/100\n",
      " - 15s - loss: 2.4639 - acc: 0.1487\n",
      "Epoch 44/100\n",
      " - 15s - loss: 2.4089 - acc: 0.1629\n",
      "Epoch 45/100\n",
      " - 15s - loss: 2.3958 - acc: 0.1596\n",
      "Epoch 46/100\n",
      " - 15s - loss: 2.3922 - acc: 0.1667\n",
      "Epoch 47/100\n",
      " - 15s - loss: 2.3600 - acc: 0.1696\n",
      "Epoch 48/100\n",
      " - 15s - loss: 2.4482 - acc: 0.1548\n",
      "Epoch 49/100\n",
      " - 15s - loss: 2.3992 - acc: 0.1699\n",
      "Epoch 50/100\n",
      " - 15s - loss: 2.4291 - acc: 0.1565\n",
      "Epoch 51/100\n",
      " - 15s - loss: 2.4099 - acc: 0.1714\n",
      "Epoch 52/100\n",
      " - 15s - loss: 2.3834 - acc: 0.1651\n",
      "Epoch 53/100\n",
      " - 15s - loss: 2.3873 - acc: 0.1878\n",
      "Epoch 54/100\n",
      " - 15s - loss: 2.3709 - acc: 0.1766\n",
      "Epoch 55/100\n",
      " - 15s - loss: 2.3683 - acc: 0.1656\n",
      "Epoch 56/100\n",
      " - 15s - loss: 2.3973 - acc: 0.1758\n",
      "Epoch 57/100\n",
      " - 15s - loss: 2.3784 - acc: 0.1643\n",
      "Epoch 58/100\n",
      " - 15s - loss: 2.3464 - acc: 0.1815\n",
      "Epoch 59/100\n",
      " - 15s - loss: 2.3249 - acc: 0.1878\n",
      "Epoch 60/100\n",
      " - 15s - loss: 2.3398 - acc: 0.1811\n",
      "Epoch 61/100\n",
      " - 15s - loss: 2.3137 - acc: 0.1894\n",
      "Epoch 62/100\n",
      " - 15s - loss: 2.3215 - acc: 0.1726\n",
      "Epoch 63/100\n",
      " - 15s - loss: 2.2978 - acc: 0.1749\n",
      "Epoch 64/100\n",
      " - 15s - loss: 2.3207 - acc: 0.1746\n",
      "Epoch 65/100\n",
      " - 15s - loss: 2.2945 - acc: 0.1955\n",
      "Epoch 66/100\n",
      " - 15s - loss: 2.2817 - acc: 0.1883\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 15s - loss: 2.3245 - acc: 0.2005\n",
      "Epoch 68/100\n",
      " - 15s - loss: 2.3183 - acc: 0.1929\n",
      "Epoch 69/100\n",
      " - 15s - loss: 2.3333 - acc: 0.1974\n",
      "Epoch 70/100\n",
      " - 15s - loss: 2.3053 - acc: 0.1892\n",
      "Epoch 71/100\n",
      " - 15s - loss: 2.3487 - acc: 0.2041\n",
      "Epoch 72/100\n",
      " - 15s - loss: 2.3129 - acc: 0.1959\n",
      "Epoch 73/100\n",
      " - 15s - loss: 2.2632 - acc: 0.1943\n",
      "Epoch 74/100\n",
      " - 15s - loss: 2.2743 - acc: 0.1893\n",
      "Epoch 75/100\n",
      " - 15s - loss: 2.3015 - acc: 0.1943\n",
      "Epoch 76/100\n",
      " - 15s - loss: 2.3109 - acc: 0.2072\n",
      "Epoch 77/100\n",
      " - 15s - loss: 2.2728 - acc: 0.1954\n",
      "Epoch 78/100\n",
      " - 15s - loss: 2.2771 - acc: 0.2099\n",
      "Epoch 79/100\n",
      " - 15s - loss: 2.2389 - acc: 0.2138\n",
      "Epoch 80/100\n",
      " - 15s - loss: 2.2935 - acc: 0.2002\n",
      "Epoch 81/100\n",
      " - 15s - loss: 2.2847 - acc: 0.2113\n",
      "Epoch 82/100\n",
      " - 15s - loss: 2.2661 - acc: 0.2156\n",
      "Epoch 83/100\n",
      " - 15s - loss: 2.2810 - acc: 0.1908\n",
      "Epoch 84/100\n",
      " - 15s - loss: 2.3242 - acc: 0.2097\n",
      "Epoch 85/100\n",
      " - 15s - loss: 2.3445 - acc: 0.2107\n",
      "Epoch 86/100\n",
      " - 15s - loss: 2.2414 - acc: 0.2152\n",
      "Epoch 87/100\n",
      " - 15s - loss: 2.2687 - acc: 0.1991\n",
      "Epoch 88/100\n",
      " - 15s - loss: 2.2588 - acc: 0.2204\n",
      "Epoch 89/100\n",
      " - 15s - loss: 2.2084 - acc: 0.2265\n",
      "Epoch 90/100\n",
      " - 15s - loss: 2.1950 - acc: 0.2327\n",
      "Epoch 91/100\n",
      " - 15s - loss: 2.2198 - acc: 0.2185\n",
      "Epoch 92/100\n",
      " - 15s - loss: 2.1390 - acc: 0.2416\n",
      "Epoch 93/100\n",
      " - 15s - loss: 2.1502 - acc: 0.2290\n",
      "Epoch 94/100\n",
      " - 15s - loss: 2.1670 - acc: 0.2291\n",
      "Epoch 95/100\n",
      " - 15s - loss: 2.1367 - acc: 0.2391\n",
      "Epoch 96/100\n",
      " - 15s - loss: 2.1611 - acc: 0.2340\n",
      "Epoch 97/100\n",
      " - 15s - loss: 2.1609 - acc: 0.2207\n",
      "Epoch 98/100\n",
      " - 15s - loss: 2.1720 - acc: 0.2242\n",
      "Epoch 99/100\n",
      " - 15s - loss: 2.1154 - acc: 0.2438\n",
      "Epoch 100/100\n",
      " - 15s - loss: 2.1183 - acc: 0.2385\n",
      ">3: train=0.187256 test=0.094116\n",
      "          train      test\n",
      "count  3.000000  3.000000\n",
      "mean   0.074340  0.093938\n",
      "std    0.098068  0.008312\n",
      "min    0.010474  0.085539\n",
      "25%    0.017881  0.089828\n",
      "50%    0.025289  0.094116\n",
      "75%    0.106272  0.098138\n",
      "max    0.187256  0.102160\n"
     ]
    }
   ],
   "source": [
    "verbose = 2\n",
    "n_epochs = 100\n",
    "n_photos_per_update = 2\n",
    "n_batches_per_epoch = int(len(train) / n_photos_per_update)\n",
    "n_repeats = 3\n",
    "\n",
    "# run experiment\n",
    "train_results, test_results = list(), list()\n",
    "for i in range(n_repeats):\n",
    "    # define the model\n",
    "    model = define_model(vocab_size, max_length)\n",
    "    # define checkpoint callback\n",
    "#     filepath = 'fe_flat-model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "#     checkpoint = ModelCheckpoint(filepath, monitor='val_loss',\n",
    "#                                  verbose=1,\n",
    "#                                  save_best_only=True,\n",
    "#                                  mode='min')\n",
    "    # fit model\n",
    "    model.fit_generator(data_generator(train_descriptions, train_features, tokenizer, max_length, n_photos_per_update), \n",
    "                        steps_per_epoch=n_batches_per_epoch, \n",
    "                        epochs=n_epochs,\n",
    "                        verbose=verbose)\n",
    "    # evaluate model on training data\n",
    "    train_score = evaluate_model(model, train_descriptions, train_features, tokenizer, max_length)\n",
    "    test_score = evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)\n",
    "    # store\n",
    "    train_results.append(train_score)\n",
    "    test_results.append(test_score)\n",
    "    print('>%d: train=%f test=%f' % ((i+1), train_score, test_score))\n",
    "# save results to file\n",
    "df = DataFrame()\n",
    "df['train'] = train_results\n",
    "df['test'] = test_results\n",
    "print(df.describe())\n",
    "df.to_csv(model_name+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Scores table\n",
    "|       |   train  |    test|\n",
    "|-------|----------|--------|\n",
    "|count  |3.000000  |3.000000|\n",
    "|mean   |0.074340  |0.093938|\n",
    "|std    |0.098068  |0.008312|\n",
    "|min    |0.010474  |0.085539|\n",
    "|25%    |0.017881  |0.089828|\n",
    "|50%    |0.025289  |0.094116|\n",
    "|75%    |0.106272  |0.098138|\n",
    "|max    |0.187256  |0.102160|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try repeating this experiment and provide more capacity for interpreting the extracted photo features. **A new Dense layer with 500 neurons is added after the Flatten layer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "    # feature extractor (encoder)\n",
    "    inputs1 = Input(shape=(7, 7, 512))\n",
    "    fe1 = Flatten()(inputs1)\n",
    "    fe2 = Dense(500, activation='relu')(fe1)\n",
    "    fe3 = Dense(128, activation='relu')(fe2)\n",
    "    fe4 = RepeatVector(max_length)(fe3)\n",
    "    # embedding\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    emb2 = Embedding(vocab_size, 50, mask_zero=True)(inputs2)\n",
    "    emb3 = LSTM(256, return_sequences=True)(emb2)\n",
    "    emb4 = TimeDistributed(Dense(128, activation='relu'))(emb3)\n",
    "    # merge inputs\n",
    "    merged = concatenate([fe4, emb4])\n",
    "    # language model (decoder)\n",
    "    lm2 = LSTM(500)(merged)\n",
    "    lm3 = Dense(500, activation='relu')(lm2)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(lm3)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_name = 'fe_flat2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 37s - loss: 7.0304 - acc: 0.0491\n",
      "Epoch 2/100\n",
      " - 21s - loss: 5.6141 - acc: 0.0540\n",
      "Epoch 3/100\n",
      " - 21s - loss: 5.4540 - acc: 0.0542\n",
      "Epoch 4/100\n",
      " - 21s - loss: 5.3973 - acc: 0.0575\n",
      "Epoch 5/100\n",
      " - 21s - loss: 5.3779 - acc: 0.0615\n",
      "Epoch 6/100\n",
      " - 22s - loss: 5.3801 - acc: 0.0630\n",
      "Epoch 7/100\n",
      " - 21s - loss: 5.3773 - acc: 0.0630\n",
      "Epoch 8/100\n",
      " - 21s - loss: 5.3462 - acc: 0.0630\n",
      "Epoch 9/100\n",
      " - 21s - loss: 5.3360 - acc: 0.0630\n",
      "Epoch 10/100\n",
      " - 21s - loss: 5.3422 - acc: 0.0630\n",
      "Epoch 11/100\n",
      " - 21s - loss: 5.3236 - acc: 0.0630\n",
      "Epoch 12/100\n",
      " - 21s - loss: 5.3188 - acc: 0.0630\n",
      "Epoch 13/100\n",
      " - 21s - loss: 5.3035 - acc: 0.0620\n",
      "Epoch 14/100\n",
      " - 21s - loss: 5.3799 - acc: 0.0630\n",
      "Epoch 15/100\n",
      " - 21s - loss: 5.3209 - acc: 0.0630\n",
      "Epoch 16/100\n",
      " - 21s - loss: 5.3344 - acc: 0.0630\n",
      "Epoch 17/100\n",
      " - 21s - loss: 5.3197 - acc: 0.0630\n",
      "Epoch 18/100\n",
      " - 21s - loss: 5.3336 - acc: 0.0630\n",
      "Epoch 19/100\n",
      " - 21s - loss: 5.3586 - acc: 0.0648\n",
      "Epoch 20/100\n",
      " - 21s - loss: 5.3118 - acc: 0.0600\n",
      "Epoch 21/100\n",
      " - 21s - loss: 5.2787 - acc: 0.0630\n",
      "Epoch 22/100\n",
      " - 21s - loss: 5.2796 - acc: 0.0630\n",
      "Epoch 23/100\n",
      " - 21s - loss: 5.2407 - acc: 0.0623\n",
      "Epoch 24/100\n",
      " - 21s - loss: 5.2639 - acc: 0.0623\n",
      "Epoch 25/100\n",
      " - 21s - loss: 5.2340 - acc: 0.0630\n",
      "Epoch 26/100\n",
      " - 21s - loss: 5.2312 - acc: 0.0630\n",
      "Epoch 27/100\n",
      " - 21s - loss: 5.2322 - acc: 0.0642\n",
      "Epoch 28/100\n",
      " - 21s - loss: 5.3095 - acc: 0.0660\n",
      "Epoch 29/100\n",
      " - 21s - loss: 5.2293 - acc: 0.0630\n",
      "Epoch 30/100\n",
      " - 21s - loss: 5.2094 - acc: 0.0615\n",
      "Epoch 31/100\n",
      " - 21s - loss: 5.2008 - acc: 0.0660\n",
      "Epoch 32/100\n",
      " - 21s - loss: 5.2011 - acc: 0.0662\n",
      "Epoch 33/100\n",
      " - 21s - loss: 5.1814 - acc: 0.0668\n",
      "Epoch 34/100\n",
      " - 21s - loss: 5.1713 - acc: 0.0652\n",
      "Epoch 35/100\n",
      " - 21s - loss: 5.1969 - acc: 0.0634\n",
      "Epoch 36/100\n",
      " - 21s - loss: 5.2150 - acc: 0.0692\n",
      "Epoch 37/100\n",
      " - 21s - loss: 5.1838 - acc: 0.0622\n",
      "Epoch 38/100\n",
      " - 21s - loss: 5.1660 - acc: 0.0544\n",
      "Epoch 39/100\n",
      " - 21s - loss: 5.1363 - acc: 0.0614\n",
      "Epoch 40/100\n",
      " - 21s - loss: 5.1878 - acc: 0.0625\n",
      "Epoch 41/100\n",
      " - 21s - loss: 5.1626 - acc: 0.0567\n",
      "Epoch 42/100\n",
      " - 21s - loss: 5.1327 - acc: 0.0633\n",
      "Epoch 43/100\n",
      " - 21s - loss: 5.1090 - acc: 0.0589\n",
      "Epoch 44/100\n",
      " - 21s - loss: 5.0878 - acc: 0.0608\n",
      "Epoch 45/100\n",
      " - 21s - loss: 5.1024 - acc: 0.0652\n",
      "Epoch 46/100\n",
      " - 21s - loss: 5.1058 - acc: 0.0576\n",
      "Epoch 47/100\n",
      " - 21s - loss: 5.0457 - acc: 0.0636\n",
      "Epoch 48/100\n",
      " - 21s - loss: 5.0281 - acc: 0.0636\n",
      "Epoch 49/100\n",
      " - 21s - loss: 5.0172 - acc: 0.0664\n",
      "Epoch 50/100\n",
      " - 21s - loss: 5.0438 - acc: 0.0661\n",
      "Epoch 51/100\n",
      " - 21s - loss: 4.9935 - acc: 0.0642\n",
      "Epoch 52/100\n",
      " - 21s - loss: 4.9781 - acc: 0.0642\n",
      "Epoch 53/100\n",
      " - 21s - loss: 4.9635 - acc: 0.0656\n",
      "Epoch 54/100\n",
      " - 21s - loss: 4.9616 - acc: 0.0670\n",
      "Epoch 55/100\n",
      " - 21s - loss: 4.9505 - acc: 0.0694\n",
      "Epoch 56/100\n",
      " - 21s - loss: 4.9264 - acc: 0.0724\n",
      "Epoch 57/100\n",
      " - 21s - loss: 4.9019 - acc: 0.0667\n",
      "Epoch 58/100\n",
      " - 21s - loss: 4.8866 - acc: 0.0619\n",
      "Epoch 59/100\n",
      " - 21s - loss: 4.8571 - acc: 0.0614\n",
      "Epoch 60/100\n",
      " - 21s - loss: 4.8656 - acc: 0.0632\n",
      "Epoch 61/100\n",
      " - 21s - loss: 4.9018 - acc: 0.0606\n",
      "Epoch 62/100\n",
      " - 21s - loss: 4.9092 - acc: 0.0644\n",
      "Epoch 63/100\n",
      " - 21s - loss: 4.9261 - acc: 0.0580\n",
      "Epoch 64/100\n",
      " - 21s - loss: 4.8646 - acc: 0.0607\n",
      "Epoch 65/100\n",
      " - 21s - loss: 4.9202 - acc: 0.0577\n",
      "Epoch 66/100\n",
      " - 21s - loss: 4.8387 - acc: 0.0665\n",
      "Epoch 67/100\n",
      " - 21s - loss: 4.8126 - acc: 0.0649\n",
      "Epoch 68/100\n",
      " - 21s - loss: 4.8322 - acc: 0.0691\n",
      "Epoch 69/100\n",
      " - 21s - loss: 4.7706 - acc: 0.0741\n",
      "Epoch 70/100\n",
      " - 21s - loss: 4.7973 - acc: 0.0678\n",
      "Epoch 71/100\n",
      " - 21s - loss: 4.7592 - acc: 0.0758\n",
      "Epoch 72/100\n",
      " - 21s - loss: 4.7264 - acc: 0.0759\n",
      "Epoch 73/100\n",
      " - 21s - loss: 4.7116 - acc: 0.0664\n",
      "Epoch 74/100\n",
      " - 21s - loss: 4.6678 - acc: 0.0802\n",
      "Epoch 75/100\n",
      " - 21s - loss: 4.7118 - acc: 0.0824\n",
      "Epoch 76/100\n",
      " - 21s - loss: 4.6448 - acc: 0.0864\n",
      "Epoch 77/100\n",
      " - 21s - loss: 4.6409 - acc: 0.0809\n",
      "Epoch 78/100\n",
      " - 21s - loss: 4.6243 - acc: 0.0836\n",
      "Epoch 79/100\n",
      " - 21s - loss: 4.6463 - acc: 0.0855\n",
      "Epoch 80/100\n",
      " - 21s - loss: 4.6838 - acc: 0.0735\n",
      "Epoch 81/100\n",
      " - 21s - loss: 4.6448 - acc: 0.0775\n",
      "Epoch 82/100\n",
      " - 21s - loss: 4.6176 - acc: 0.0773\n",
      "Epoch 83/100\n",
      " - 21s - loss: 4.6388 - acc: 0.0753\n",
      "Epoch 84/100\n",
      " - 21s - loss: 4.6067 - acc: 0.0823\n",
      "Epoch 85/100\n",
      " - 21s - loss: 4.6110 - acc: 0.0778\n",
      "Epoch 86/100\n",
      " - 21s - loss: 4.5766 - acc: 0.0754\n",
      "Epoch 87/100\n",
      " - 21s - loss: 4.5548 - acc: 0.0827\n",
      "Epoch 88/100\n",
      " - 21s - loss: 4.5472 - acc: 0.0829\n",
      "Epoch 89/100\n",
      " - 21s - loss: 4.5606 - acc: 0.0790\n",
      "Epoch 90/100\n",
      " - 21s - loss: 4.5592 - acc: 0.0872\n",
      "Epoch 91/100\n",
      " - 21s - loss: 4.6957 - acc: 0.0756\n",
      "Epoch 92/100\n",
      " - 21s - loss: 4.6341 - acc: 0.0827\n",
      "Epoch 93/100\n",
      " - 21s - loss: 4.5672 - acc: 0.0816\n",
      "Epoch 94/100\n",
      " - 21s - loss: 4.6055 - acc: 0.0824\n",
      "Epoch 95/100\n",
      " - 21s - loss: 4.6501 - acc: 0.0715\n",
      "Epoch 96/100\n",
      " - 21s - loss: 4.5910 - acc: 0.0831\n",
      "Epoch 97/100\n",
      " - 21s - loss: 4.6613 - acc: 0.0727\n",
      "Epoch 98/100\n",
      " - 21s - loss: 4.7550 - acc: 0.0759\n",
      "Epoch 99/100\n",
      " - 21s - loss: 4.6283 - acc: 0.0851\n",
      "Epoch 100/100\n",
      " - 21s - loss: 4.5798 - acc: 0.0828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shourya97/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1: train=0.111371 test=0.103015\n",
      "Epoch 1/100\n",
      " - 22s - loss: 7.0019 - acc: 0.0516\n",
      "Epoch 2/100\n",
      " - 21s - loss: 5.6273 - acc: 0.0578\n",
      "Epoch 3/100\n",
      " - 21s - loss: 5.4741 - acc: 0.0620\n",
      "Epoch 4/100\n",
      " - 21s - loss: 5.4111 - acc: 0.0609\n",
      "Epoch 5/100\n",
      " - 21s - loss: 5.3797 - acc: 0.0630\n",
      "Epoch 6/100\n",
      " - 21s - loss: 5.3479 - acc: 0.0630\n",
      "Epoch 7/100\n",
      " - 21s - loss: 5.3200 - acc: 0.0630\n",
      "Epoch 8/100\n",
      " - 21s - loss: 5.3582 - acc: 0.0635\n",
      "Epoch 9/100\n",
      " - 21s - loss: 5.3176 - acc: 0.0630\n",
      "Epoch 10/100\n",
      " - 21s - loss: 5.3036 - acc: 0.0630\n",
      "Epoch 11/100\n",
      " - 21s - loss: 5.3510 - acc: 0.0680\n",
      "Epoch 12/100\n",
      " - 21s - loss: 5.2638 - acc: 0.0685\n",
      "Epoch 13/100\n",
      " - 21s - loss: 5.2073 - acc: 0.0712\n",
      "Epoch 14/100\n",
      " - 21s - loss: 5.1750 - acc: 0.0685\n",
      "Epoch 15/100\n",
      " - 21s - loss: 5.1513 - acc: 0.0738\n",
      "Epoch 16/100\n",
      " - 21s - loss: 5.1048 - acc: 0.0796\n",
      "Epoch 17/100\n",
      " - 21s - loss: 5.0756 - acc: 0.0796\n",
      "Epoch 18/100\n",
      " - 21s - loss: 5.0921 - acc: 0.0780\n",
      "Epoch 19/100\n",
      " - 21s - loss: 5.0662 - acc: 0.0779\n",
      "Epoch 20/100\n",
      " - 21s - loss: 4.9887 - acc: 0.0773\n",
      "Epoch 21/100\n",
      " - 21s - loss: 5.0243 - acc: 0.0777\n",
      "Epoch 22/100\n",
      " - 21s - loss: 4.9669 - acc: 0.0775\n",
      "Epoch 23/100\n",
      " - 21s - loss: 4.9699 - acc: 0.0778\n",
      "Epoch 24/100\n",
      " - 21s - loss: 4.9656 - acc: 0.0747\n",
      "Epoch 25/100\n",
      " - 21s - loss: 4.9192 - acc: 0.0773\n",
      "Epoch 26/100\n",
      " - 21s - loss: 4.9119 - acc: 0.0755\n",
      "Epoch 27/100\n",
      " - 21s - loss: 4.8729 - acc: 0.0816\n",
      "Epoch 28/100\n",
      " - 21s - loss: 4.8731 - acc: 0.0763\n",
      "Epoch 29/100\n",
      " - 21s - loss: 4.8326 - acc: 0.0810\n",
      "Epoch 30/100\n",
      " - 21s - loss: 4.8096 - acc: 0.0810\n",
      "Epoch 31/100\n",
      " - 21s - loss: 4.8609 - acc: 0.0870\n",
      "Epoch 32/100\n",
      " - 21s - loss: 4.8510 - acc: 0.0847\n",
      "Epoch 33/100\n",
      " - 21s - loss: 4.7547 - acc: 0.0825\n",
      "Epoch 34/100\n",
      " - 21s - loss: 4.7519 - acc: 0.0852\n",
      "Epoch 35/100\n",
      " - 21s - loss: 4.7569 - acc: 0.0862\n",
      "Epoch 36/100\n",
      " - 21s - loss: 4.7903 - acc: 0.0884\n",
      "Epoch 37/100\n",
      " - 21s - loss: 4.7767 - acc: 0.0788\n",
      "Epoch 38/100\n",
      " - 21s - loss: 4.7131 - acc: 0.0872\n",
      "Epoch 39/100\n",
      " - 21s - loss: 4.7279 - acc: 0.0811\n",
      "Epoch 40/100\n",
      " - 21s - loss: 4.7617 - acc: 0.0840\n",
      "Epoch 41/100\n",
      " - 21s - loss: 4.7395 - acc: 0.0860\n",
      "Epoch 42/100\n",
      " - 21s - loss: 4.7509 - acc: 0.0848\n",
      "Epoch 43/100\n",
      " - 21s - loss: 4.6806 - acc: 0.0859\n",
      "Epoch 44/100\n",
      " - 21s - loss: 4.6498 - acc: 0.0886\n",
      "Epoch 45/100\n",
      " - 21s - loss: 4.6249 - acc: 0.0882\n",
      "Epoch 46/100\n",
      " - 21s - loss: 4.6355 - acc: 0.0907\n",
      "Epoch 47/100\n",
      " - 21s - loss: 4.6672 - acc: 0.0826\n",
      "Epoch 48/100\n",
      " - 21s - loss: 4.6694 - acc: 0.0824\n",
      "Epoch 49/100\n",
      " - 21s - loss: 4.6460 - acc: 0.0786\n",
      "Epoch 50/100\n",
      " - 21s - loss: 4.6237 - acc: 0.0789\n",
      "Epoch 51/100\n",
      " - 21s - loss: 4.6436 - acc: 0.0760\n",
      "Epoch 52/100\n",
      " - 21s - loss: 4.6613 - acc: 0.0842\n",
      "Epoch 53/100\n",
      " - 21s - loss: 4.6802 - acc: 0.0832\n",
      "Epoch 54/100\n",
      " - 21s - loss: 4.6892 - acc: 0.0872\n",
      "Epoch 55/100\n",
      " - 21s - loss: 4.6780 - acc: 0.0867\n",
      "Epoch 56/100\n",
      " - 21s - loss: 4.6146 - acc: 0.0911\n",
      "Epoch 57/100\n",
      " - 21s - loss: 4.6362 - acc: 0.0917\n",
      "Epoch 58/100\n",
      " - 21s - loss: 4.6094 - acc: 0.0935\n",
      "Epoch 59/100\n",
      " - 21s - loss: 4.5729 - acc: 0.0912\n",
      "Epoch 60/100\n",
      " - 21s - loss: 4.5521 - acc: 0.0912\n",
      "Epoch 61/100\n",
      " - 21s - loss: 4.5191 - acc: 0.0923\n",
      "Epoch 62/100\n",
      " - 21s - loss: 4.5379 - acc: 0.0898\n",
      "Epoch 63/100\n",
      " - 21s - loss: 4.5513 - acc: 0.0910\n",
      "Epoch 64/100\n",
      " - 21s - loss: 4.5243 - acc: 0.0905\n",
      "Epoch 65/100\n",
      " - 21s - loss: 4.5224 - acc: 0.0909\n",
      "Epoch 66/100\n",
      " - 21s - loss: 4.5128 - acc: 0.0948\n",
      "Epoch 67/100\n",
      " - 21s - loss: 4.5298 - acc: 0.0913\n",
      "Epoch 68/100\n",
      " - 21s - loss: 4.5183 - acc: 0.0892\n",
      "Epoch 69/100\n",
      " - 21s - loss: 4.5281 - acc: 0.0879\n",
      "Epoch 70/100\n",
      " - 21s - loss: 4.4961 - acc: 0.0899\n",
      "Epoch 71/100\n",
      " - 21s - loss: 4.4703 - acc: 0.0929\n",
      "Epoch 72/100\n",
      " - 21s - loss: 4.4787 - acc: 0.0933\n",
      "Epoch 73/100\n",
      " - 21s - loss: 4.4570 - acc: 0.0897\n",
      "Epoch 74/100\n",
      " - 21s - loss: 4.4278 - acc: 0.0968\n",
      "Epoch 75/100\n",
      " - 21s - loss: 4.4078 - acc: 0.0929\n",
      "Epoch 76/100\n",
      " - 21s - loss: 4.3832 - acc: 0.0952\n",
      "Epoch 77/100\n",
      " - 21s - loss: 4.3957 - acc: 0.0886\n",
      "Epoch 78/100\n",
      " - 21s - loss: 4.3963 - acc: 0.0956\n",
      "Epoch 79/100\n",
      " - 21s - loss: 4.4423 - acc: 0.0906\n",
      "Epoch 80/100\n",
      " - 21s - loss: 4.4424 - acc: 0.0932\n",
      "Epoch 81/100\n",
      " - 21s - loss: 4.4214 - acc: 0.0892\n",
      "Epoch 82/100\n",
      " - 21s - loss: 4.3835 - acc: 0.0866\n",
      "Epoch 83/100\n",
      " - 21s - loss: 4.3741 - acc: 0.0845\n",
      "Epoch 84/100\n",
      " - 21s - loss: 4.3531 - acc: 0.0915\n",
      "Epoch 85/100\n",
      " - 21s - loss: 4.3403 - acc: 0.0992\n",
      "Epoch 86/100\n",
      " - 21s - loss: 4.3437 - acc: 0.0960\n",
      "Epoch 87/100\n",
      " - 21s - loss: 4.3459 - acc: 0.0910\n",
      "Epoch 88/100\n",
      " - 21s - loss: 4.3717 - acc: 0.0927\n",
      "Epoch 89/100\n",
      " - 21s - loss: 4.3502 - acc: 0.0891\n",
      "Epoch 90/100\n",
      " - 21s - loss: 4.3558 - acc: 0.0932\n",
      "Epoch 91/100\n",
      " - 21s - loss: 4.3357 - acc: 0.0952\n",
      "Epoch 92/100\n",
      " - 21s - loss: 4.4080 - acc: 0.0950\n",
      "Epoch 93/100\n",
      " - 21s - loss: 4.3394 - acc: 0.0988\n",
      "Epoch 94/100\n",
      " - 21s - loss: 4.3078 - acc: 0.0940\n",
      "Epoch 95/100\n",
      " - 21s - loss: 4.3107 - acc: 0.0910\n",
      "Epoch 96/100\n",
      " - 21s - loss: 4.3076 - acc: 0.0943\n",
      "Epoch 97/100\n",
      " - 21s - loss: 4.3223 - acc: 0.0946\n",
      "Epoch 98/100\n",
      " - 21s - loss: 4.3483 - acc: 0.0973\n",
      "Epoch 99/100\n",
      " - 21s - loss: 4.3452 - acc: 0.0902\n",
      "Epoch 100/100\n",
      " - 21s - loss: 4.3432 - acc: 0.0915\n",
      ">2: train=0.128856 test=0.104797\n",
      "Epoch 1/100\n",
      " - 22s - loss: 7.0335 - acc: 0.0457\n",
      "Epoch 2/100\n",
      " - 21s - loss: 5.6236 - acc: 0.0571\n",
      "Epoch 3/100\n",
      " - 21s - loss: 5.4510 - acc: 0.0601\n",
      "Epoch 4/100\n",
      " - 21s - loss: 5.3949 - acc: 0.0612\n",
      "Epoch 5/100\n",
      " - 21s - loss: 5.3778 - acc: 0.0568\n",
      "Epoch 6/100\n",
      " - 21s - loss: 5.3818 - acc: 0.0563\n",
      "Epoch 7/100\n",
      " - 21s - loss: 5.3931 - acc: 0.0630\n",
      "Epoch 8/100\n",
      " - 21s - loss: 5.4230 - acc: 0.0630\n",
      "Epoch 9/100\n",
      " - 21s - loss: 5.4206 - acc: 0.0599\n",
      "Epoch 10/100\n",
      " - 21s - loss: 5.3833 - acc: 0.0630\n",
      "Epoch 11/100\n",
      " - 21s - loss: 5.3812 - acc: 0.0630\n",
      "Epoch 12/100\n",
      " - 21s - loss: 5.3559 - acc: 0.0630\n",
      "Epoch 13/100\n",
      " - 21s - loss: 5.3646 - acc: 0.0630\n",
      "Epoch 14/100\n",
      " - 21s - loss: 5.3177 - acc: 0.0630\n",
      "Epoch 15/100\n",
      " - 21s - loss: 5.3752 - acc: 0.0630\n",
      "Epoch 16/100\n",
      " - 21s - loss: 5.3261 - acc: 0.0630\n",
      "Epoch 17/100\n",
      " - 21s - loss: 5.3206 - acc: 0.0630\n",
      "Epoch 18/100\n",
      " - 21s - loss: 5.3193 - acc: 0.0623\n",
      "Epoch 19/100\n",
      " - 21s - loss: 5.3173 - acc: 0.0641\n",
      "Epoch 20/100\n",
      " - 21s - loss: 5.2465 - acc: 0.0662\n",
      "Epoch 21/100\n",
      " - 21s - loss: 5.3163 - acc: 0.0641\n",
      "Epoch 22/100\n",
      " - 21s - loss: 5.3196 - acc: 0.0649\n",
      "Epoch 23/100\n",
      " - 21s - loss: 5.2603 - acc: 0.0644\n",
      "Epoch 24/100\n",
      " - 21s - loss: 5.2424 - acc: 0.0672\n",
      "Epoch 25/100\n",
      " - 21s - loss: 5.1991 - acc: 0.0679\n",
      "Epoch 26/100\n",
      " - 21s - loss: 5.1859 - acc: 0.0622\n",
      "Epoch 27/100\n",
      " - 21s - loss: 5.1776 - acc: 0.0593\n",
      "Epoch 28/100\n",
      " - 21s - loss: 5.1768 - acc: 0.0640\n",
      "Epoch 29/100\n",
      " - 21s - loss: 5.1918 - acc: 0.0589\n",
      "Epoch 30/100\n",
      " - 21s - loss: 5.1899 - acc: 0.0608\n",
      "Epoch 31/100\n",
      " - 21s - loss: 5.1897 - acc: 0.0623\n",
      "Epoch 32/100\n",
      " - 21s - loss: 5.2113 - acc: 0.0632\n",
      "Epoch 33/100\n",
      " - 21s - loss: 5.2306 - acc: 0.0572\n",
      "Epoch 34/100\n",
      " - 21s - loss: 5.2610 - acc: 0.0663\n",
      "Epoch 35/100\n",
      " - 21s - loss: 5.1980 - acc: 0.0637\n",
      "Epoch 36/100\n",
      " - 21s - loss: 5.2201 - acc: 0.0682\n",
      "Epoch 37/100\n",
      " - 21s - loss: 5.2575 - acc: 0.0680\n",
      "Epoch 38/100\n",
      " - 21s - loss: 5.1944 - acc: 0.0657\n",
      "Epoch 39/100\n",
      " - 21s - loss: 5.2278 - acc: 0.0710\n",
      "Epoch 40/100\n",
      " - 21s - loss: 5.1837 - acc: 0.0685\n",
      "Epoch 41/100\n",
      " - 21s - loss: 5.2137 - acc: 0.0671\n",
      "Epoch 42/100\n",
      " - 21s - loss: 5.2325 - acc: 0.0627\n",
      "Epoch 43/100\n",
      " - 21s - loss: 5.1508 - acc: 0.0709\n",
      "Epoch 44/100\n",
      " - 21s - loss: 5.1320 - acc: 0.0747\n",
      "Epoch 45/100\n",
      " - 21s - loss: 5.1186 - acc: 0.0729\n",
      "Epoch 46/100\n",
      " - 21s - loss: 5.0982 - acc: 0.0727\n",
      "Epoch 47/100\n",
      " - 21s - loss: 5.0749 - acc: 0.0724\n",
      "Epoch 48/100\n",
      " - 21s - loss: 5.0663 - acc: 0.0712\n",
      "Epoch 49/100\n",
      " - 21s - loss: 5.0508 - acc: 0.0733\n",
      "Epoch 50/100\n",
      " - 21s - loss: 5.1124 - acc: 0.0601\n",
      "Epoch 51/100\n",
      " - 21s - loss: 5.0656 - acc: 0.0657\n",
      "Epoch 52/100\n",
      " - 21s - loss: 5.0473 - acc: 0.0667\n",
      "Epoch 53/100\n",
      " - 21s - loss: 4.9581 - acc: 0.0779\n",
      "Epoch 54/100\n",
      " - 21s - loss: 4.9948 - acc: 0.0724\n",
      "Epoch 55/100\n",
      " - 21s - loss: 4.9761 - acc: 0.0776\n",
      "Epoch 56/100\n",
      " - 21s - loss: 4.9963 - acc: 0.0740\n",
      "Epoch 57/100\n",
      " - 21s - loss: 4.9117 - acc: 0.0701\n",
      "Epoch 58/100\n",
      " - 21s - loss: 4.8930 - acc: 0.0753\n",
      "Epoch 59/100\n",
      " - 21s - loss: 4.8915 - acc: 0.0724\n",
      "Epoch 60/100\n",
      " - 21s - loss: 4.8852 - acc: 0.0756\n",
      "Epoch 61/100\n",
      " - 21s - loss: 4.9124 - acc: 0.0784\n",
      "Epoch 62/100\n",
      " - 21s - loss: 4.8967 - acc: 0.0703\n",
      "Epoch 63/100\n",
      " - 21s - loss: 5.0156 - acc: 0.0722\n",
      "Epoch 64/100\n",
      " - 21s - loss: 4.9745 - acc: 0.0693\n",
      "Epoch 65/100\n",
      " - 21s - loss: 4.9665 - acc: 0.0704\n",
      "Epoch 66/100\n",
      " - 21s - loss: 4.9174 - acc: 0.0743\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 21s - loss: 4.9514 - acc: 0.0732\n",
      "Epoch 68/100\n",
      " - 21s - loss: 4.9784 - acc: 0.0688\n",
      "Epoch 69/100\n",
      " - 21s - loss: 5.0240 - acc: 0.0613\n",
      "Epoch 70/100\n",
      " - 21s - loss: 4.9895 - acc: 0.0755\n",
      "Epoch 71/100\n",
      " - 21s - loss: 4.8955 - acc: 0.0724\n",
      "Epoch 72/100\n",
      " - 21s - loss: 4.9293 - acc: 0.0719\n",
      "Epoch 73/100\n",
      " - 21s - loss: 4.9268 - acc: 0.0735\n",
      "Epoch 74/100\n",
      " - 21s - loss: 4.8973 - acc: 0.0670\n",
      "Epoch 75/100\n",
      " - 21s - loss: 5.0281 - acc: 0.0778\n",
      "Epoch 76/100\n",
      " - 21s - loss: 4.9631 - acc: 0.0686\n",
      "Epoch 77/100\n",
      " - 21s - loss: 4.9221 - acc: 0.0769\n",
      "Epoch 78/100\n",
      " - 21s - loss: 4.9679 - acc: 0.0734\n",
      "Epoch 79/100\n",
      " - 21s - loss: 4.9843 - acc: 0.0756\n",
      "Epoch 80/100\n",
      " - 21s - loss: 4.8468 - acc: 0.0771\n",
      "Epoch 81/100\n",
      " - 21s - loss: 4.8416 - acc: 0.0769\n",
      "Epoch 82/100\n",
      " - 21s - loss: 4.7963 - acc: 0.0813\n",
      "Epoch 83/100\n",
      " - 21s - loss: 4.7582 - acc: 0.0821\n",
      "Epoch 84/100\n",
      " - 21s - loss: 4.7541 - acc: 0.0807\n",
      "Epoch 85/100\n",
      " - 21s - loss: 4.7709 - acc: 0.0817\n",
      "Epoch 86/100\n",
      " - 21s - loss: 4.7308 - acc: 0.0862\n",
      "Epoch 87/100\n",
      " - 21s - loss: 4.6967 - acc: 0.0875\n",
      "Epoch 88/100\n",
      " - 21s - loss: 4.6943 - acc: 0.0910\n",
      "Epoch 89/100\n",
      " - 21s - loss: 4.7275 - acc: 0.0912\n",
      "Epoch 90/100\n",
      " - 21s - loss: 4.7683 - acc: 0.0849\n",
      "Epoch 91/100\n",
      " - 21s - loss: 4.7522 - acc: 0.0882\n",
      "Epoch 92/100\n",
      " - 21s - loss: 4.7054 - acc: 0.0868\n",
      "Epoch 93/100\n",
      " - 21s - loss: 4.7037 - acc: 0.0935\n",
      "Epoch 94/100\n",
      " - 21s - loss: 4.7163 - acc: 0.0867\n",
      "Epoch 95/100\n",
      " - 21s - loss: 4.6973 - acc: 0.0891\n",
      "Epoch 96/100\n",
      " - 21s - loss: 4.7147 - acc: 0.0823\n",
      "Epoch 97/100\n",
      " - 21s - loss: 4.6861 - acc: 0.0875\n",
      "Epoch 98/100\n",
      " - 21s - loss: 4.6822 - acc: 0.0855\n",
      "Epoch 99/100\n",
      " - 21s - loss: 4.6602 - acc: 0.0889\n",
      "Epoch 100/100\n",
      " - 21s - loss: 4.6233 - acc: 0.0931\n",
      ">3: train=0.127245 test=0.017394\n",
      "          train      test\n",
      "count  3.000000  3.000000\n",
      "mean   0.122491  0.075069\n",
      "std    0.009664  0.049956\n",
      "min    0.111371  0.017394\n",
      "25%    0.119308  0.060204\n",
      "50%    0.127245  0.103015\n",
      "75%    0.128050  0.103906\n",
      "max    0.128856  0.104797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shourya97/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "verbose = 2\n",
    "n_epochs = 100\n",
    "n_photos_per_update = 2\n",
    "n_batches_per_epoch = int(len(train) / n_photos_per_update)\n",
    "n_repeats = 3\n",
    "\n",
    "# run experiment\n",
    "train_results, test_results = list(), list()\n",
    "for i in range(n_repeats):\n",
    "    # define the model\n",
    "    model = define_model(vocab_size, max_length)\n",
    "    # define checkpoint callback\n",
    "#     filepath = 'fe_flat2-model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "#     checkpoint = ModelCheckpoint(filepath, monitor='val_loss',\n",
    "#                                  verbose=1,\n",
    "#                                  save_best_only=True,\n",
    "#                                  mode='min')\n",
    "    # fit model\n",
    "    model.fit_generator(data_generator(train_descriptions, train_features, tokenizer, max_length, n_photos_per_update), \n",
    "                        steps_per_epoch=n_batches_per_epoch, \n",
    "                        epochs=n_epochs,\n",
    "                        verbose=verbose)\n",
    "    # evaluate model on training data\n",
    "    train_score = evaluate_model(model, train_descriptions, train_features, tokenizer, max_length)\n",
    "    test_score = evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)\n",
    "    # store\n",
    "    train_results.append(train_score)\n",
    "    test_results.append(test_score)\n",
    "    print('>%d: train=%f test=%f' % ((i+1), train_score, test_score))\n",
    "# save results to file\n",
    "df = DataFrame()\n",
    "df['train'] = train_results\n",
    "df['test'] = test_results\n",
    "print(df.describe())\n",
    "df.to_csv(model_name+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU scores table\n",
    "|       |   train  |    test|\n",
    "|-------|----------|--------|\n",
    "|count  |3.000000  |3.000000|\n",
    "|mean   |0.122491  |0.075069|\n",
    "|std    |0.009664  |0.049956|\n",
    "|min    |0.111371  |0.017394|\n",
    "|25%    |0.119308  |0.060204|\n",
    "|50%    |0.127245  |0.103015|\n",
    "|75%    |0.128050  |0.103906|\n",
    "|max    |0.128856  |0.104797|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Results\n",
    "**Larger BLEU scores are better.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdbffaad518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdb9f135780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from os import listdir\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load all .csv results into a dataframe\n",
    "train, test = DataFrame(), DataFrame()\n",
    "directory = 'results'\n",
    "for name in listdir(directory):\n",
    "    if not name.endswith('csv'):\n",
    "        continue\n",
    "    filename = directory + '/' + name\n",
    "    data = read_csv(filename, header=0)\n",
    "    experiment = name.split('.')[0]\n",
    "    train[experiment] = data['train']\n",
    "    test[experiment] = data['test']\n",
    "\n",
    "# plot results on train\n",
    "train.boxplot(vert=False)\n",
    "pyplot.show()\n",
    "# plot results on test\n",
    "test.boxplot(vert=False)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
